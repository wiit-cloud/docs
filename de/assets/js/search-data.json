{"0": {
    "doc": "Not found",
    "title": "404 Not Found",
    "content": "We cannot find the page you are looking for! . Click here to go back home . ",
    "url": "/404.html#404-not-found",
    
    "relUrl": "/404.html#404-not-found"
  },"1": {
    "doc": "Not found",
    "title": "Not found",
    "content": " ",
    "url": "/404.html",
    
    "relUrl": "/404.html"
  },"2": {
    "doc": "Willkommen!",
    "title": "Willkommen!",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"3": {
    "doc": "Einführung",
    "title": "Einführung",
    "content": "Elastic Cloud Enterprise by WIIT (ECE) ist eine von WIIT gehostete und betriebene Cloud-Plattform, auf der Sie Ihre Elastic-Workloads in Form sogenannter Deployments betreiben können. Ein Deployment ist ein isoliertes Elastic Cluster, das mit ECE verwaltet wird. Es besteht aus mehreren Komponenten des Elastic Stacks. Für jedes Deployment können Sie folgendes individuell festlegen: . | Welche der unten aufgeführten Elastic Stack Komponenten verwendet werden sollen | Die Größe der jeweiligen Komponenten | Die Anzahl der Availability Zones (1 bis 3) über die sie verteilt werden sollen | Welche der von WIIT bereitgestellten Versionen für den Elastic Stack verwendet werden soll | . Jedes Deployment bekommt eigene, aus dem Internet erreichbare URLs für Elasticsearch, Kibana, Fleet und Enterprise Search. ",
    "url": "/ece/intro",
    
    "relUrl": "/ece/intro"
  },"4": {
    "doc": "Einführung",
    "title": "Komponenten des Elastic Stacks",
    "content": "ECE enthält alle aktuell von Elastic angebotenen Server-Komponenten: . | Elasticsearch in den 4 Tiers Hot, Warm, Cold und Frozen sowie Master Nodes und Coordinating/Ingest Nodes | Kibana | Machine Learning | Integration Server (Fleet und APM) | Enterprise Search | . Die Definition der Elasticsearch Data Tier finden Sie hier: https://www.elastic.co/guide/en/elasticsearch/reference/current/data-tiers.html. ",
    "url": "/ece/intro#komponenten-des-elastic-stacks",
    
    "relUrl": "/ece/intro#komponenten-des-elastic-stacks"
  },"5": {
    "doc": "Einführung",
    "title": "Funktionsumfang der Komponenten",
    "content": "Alle ECE Deployments sind mit der Elastic Enterprise Lizenz ausgestattet, d.h. alle Komponenten enthalten alle Funktionalitäten der höchstwertigen Elastic-Lizenz. Dazu gehören Machine Learning, durchsuchbare Snapshots im Frozen Tier (eine sehr kostengünstige Variante für die Speicherung selten abgerufener Daten), Cross-Cluster-Search, alle SIEM (Security Information and Event Management) Funktionen des Elastic Stacks, Watcher und Alerting, der neue AI Assistant u.v.m. Einen Überblick über die Komponenten finden Sie hier https://www.elastic.co/de/pricing/. Eine detaillierte Auflistung erhalten Sie hier: https://www.elastic.co/de/subscriptions. ",
    "url": "/ece/intro#funktionsumfang-der-komponenten",
    
    "relUrl": "/ece/intro#funktionsumfang-der-komponenten"
  },"6": {
    "doc": "Einführung",
    "title": "Berechnung der Größe des Deployments und Preismodell",
    "content": "Die Größe des Deployments wird in GB RAM gemessen. Jeder Elastic Komponente wird nach einem bestimmten Schlüssel zum RAM entsprechend CPU und Storage zugeordnet. Dieser Zuordnungsschlüssel kann pro Komponente verschieden sein. Für die meisten Anwendungsfälle ist der Arbeitsspeicher der bestimmende Faktor für die Performance eines Elastic Workloads. Da die Zuordnung nach festen Schlüsseln erfolgt, reicht ein Parameter (GB RAM) aus, um die anderen Parameter (CPU, Storage) zu berechnen. Dadurch wird auch das Preismodell sehr transparent und leicht verständlich. Sie müssen lediglich den Arbeitsspeicher des Deployments kennen, um Ihre Kosten zu berechnen. Sie zahlen für die genutzten GB RAM Ihres Deployments. Die Erfassung der Nutzung und die Abrechnung erfolgen stundengenau. Zusätzlich fallen Kosten für den genutzten Snapshot-Storage in unserem Object Storage Cluster an. Der (ein- und ausgehende) Datenverkehr ist kostenfrei. ",
    "url": "/ece/intro#berechnung-der-gr%C3%B6%C3%9Fe-des-deployments-und-preismodell",
    
    "relUrl": "/ece/intro#berechnung-der-größe-des-deployments-und-preismodell"
  },"7": {
    "doc": "Einführung",
    "title": "Autoscaling Feature",
    "content": "Die Komponenten Elasticsearch und Machine Learning verfügen über ein sogenanntes Autoscaling. Dieses Feature haben Sie bei einem selbst gehosteten Elastic Stack in dieser Form nicht oder müssten es selbst entwickeln. Mit Autoscaling überwacht ECE den genutzten Festplattenspeicher der Elasticsearch-Komponenten und fügt bei Überschreiten eines Schwellenwertes automatisch weitere Ressourcen hinzu. Dadurch müssen Sie auch bei Wachstum Ihrer Datenmenge keine Ausfälle wegen voller Festplatten o.ä. befürchten. Machine Learning Workloads werden in der notwendigen Größe nur dann gestartet, wenn sie auch benötigt werden. Die Ressourcen in Ihrem Deployment und somit auch Ihre Kosten wachsen mit Ihrer Datenmenge. Dadurch wird eine effiziente Ausnutzung der Ressourcen und eine kosteneffiziente Abbildung Ihrer Workloads auf der ECE Plattform möglich. Für jede Komponente mit Autoscaling lassen sich auch Obergrenzen definieren bis zu denen automatisch hochskaliert wird. Somit gerät z.B. bei Fehlkonfigurationen Ihr Rechnungsbetrag nicht außer Kontrolle. ",
    "url": "/ece/intro#autoscaling-feature",
    
    "relUrl": "/ece/intro#autoscaling-feature"
  },"8": {
    "doc": "Einführung",
    "title": "Ihre Vorteile durch ECE von WIIT",
    "content": "Mit der ECE-Lösung von WIIT erhalten Sie alle Funktionalitäten der Software von Elastic, dem Marktführer bei Suche, SIEM und Observability, inklusive Enterprise-Lizenz. WIIT hostet ausschließlich in hochsicheren deutschen Rechenzentren und leistet alle Betriebsaufgaben von Deutschland aus. Sie erhalten alles aus einer Hand von einem deutschen Vertragspartner. Im Gegensatz zu einem “klassischen” Deployment des Elastic Stacks in einem Cluster mit virtuellen Maschinen, die von der jeweiligen Cloud-Plattform auf vorgegebene Größen beschränkt sind, werden bei ECE Docker-Container verwendet. Diese können bei Bedarf automatisch skalieren. Durch das feingranulare Autoscaling können Sie die Ressourcen effizient ausnutzen und die Workloads kosteneffizient betreiben. ",
    "url": "/ece/intro#ihre-vorteile-durch-ece-von-wiit",
    
    "relUrl": "/ece/intro#ihre-vorteile-durch-ece-von-wiit"
  },"9": {
    "doc": "Einführung",
    "title": "Leistungen der WIIT",
    "content": ". | WIIT stellt in seinen sicheren und zertifizierten Rechenzentren in Deutschland die Cloud-Umgebung bereit, auf denen Ihre Elastic Workloads laufen. | WIIT gibt Ihnen die Möglichkeit, zwecks hoher Verfügbarkeit Ihre Workloads über 3 Verfügbarkeitszonen (Availability Zones) zu verteilen. | WIIT kümmert sich darum, dass genügend Ressourcen für die Skalierung Ihres Deployments zur Verfügung stehen. | WIIT überwacht die ECE-Umgebung und sorgt dafür, dass sie verfügbar ist (99,85 %). Auch außerhalb der Bürozeiten wird automatisch eine Rufbereitschaft alarmiert, um eventuelle Probleme an der ECE-Plattform möglichst schnell zu beheben. | WIIT sorgt für regelmäßige Updates und Security-Patches der Server. | WIIT stellt neue Versionen des Elastic Stacks zur Installation bereit, meist nur wenige Werktage nachdem diese von Elastic veröffentlicht wurden. Wir führen jedoch keine automatischen oder unaufgeforderten Versions-Upgrades Ihres Deployments durch (ausgenommen EOL-Versionen). | WIIT stellt aus dem Internet erreichbare URLs für alle Deployments inklusive TLS-Zertifikat für die verschlüsselte Kommunikation zur Verfügung (kundeneigene Zertifikate sind leider nicht möglich). | WIIT stellt Backup- und Snapshot-Storage im redundanten WIIT Object Storage Cluster zur Verfügung. Für jedes Deployment wird standardmäßig eine (von Ihnen änderbare) Snapshot Policy aktiviert, über die automatisch in regelmäßigen Abständen eine Datensicherung Ihrer Elasticsearch-Daten in den Object Storage erfolgt. Dadurch sind Sie sehr gut gegen Datenverlust geschützt. | All Ihre Deployments in der ECE-Cloud von WIIT sind mit der Elastic Enterprise Lizenz ausgestattet. Sie müssen keine Lizenzen bei Elastic einkaufen, Sie erhalten von WIIT alles aus einer Hand. | . ",
    "url": "/ece/intro#leistungen-der-wiit",
    
    "relUrl": "/ece/intro#leistungen-der-wiit"
  },"10": {
    "doc": "Einführung",
    "title": "Ihre Verantwortlichkeit als Kunde",
    "content": "Als Kunde sind Sie für alles verantwortlich was innerhalb Ihres Deployments passiert, insbesondere: . | Das Einsammeln von Daten aus Client-Systemen und das Laden dieser Daten in Elasticsearch | Die Konfiguration von Lifecycle Policies | Das Anlegen von Nutzern und die Vergabe von Berechtigungen | Die Überwachung der Shard-Gesundheit und bei Bedarf deren Reparatur (s. hierzu die Dokumentation von elastic.co) | Konfiguration von Alerts - sofern gewünscht | Wiederherstellung von Snapshots bei Bedarf | Überwachung der Performance Ihres Deployments, sofern Sie dafür spezielle Anforderungen haben. Welche Performance für Ihren Anwendungsfall akzeptabel ist, können wir nicht wissen. | . Weitere Bestimmungen entnehmen Sie den Allgemeinen Geschäftsbedingungen der WIIT. ",
    "url": "/ece/intro#ihre-verantwortlichkeit-als-kunde",
    
    "relUrl": "/ece/intro#ihre-verantwortlichkeit-als-kunde"
  },"11": {
    "doc": "Einführung",
    "title": "Service Description",
    "content": "Für detailliertere Informationen kontaktieren Sie bitte unseren Vertrieb. ",
    "url": "/ece/intro#service-description",
    
    "relUrl": "/ece/intro#service-description"
  },"12": {
    "doc": "Einführung",
    "title": "Auszuführende Schritte nach dem ECE-Deployment",
    "content": "Wir empfehlen Ihnen, folgende Schritte durchzuführen, um die Sicherheit Ihres ECE Deployments zu erhöhen und Daten in Elastic zu laden: . Erhöhung der Sicherheit . | Legen Sie in Kibana die benötigten Spaces, Rollen und Nutzer an. Weitere Informationen dazu finden Sie hier: https://www.elastic.co/guide/en/kibana/current/tutorial-secure-access-to-kibana.html. | Verwenden Sie nicht den elastic Superuser für die tägliche Arbeit in Kibana oder um Daten anzuliefern. | Sofern gewünscht, konfigurieren Sie Single Sign-On mittels SAML, LDAP, Active Directory, OpenID Connect oder Kerberos sowie dynamische role mappings, wenn Sie Role-based oder Attribute-based Access Control benötigen. Für die Einstellung der Parameter in Ihrem Deployment wenden Sie sich vorläufig an unseren Support. | Falls gewünscht, lassen Sie durch unseren Support traffic filter konfigurieren. Damit können Sie unerwünschte Zugriffe auf Ihr Deployment verhindern. | Legen Sie ggf. (falls Sie nicht den Agent verwenden, s.u.) Service Accounts für die Datenanlieferung an und erzeugen Sie dafür API Keys. Weitere Informationen finden Sie hier: Grant access using API keys. | . Laden von Daten in Elastic . | Laden Sie Daten in Ihren Cluster. Wir empfehlen hierfür den Elastic Agent in Zusammenarbeit mit dem Fleet Server. Eine detaillierte Anleitung finden Sie hier: Laden von Daten in ECE. | Um Verbindungsprobleme zu ihrem Cluster zu vermeiden, stellen sie sicher, Sniffingin den verwendeten Elasticsearch Clients zu deaktivieren (z.B. fluentd). Elastic Agent, Filebeat etc. verhalten sich automatisch richtig. | Falls Sie Daten aus Ihrem bestehenden Elastic Cluster in Ihr neues Deployment migrieren möchten, finden Sie einige Möglichkeiten in der Elastic Dokumentation: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-migrating-data.html. | Prüfen Sie in Kibana Ihre Index Templates und Lifecycle Policies, damit Sie Ihre Daten den von Ihnen gewünschten Tiers Hot, Warm, Cold, Frozen (und Delete) zuweisen können. Die von Elastic automatisch angelegten Lifecycle Policies haben häufig nur ein Hot Tier ohne Ablaufdatum konfiguriert. Weitere Informationen finden Sie hier: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html. Dies gilt insbesondere für die standardmäßig deployten Lifecycle Policies logs und metrics, die bei Verwendung des Elastic Agents die relevantesten Policies sind. | . Ein Beispiel für die Konfiguration einer Policy, bei der die Daten nach 70 Tagen gelöscht werden, finden Sie hier: Aktualisieren der Standard Lifecycle Policies. ",
    "url": "/ece/intro#auszuf%C3%BChrende-schritte-nach-dem-ece-deployment",
    
    "relUrl": "/ece/intro#auszuführende-schritte-nach-dem-ece-deployment"
  },"13": {
    "doc": "Einführung",
    "title": "Support-Leistungen von WIIT",
    "content": "Bei Support-Anfragen können Sie sich an unseren Support wenden, der werktags von 8-18 Uhr erreichbar ist. Der Support von WIIT kann Sie bei allen Themen unterstützen, die im Rahmen des ECE-Dienstes in der Verantwortung von WIIT liegen: . | Verfügbarkeit und Erreichbarkeit der Plattform | Fragen zur Verwendung von ECE | Fehler in der Elastic Software (wir würden diese entsprechend an Elastic weiterleiten) | . Während wir daran arbeiten, Ihnen in Zukunft eine Self-Service-Oberfläche für die Verwaltung Ihres ECE-Deployments zur Verfügung zu stellen, müssen Sie vorläufig folgende Einstellungen an Ihrem Deployment über unseren Support beauftragen. Während dieser Zeit sind die Leistungen auch im Support-Umfang enthalten: . | Erweiterte Konfiguration des Deployments (d.h. Anpassungen an der elasticsearch.yml bzw. kibana.yml) | Einträge im Elasticsearch Keystore | Upgrade der Elastic-Version Ihres Deployments | Konfiguration von IP-Filtern für Ihr Deployment | Konfiguration von Vertrauensbeziehungen zwischen Deployments, damit Sie Cross-Cluster Search bzw. Cross-Cluster Replication verwenden können | . Nicht durch den Support der WIIT abgedeckt sind: . | Generelle Beratung zur Elastic Software oder deren Verwendung in Ihrem Deployment | . Ggf. können wir auf Anfrage einige der nicht abgedeckten Leistungen als vergütete Consulting-Leistung anbieten. Bitte wenden Sie sich hierzu an den Vertrieb. Ebenso können wir keine 24/7-Rufbereitschaft für Kunden anbieten. Dies ist auch nicht nötig, da die ordnungsgemäße Funktionalität der ECE-Plattform von einem automatisierten Monitoring überwacht wird. Wird eine Einschränkung des Service festgestellt, wird unsere interne Rufbereitschaft automatisch alarmiert und beginnt mit der Problembehebung. ",
    "url": "/ece/intro#support-leistungen-von-wiit",
    
    "relUrl": "/ece/intro#support-leistungen-von-wiit"
  },"14": {
    "doc": "Laden von Daten in Elastic",
    "title": "Laden von Daten in Elastic",
    "content": "Nachdem Ihr Deployment eingerichtet wurde, können Sie mit Elastic Agent und Fleet Daten aus Ihren Logdaten-Quellen (Log Sources) an Ihr Deployment senden. Dieses Dokument beschreibt die nötigen Schritte. ",
    "url": "/ece/shipdata/",
    
    "relUrl": "/ece/shipdata/"
  },"15": {
    "doc": "Laden von Daten in Elastic",
    "title": "Begriffsklärung",
    "content": "Bevor wir beginnen, möchten wir einige Begriffe erklären, die in diesem Dokument verwendet werden. | Begriff | Definition | . | Elastic Agent | Komponente für die Sammlung von Log- und Metrikdaten, die Client-seitig auf den Logdaten-Quellen installiert wird. Es handelt sich um die einzige Komponente, die installiert werden muss, da sie alle Beats mitbringt und diese mit den zugehörigen Konfigurationen auf dem Client (Klienten) verwaltet. | . | Fleet Server | Komponente im Elastic-Deployment, mit der die Agenten verwaltet werden. Alle Einstellungen in Fleet werden in Kibana vorgenommen. | . | Integration | In diesem Zusammenhang handelt es sich um eine (meist durch Elastic selbst) vorkonfigurierte Zusammenstellung von Agenteneinstellungen, Ingest-Pipelines und Kibana-Dashboards für einen bestimmten Logdaten-Quellen-Typ (z. B. Tomcat, Nginx, MySQL, Cisco Komponenten, …). Für die gängigsten Logdaten-Quellen sind Integrationen verfügbar (siehe: https://www.elastic.co/de/integrations/data-integrations). | . | Agent Policy | Eine Sammlung von Einstellungen und Integrationen in Form einer Richtlinie (Policy), die dem Agenten und den darunter liegenden Beats mitteilt, welche Daten wo gesammelt werden sollen. Sie benötigen für jeden Quellentyp eine separate Agentenrichtlinie (Agent Policy). Wenn beispielsweise alle Ihre MySQL-Datenbanken ihre Logdateien im selben lokalen Pfad speichern (was sie ohnehin tun sollten), reicht eine Agentenrichtlinie für MySQL-Datenbanken aus. Eine Agentenrichtlinie kann mehrere Integrationen enthalten. Somit kann ein Agent Logdateien von mehreren Anwendungen auf derselben Quelle sammeln. Sie können nur eine Richtlinie pro Agent haben. Angenommen, Sie haben auf einigen Hosts nur Apache httpd und auf anderen Hosts httpd und Tomcat installiert, dann benötigen Sie mindestens zwei Agentenrichtlinien: eine für httpd und eine für httpd+tomcat. | . | Enrollment Token | Wird vom Agenten beim Start auf dem Client benötigt. Das Token erfüllt zwei Zwecke: Erstens, ermöglicht es die Erstauthentifizierung des Agenten gegenüber den Fleet- und Elastic-Servern. Zweitens, verweist es auf genau eine Agentenrichtlinie, sodass der Fleet-Server dem Agenten mitteilen kann, von welchen Quellen er Daten sammeln soll. Jede Agentenrichtlinie verfügt über mindestens ein Registrierungstoken. Registrierungstokens sind von Natur aus sensible Daten und sollten daher auf sichere Weise gespeichert werden. | . ",
    "url": "/ece/shipdata/#begriffskl%C3%A4rung",
    
    "relUrl": "/ece/shipdata/#begriffsklärung"
  },"16": {
    "doc": "Laden von Daten in Elastic",
    "title": "Erstellen der Agentenrichtlinie",
    "content": "Da die Agentenrichtlinie für den Agenten definiert, welche Daten erfasst werden sollen, muss diese als erstes definiert werden. Definieren Sie am besten eine Agentenrichtlinie pro Quellentyp. Gehen Sie in Kibana zu Fleet → Agent Policies. Erstellen Sie eine neue Agentenrichtlinie und geben Sie ihr einen aussagekräftigen Namen (meist ist es nicht nötig, die Advanced Optionen zu ändern). Klicken Sie dann auf Ihre neue Integration. Sie werden sehen, dass bereits eine Integration, die Systemintegration, vorkonfiguriert ist. Diese sammelt die System-Logs und Metriken vom Client-Computer. Sie müssen sich keine Sorgen wegen des Betriebssystems machen. Egal ob Linux, Windows oder MacOS, der Agent findet das selbst heraus und konfiguriert sich korrekt. Fügen Sie je nach Art der Quelle, von der Sie Daten sammeln möchten, weitere Integrationen hinzu, wie z.B. Apache Produkte: . Durch einen Klick auf die entsprechende Kachel erhalten Sie nicht nur einen Überblick darüber, was die Integration beinhaltet. Sie können, nachdem Sie “Add Integration” ausgewählt haben, auf der nachfolgenden Bildschirmseite auch die Einstellungen anpassen. Der Inhalt der Seite ist vom Typ der Integration abhängig. Wenn Sie Ihre MySQL-Logdaten beispielsweise nicht im Standardpfad speichern, können Sie dies hier ändern. Es wird wahrscheinlich ungefähr wie in dem folgenden Bild aussehen: . ",
    "url": "/ece/shipdata/#erstellen-der-agentenrichtlinie",
    
    "relUrl": "/ece/shipdata/#erstellen-der-agentenrichtlinie"
  },"17": {
    "doc": "Laden von Daten in Elastic",
    "title": "Abrufen des Enrollment Tokens",
    "content": "Für jede Agentenrichtlinie wird automatisch ein Enrollment Token (Registrierungstoken) mit dem Namen Default erstellt. Dieses können Sie jederzeit in Kibana abrufen. ",
    "url": "/ece/shipdata/#abrufen-des-enrollment-tokens",
    
    "relUrl": "/ece/shipdata/#abrufen-des-enrollment-tokens"
  },"18": {
    "doc": "Laden von Daten in Elastic",
    "title": "Installieren des Agenten auf dem Content-Host",
    "content": "In Kibana gibt es einen Wizard, der Sie bei der Installation des Agenten auf den Clients unterstützt. Für den ersten Rollout dieser Art starten Sie die Installation in Kibana → Fleet → Agents → Add Agent. Bei mehreren Agenten mit der selben Agentenrichtlinie können Sie die Installation auch automatisieren, da die Installationsbefehle und -parameter identisch sind. Wählen Sie die richtige Agentenrichtlinie für den Host aus, auf dem Sie die Installation durchführen möchten. Lassen Sie Enroll in Fleet aktiviert. Kibana präsentiert Ihnen dann verschiedene Optionen für die Befehle, die auf dem Client-Computer auszuführen sind: . Der URL-Parameter ist automatisch auf das Deployment eingestellt, in dem Sie sich gerade befinden, und das Enrollment-Token verweist auf die Agentenrichtlinie. Sobald diese Befehle auf dem Client-Computer ausgeführt werden, registriert sich der Agent beim Fleet-Server und erscheint in Kibana in der Agentenliste. Alle nachfolgenden Installationen von Agenten, die dieselbe Agentenrichtlinie verwenden, verfügen über denselben Befehls- und Parametersatz. Sie müssen den Assistenten nicht erneut durchlaufen. Sie können somit diesen Installationsteil im Tool Ihrer Wahl automatisieren. ",
    "url": "/ece/shipdata/#installieren-des-agenten-auf-dem-content-host",
    
    "relUrl": "/ece/shipdata/#installieren-des-agenten-auf-dem-content-host"
  },"19": {
    "doc": "Laden von Daten in Elastic",
    "title": "Auszuführende Schritte nach der Installation",
    "content": "Überprüfen Sie unbedingt die Lifecycle Policies (ILM), einschließlich aller vom System bereitgestellten Richtlinien wie logs und metrics. Standardmäßig sind diese mit einer unbegrenzten Lebensdauer versehen. Wir empfehlen, das zu ändern. Entdecken Sie die ansprechenden neuen Dashboards, die mit jeder zusätzlichen Integration geliefert werden. Machen Sie sich mit dem Fleet-Menü in Kibana vertraut. Suchen Sie nach möglichen Aktualisierungen für die Agentenversionen oder Integrationen. Sie können all dies im Fleet-Menü in Kibana erledigen, dafür müssen Sie sich nicht auf den Client-Computern einloggen. ",
    "url": "/ece/shipdata/#auszuf%C3%BChrende-schritte-nach-der-installation",
    
    "relUrl": "/ece/shipdata/#auszuführende-schritte-nach-der-installation"
  },"20": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Anpassen von Lifecycle Policies",
    "content": "Dieses Dokument beschreibt, wie Sie die (Standard) Index Lifecycle Policies (ILM) anpassen können. | Begriffsklärung | Auffinden der zugeordneten ILM-Richtlinie | Erstellen einer neuen ILM Richtlinie . | Verwenden der Benutzeroberfläche (UI) . | Hot Phase | Warm Phase | Cold Phase | Frozen Phase | Delete Phase | . | Verwenden von Dev Tools | . | Zuweisen der Richtlinie zu einem Index | Rollover des Data Streams | . ",
    "url": "/ece/updateilm/",
    
    "relUrl": "/ece/updateilm/"
  },"21": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Begriffsklärung",
    "content": "Bevor wir beginnen, möchten wir einige Begriffe erklären, die in diesem Dokument verwendet werden. | Begriff | Definition | . | ILM | Abkürzung für Index Lifecycle Management. Bestimmt den Lebenszyklus Ihrer Daten sowie deren Zuordnung zu den verschiedenen Phasen. Wird in Richtlinien definiert (“Index Lifecycle Policies”). | . | Data stream | Der Datenstrom ist eine Sammlung einzelner Indizes. Man könnte es fast als kleines DNS (Domain Name System) für das Routing von Anfragen an Indizes betrachten. Ein Datenstrom kann mehrere Indizes enthalten. | . | Rollover | Prozess der (automatischen) Erstellung eines neuen Indexes innerhalb eines Datenstroms in der Hot Phase, sodass ein einzelner Index nicht unbegrenzt wächst. Er ist in der Hot Phase der ILM-Richtlinie definiert. | . ",
    "url": "/ece/updateilm/#begriffskl%C3%A4rung",
    
    "relUrl": "/ece/updateilm/#begriffsklärung"
  },"22": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Auffinden der zugeordneten ILM-Richtlinie",
    "content": "Bei Verwendung des Elastic Agents und Fleet gelten Standardrichtlinien für die Datenströme. Diese Richtlinien sind einfach gehalten und beinhalten nur wenige Phasen, oft sogar nur die Hot Phase. Die auf die Datenströme angewendete Richtlinie finden Sie in Kibana. | Öffnen Sie Kibana. | Wählen Sie im Menü Stack Management aus. | Öffnen Sie Index Management und gehen Sie zu Data Streams. | Im Popup rechts sehen Sie die zugeordnete ILM Richtlinie. | . ",
    "url": "/ece/updateilm/#auffinden-der-zugeordneten-ilm-richtlinie",
    
    "relUrl": "/ece/updateilm/#auffinden-der-zugeordneten-ilm-richtlinie"
  },"23": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Erstellen einer neuen ILM Richtlinie",
    "content": "Verwenden der Benutzeroberfläche (UI) . | Um eine neue ILM-Richtlinie zu erstellen, wählen Sie Stack Management. | Wählen Sie Index Lifecycle Management und klicken Sie auf Create Policy. | Geben Sie einen neuen Namen für die Richtlinie ein. | . Hot Phase . In der Hot Phase werden alle Daten gespeichert, die neu indiziert werden und auf die ständig zugegriffen wird. Sie ist immer die erste Phase und daher verpflichtend. In unserem Beispiel haben wir den Schalter “Use recommended default” deaktiviert, um das maximale Alter der Daten bis zum Rollover vom Standardwert von 30 Tagen auf 7 Tage zu verkürzen. Warm Phase . Nachdem 7 Tage lang Daten gesammelt wurden oder der Index eine Größe von 50 GB erreicht hat, sollen die Indizes unmittelbar in die zweite Phase, die sogenannte Warm Phase, übergehen. In dieser Stufe wird die Priorität der Indizes reduziert und auf “schreibgeschützt” gesetzt. Cold Phase . Nach 7 Tagen in der Warm Phase gehen die Daten in die nächste Phase über, die sogenannte Cold Phase. In dieser Phase können durchsuchbare Snapshots verwendet werden, um den Platzbedarf auf den Elastic Instanzen zu verringern. Die Snapshots sind im Object Storage gespeichert, der selbst schon hochverfügbar ausgelegt ist. Daher können wir die Anzahl der Replikate und die Priorität auf Null setzen. Frozen Phase . In dieser Phase werden die Daten von den Elastic Knoten entfernt und nur ein kleiner Cache verbleibt hier. Wenn auf die Daten zugegriffen wird, werden diese aus dem Snapshot geholt und bereitgestellt. Delete Phase . In dieser Phase werden die Indizes entfernt, die ein bestimmtes Alter erreicht haben. Verwenden von Dev Tools . Alternativ kann auch in den Dev Tools von Kibana eine neue ILM-Richtlinie erstellt werden. Die Konsole finden Sie in Kibana unter Management → Dev Tools. PUT _ilm/policy/&lt;add-some-name-here&gt; { \"policy\": { \"phases\": { \"hot\": { \"min_age\": \"0ms\", \"actions\": { \"rollover\": { \"max_primary_shard_size\": \"50gb\", \"max_age\": \"7d\" }, \"set_priority\": { \"priority\": 100 } } }, \"warm\": { \"min_age\": \"0d\", \"actions\": { \"set_priority\": { \"priority\": 50 }, \"readonly\": {} } }, \"cold\": { \"min_age\": \"7d\", \"actions\": { \"readonly\": {}, \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true }, \"set_priority\": { \"priority\": 0 }, \"allocate\": { \"number_of_replicas\": 0 } } }, \"frozen\": { \"min_age\": \"14d\", \"actions\": { \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true } } }, \"delete\": { \"min_age\": \"70d\", \"actions\": { \"delete\": { \"delete_searchable_snapshot\": true } } } } } } . ",
    "url": "/ece/updateilm/#erstellen-einer-neuen-ilm-richtlinie",
    
    "relUrl": "/ece/updateilm/#erstellen-einer-neuen-ilm-richtlinie"
  },"24": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Zuweisen der Richtlinie zu einem Index",
    "content": "Die neu erstellte Richtlinie kann dann über das Menü Index Lifecycle Policies z.B. einem Index Template zugewiesen werden. ",
    "url": "/ece/updateilm/#zuweisen-der-richtlinie-zu-einem-index",
    
    "relUrl": "/ece/updateilm/#zuweisen-der-richtlinie-zu-einem-index"
  },"25": {
    "doc": "Anpassen von Lifecycle Policies",
    "title": "Rollover des Data Streams",
    "content": "Damit die neue ILM-Richtlinie sofort benutzt wird, muss ein Rollover für den Datenstrom angestoßen werden. Dies kann auch in den Dev Tools von Kibana erfolgen, hier am Beispiel für den Datenstrom auditbeat-8.5.0: . POST auditbeat-8.5.0/_rollover . ",
    "url": "/ece/updateilm/#rollover-des-data-streams",
    
    "relUrl": "/ece/updateilm/#rollover-des-data-streams"
  },"26": {
    "doc": "Produkt Übersicht",
    "title": "Produkt Übersicht",
    "content": "Die ONCITE Open Edition bietet eine benutzerfreundliche virtuelle Maschine (VM) und eine Container-Plattform auf Open-Source-Basis. Sie ist skalierbar von der Größe einer Edge bis zur Größe eines Rechenzentrums und bietet volle Datenhoheit sowie Echtzeitfähigkeiten. ",
    "url": "/edge/productoverview/",
    
    "relUrl": "/edge/productoverview/"
  },"27": {
    "doc": "Produkt Übersicht",
    "title": "Architektur",
    "content": "Um virtuelle Maschinen auf der Edge bereitzustellen, verwenden wir OpenStack. Der bereitgestellte Speicher wird von CEPH verwaltet. Um die virtuellen Maschinen zu erstellen und zu verwalten, kann der Kunde das OperationsCenter verwenden, das über LDAP für Single Sign-On verbunden ist. Administratoren können das OpenStack Horizon verwenden, das nicht mit LDAP verbunden ist. ",
    "url": "/edge/productoverview/#architektur",
    
    "relUrl": "/edge/productoverview/#architektur"
  },"28": {
    "doc": "Operations Center",
    "title": "Operations Center",
    "content": " ",
    "url": "/edge/operationscenter/",
    
    "relUrl": "/edge/operationscenter/"
  },"29": {
    "doc": "Release Notes",
    "title": "Release Notes",
    "content": " ",
    "url": "/edge/operationscenter/release_notes/",
    
    "relUrl": "/edge/operationscenter/release_notes/"
  },"30": {
    "doc": "Release Notes",
    "title": "1.5.14",
    "content": "Verbesserungen . | VPNaaS: Überwachung der VPN Server und Gateway VMs verbessert | . ",
    "url": "/edge/operationscenter/release_notes/#1514",
    
    "relUrl": "/edge/operationscenter/release_notes/#1514"
  },"31": {
    "doc": "Release Notes",
    "title": "1.5.13",
    "content": "Fehlerbehebung . | Limitierung von Projektnamen auf 25 Character (vorher 50) | . ",
    "url": "/edge/operationscenter/release_notes/#1513",
    
    "relUrl": "/edge/operationscenter/release_notes/#1513"
  },"32": {
    "doc": "Release Notes",
    "title": "1.5.11",
    "content": "Fehlerbehebung: . | Backups sind nicht mehr wegen alte VMs mit Leerzeichen am Anfang blockiert | . ",
    "url": "/edge/operationscenter/release_notes/#1511",
    
    "relUrl": "/edge/operationscenter/release_notes/#1511"
  },"33": {
    "doc": "Release Notes",
    "title": "1.5.9",
    "content": "Verbesserungen: . | Updates für den Backend Ingress | . ",
    "url": "/edge/operationscenter/release_notes/#159",
    
    "relUrl": "/edge/operationscenter/release_notes/#159"
  },"34": {
    "doc": "Release Notes",
    "title": "1.5.6",
    "content": "Änderungen Resource Overview: . Fehlerbehebung: . | Ports werden jetzt alle im Tooltip angezeigt und nicht mehr gekürzt Features: | IP zeigt nun sowohl interne als auch floating IP an | . ",
    "url": "/edge/operationscenter/release_notes/#156",
    
    "relUrl": "/edge/operationscenter/release_notes/#156"
  },"35": {
    "doc": "Release Notes",
    "title": "1.5.4",
    "content": "Features: . | Flavor für VMs können für das Operations Center verborgen werden. Um einen Flavor zu verbergen muss im Openstack die Metadata mit einen Custom Feld visibility ergänzt und dieses auf false gesetzt werden. Der Flavor ist jetzt nicht mehr in Operations Center sichtbar. Hinweis: Der Flavor ist nur in der Auswahl für neue VMs verborgen. VMs die den Flavor bereits nutzen sind davon nicht beeinflusst und können weiterhin im Operations Center verwaltet werden. | . ",
    "url": "/edge/operationscenter/release_notes/#154",
    
    "relUrl": "/edge/operationscenter/release_notes/#154"
  },"36": {
    "doc": "Release Notes",
    "title": "1.5.3",
    "content": "Fehlerbehebung: . | VPNaaS widerrufene Zertifikate wurden nicht korrekt an die CRL angehängt | . Features: . | Passwort Generator und Tooltip für VM Passwörter hinzugefügt | . Verbesserungen: . | Anlegen externer User mit frauenhofer.de Email blockiert und Tooltip hierfür hinzugefügt | VPNaas Zertifikates Gültigkeit auf 10 Jahre erhöht | Limitierung der Projektnamen länge um VPNaaS Probleme zu vermeiden | Update der VPNaaS Version um für die Unterstützung neuer OpenVPN clients | Refresh Button in der Resource Overview ruf IP und Status Information der VMs ab | Verbesserte Backup Performance | . ",
    "url": "/edge/operationscenter/release_notes/#153",
    
    "relUrl": "/edge/operationscenter/release_notes/#153"
  },"37": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/edge/operationscenter/vpnaas/",
    
    "relUrl": "/edge/operationscenter/vpnaas/"
  },"38": {
    "doc": "VPN as a Service",
    "title": "Übersicht",
    "content": "Dieser Abschnitt veranschaulicht, wie die Nutzer mit dem Kundenprojekte von OpenStack über das OpenStack-Projekt-VPN-Gateway kommunizieren. Das OpenStack-Projekt-VPN-Gateway verwendet eine einzige öffentliche IP-Adresse. Es authentifiziert und leitet den Datenverkehr über verschiedene Ports zu den jeweiligen VPN-Servern weiter. Typischerweise hat jedes Projekt seinen eigenen VPN-Server. In dieser Konfiguration hat jedes VPN-Gateway eine einzige öffentliche IP-Adresse, die für alle VPN-Server gültig ist. ",
    "url": "/edge/operationscenter/vpnaas/#%C3%BCbersicht",
    
    "relUrl": "/edge/operationscenter/vpnaas/#übersicht"
  },"39": {
    "doc": "VPN as a Service",
    "title": "Zweck",
    "content": "GEC bietet eine VPNaaS-Lösung an, die es dem Kunden ermöglicht, ihre Anwendungen und Systeme mit den Systemen von Projektpartner zu integrieren. Externer Partner können Einzelpersonen sein, die mit ihren Computern oder Kommunikationssystemen auf das Projekt zugreifen. ",
    "url": "/edge/operationscenter/vpnaas/#zweck",
    
    "relUrl": "/edge/operationscenter/vpnaas/#zweck"
  },"40": {
    "doc": "VPN as a Service",
    "title": "Anforderungen",
    "content": "Die VPNaaS-Lösung hat folgende Anforderungen: . | Das Gateway benötigt eine öffentlich erreichbare IP welche mit der Floating-IP verbunden ist. | . ",
    "url": "/edge/operationscenter/vpnaas/#anforderungen",
    
    "relUrl": "/edge/operationscenter/vpnaas/#anforderungen"
  },"41": {
    "doc": "VPN as a Service",
    "title": "Einschränkungen",
    "content": "Die VPNaaS-Lösung unterliegt den folgenden Einschränkungen: . | Die Anzahl der VPN-Server wird durch den vordefinierten Portbereich im Gateway begrenzt. | Das Netzwerk unterstützt bis zu 250 Benutzer. | Konfigurationsänderungen am Gateway und Server sind nach der Erstellung nicht möglich; sie müssen neu erstellt werden. | Nach der Neuerstellung werden alle OVPN-Konfigurationen ungültig. | . ",
    "url": "/edge/operationscenter/vpnaas/#einschr%C3%A4nkungen",
    
    "relUrl": "/edge/operationscenter/vpnaas/#einschränkungen"
  },"42": {
    "doc": "VPN as a Service",
    "title": "Komponenten und Kommunikationsfluss",
    "content": "Dieser Abschnitt umreißt den Kommunikationsfluss zwischen dem Operations Center und den Kunden-VPN-Servern. Der Kunde initiiert eine Anfrage, und das VPN-Gateway authentifiziert und leitet die Anfragen an den jeweiligen VPN-Server weiter. VPN Gateway . Das VPN-Gateway dient als zentrale Managementzentrale für GEC. Es verwaltet VPN-Server und die Externe Verbindungen. Mit nur einer öffentlichen IP-Adresse stellen IPtable-Regeln sicher, dass Verbindungen zum richtigen VPN-Server geleitet werden. Diese Einrichtung erfordert ein Wide Area Network (WAN) und ein mit dem VPN-Gateway verbundenes VPN-Transfernetzwerk. Das VPN-Gateway verwaltet ausschließlich VPN-Server und Proxy-Verbindungen zu und von diesen Servern, um sicherzustellen, dass der Datenverkehr sein beabsichtigtes Ziel erreicht. VPN Server . Der VPN-Server verwendet OpenVPN, um VPN-Verbindungen mit dem Kundennetzwerk herzustellen. Diese Netzwerkverbindung arbeitet in einem eigenen Virtual Routing and Forwarding (VRF), um die Isolation zu verbessern, ohne zusätzliche Ports freizugeben. Externe Verbindungen werden über das VPN-Gateway geroutet. Der API-Server für den VPN-Server verwaltet den auf der Maschine laufenden OpenVPN-Server. ",
    "url": "/edge/operationscenter/vpnaas/#komponenten-und-kommunikationsfluss",
    
    "relUrl": "/edge/operationscenter/vpnaas/#komponenten-und-kommunikationsfluss"
  },"43": {
    "doc": "VPN as a Service",
    "title": "Sicherheit",
    "content": "WAN . | Nur die für die bestehende VPN-Server erforderlichen Ports sind geöffnet. | SSH ist deaktiviert. | Virtual Routing and Forwarding (VRF)-Trennung ist verfügbar. | Sicherheitsgruppen erlauben nur die VPN-Server-Portbereiche. | Firewall-Regeln sind vorhanden und werden bei Bedarf angepasst. | . VPNaaS . | Separate VRF wird für WAN verwendet. | Firewall-Regeln sind vorhanden und werden bei Bedarf angepasst. | SSH- und API-Zugriff werden bereitgestellt. | SSH-Schlüsselauthentifizierung wird verwendet. | . ",
    "url": "/edge/operationscenter/vpnaas/#sicherheit",
    
    "relUrl": "/edge/operationscenter/vpnaas/#sicherheit"
  },"44": {
    "doc": "VPN as a Service",
    "title": "Server- und Benutzerverwaltung",
    "content": "Serverstatus . | Klicken Sie auf die VPN-Ressource. | Überprüfen Sie den Status unter Eigenschaften. Die Konfigurationsbearbeitung ist nur möglich, wenn der Status aktiv ist. | . Benutzer hinzufügen . Hinweis: Der Projektinhaber wird automatisch für den Remotezugriff hinzugefügt und kann nicht entfernt werden. Voraussetzungen: Sie können nur Benutzer auswählen, die zuvor dem Projekt hinzugefügt wurden. | Gehen Sie zur VPN-Konfiguration. | Wählen Sie den gewünschten Benutzer aus, um ihn als VPN-Benutzer hinzuzufügen. | Speichern Sie die Auswahl. Der Benutzer wird sofort hinzugefügt. | . OVPN-Konfiguration und Passphrase erhalten . Hinweis: Sie können die OVPN-Konfiguration nur für sich selbst oder als Projektinhaber für externe Benutzer herunterladen und eine Passphrase anzeigen. Sie können die OVPN-Konfiguration oder Passphrasen für andere Benutzer nicht herunterladen. | Gehen Sie zur VPN-Konfiguration. | Klicken Sie auf den Welt-Icon-Link und wählen Sie zwischen Download und Passphrase. | . Benutzer löschen . | Klicken Sie auf das Löschsymbol neben dem Benutzer, den Sie löschen möchten. | Klicken Sie auf Speichern. | . ",
    "url": "/edge/operationscenter/vpnaas/#server--und-benutzerverwaltung",
    
    "relUrl": "/edge/operationscenter/vpnaas/#server--und-benutzerverwaltung"
  },"45": {
    "doc": "Openstack",
    "title": "Openstack",
    "content": "Openstack ist die Software die genutzt wird für den Infrastructure as a Service Layer. Alle VMs werden im Openstack erstellt, Openstack ist direkt mit CEPH verbunden worüber der Storage angeboten wird. ",
    "url": "/edge/openstack/",
    
    "relUrl": "/edge/openstack/"
  },"46": {
    "doc": "Openstack",
    "title": "Openstack - Horizon",
    "content": "Das Dashboard für Openstack ist Horizon. Horizon Login . Alle Administratoren erhalten bei der Übergabe einen Login. Horizon Instances . In der Instance Übersicht kann man die VMs ansehen die im ausgewählten Projekt laufen. Horizon Network . Im reiter Network kann man die Netzweke im aktuell Projekt sehen und die Provider Netzwerke. Die Provider Netzwerke sind die Netzwerke welche die VMs zum Kunden Netzwerk verbinden. Desweiteren kann man sich die Netzwerk Topologie ansehen, wie die VMs verbunden sind und wie die Netzwerke verbunden sind. Horizon New Network . Um ein neues Netzwerk anzulegen muss im Reiter “Network” auf “Create Network” geklickt werden. Dann kann man den Namen auswählen. Und man muss ein Subnet erstellen. Um mit dem Netzwerk zu arbeiten oder das Netzwerk nach aussen zu verbinden muss ein Router erstellt werden. Der Router muss dann mit einem Netzwerk verbunden werden. Das passiert durch die Erstellung eines Interfaces im Router, welches den Router dann mit einem Netzwerk verbindet. Um zwei Netzwerke zu verbinden braucht der Router ein Interface in beiden Netzwerken . ",
    "url": "/edge/openstack/#openstack---horizon",
    
    "relUrl": "/edge/openstack/#openstack---horizon"
  },"47": {
    "doc": "VM Erstellung",
    "title": "Erstellung einer VM",
    "content": "Zum erstellen einer neue Virtuele Machine im Openstack muss man sich im Openstack Dashboard einloggen. Diesen Zugriff haben nur Administratoren. Nach dem einloggen, können neue VMs im Reiter Instances erstellt werden. Man klickt auf “Launch Instance” um mit einen Wizard durch die Erstellung der VM durch geführt zu werden. ",
    "url": "/edge/openstack/create_vm/#erstellung-einer-vm",
    
    "relUrl": "/edge/openstack/create_vm/#erstellung-einer-vm"
  },"48": {
    "doc": "VM Erstellung",
    "title": "Details",
    "content": "Hier kann der Name ausgesucht werden, welche die VM erhält. Desweiteren kann die Anzahl der VMs geändert werden, der standard ist eine. ",
    "url": "/edge/openstack/create_vm/#details",
    
    "relUrl": "/edge/openstack/create_vm/#details"
  },"49": {
    "doc": "VM Erstellung",
    "title": "Source",
    "content": "Hier kann ausgewählt werden ob die Instance aus einem existierenden Volume erstellt werden soll. Oder ob die VM aus einem Image erstellt werden soll. Desweiteren kann eingestellt werden ob für die VM ein neues Volume erstellt werden soll und ob dieses Volume nach der Löschung der VM mit gelöscht werden soll. ",
    "url": "/edge/openstack/create_vm/#source",
    
    "relUrl": "/edge/openstack/create_vm/#source"
  },"50": {
    "doc": "VM Erstellung",
    "title": "Flavour",
    "content": "Hier kann eingestellt werden welche größe die VM haben soll und ob diese gegebenfalls mit einer Grafikkarte oder TSN Karte ausgestattet werden soll. ",
    "url": "/edge/openstack/create_vm/#flavour",
    
    "relUrl": "/edge/openstack/create_vm/#flavour"
  },"51": {
    "doc": "VM Erstellung",
    "title": "Network",
    "content": "Hier kann das Netzwerk ausgewählt werden mit welcher die VM bei der Erstellung verbunden ist. ",
    "url": "/edge/openstack/create_vm/#network",
    
    "relUrl": "/edge/openstack/create_vm/#network"
  },"52": {
    "doc": "VM Erstellung",
    "title": "Key Pair",
    "content": "Hier kann der SSH Key ausgewählt werden, mit welchen man sich über SSH verbinden kann. Desweiteren können hier neue SSH keys importiert werden. ",
    "url": "/edge/openstack/create_vm/#key-pair",
    
    "relUrl": "/edge/openstack/create_vm/#key-pair"
  },"53": {
    "doc": "VM Erstellung",
    "title": "VM Erstellung",
    "content": " ",
    "url": "/edge/openstack/create_vm/",
    
    "relUrl": "/edge/openstack/create_vm/"
  },"54": {
    "doc": "Resourcen Metriken",
    "title": "Metriken aus Libvirt und Operations Center",
    "content": " ",
    "url": "/edge/openstack/metrics/#metriken-aus-libvirt-und-operations-center",
    
    "relUrl": "/edge/openstack/metrics/#metriken-aus-libvirt-und-operations-center"
  },"55": {
    "doc": "Resourcen Metriken",
    "title": "Übersicht",
    "content": "Das Feature ermöglicht die Bereitstellung von Metriken aus Libvirt und dem Operations Center. Diese Metriken umfassen Informationen zur Nutzung von Flavors, CPU, RAM und weiteren Ressourcen sowie Daten zu den Projektbesitzern. ",
    "url": "/edge/openstack/metrics/#%C3%BCbersicht",
    
    "relUrl": "/edge/openstack/metrics/#übersicht"
  },"56": {
    "doc": "Resourcen Metriken",
    "title": "Endpunkt",
    "content": "URL: https://https://resources._EDGE_.gecgo.net/federate . Für den Zugriff werden Zugangsdaten benötigt. Die können über den Helpdesk bezogen werden. Beispielkonfiguration Prometheus . Weitere Details in der Prometheus Dokumentation zu Federation und scrape Konfiguration. scrape_configs: - job_name: 'edge_metrics' scrape_interval: 15s honor_labels: true metrics_path: '/federate' scheme: https basic_auth: username: scrape password_file: ./scrape_user_edge params: 'match[]': - '{__name__=~\".+\"}' static_configs: - targets: - 'resources._EDGE_.gecgo.net' . ",
    "url": "/edge/openstack/metrics/#endpunkt",
    
    "relUrl": "/edge/openstack/metrics/#endpunkt"
  },"57": {
    "doc": "Resourcen Metriken",
    "title": "Metriken aus Libvirt",
    "content": "Die Metriken aus Libvirt bieten detaillierte Informationen zu den virtuellen Maschinen (VMs), einschließlich: . | Genutzte Flavors: Informationen zu den zugewiesenen Ressourcenprofilen der VMs. | CPU-Nutzung: Aktuelle Auslastung der CPU-Ressourcen. | RAM-Nutzung: Aktuelle Auslastung des Arbeitsspeichers. | Weitere Metriken: Zusätzliche Informationen zu den VMs, die über Libvirt verfügbar sind. | . ",
    "url": "/edge/openstack/metrics/#metriken-aus-libvirt",
    
    "relUrl": "/edge/openstack/metrics/#metriken-aus-libvirt"
  },"58": {
    "doc": "Resourcen Metriken",
    "title": "Metriken aus dem Operations Center",
    "content": "Zusätzlich zu den Libvirt-Metriken werden auch Metriken zu den Projektbesitzern aus dem Operations Center bereitgestellt. Ein Beispiel für eine solche Metrik ist: . sql_oc_membership{job=\"oc-exporter\",instance=\"de-host-rack-01\",owner=\"jemand@userdomain.de\",project_name=\"projectname-dea9e633-9a03-4130-900b-dfde07734bff\"} 1 . Diese Metrik enthält folgende Informationen: . | Job: Der Name des Export-Jobs (oc-exporter). | Instance: Die Instanz-ID (de-host-rack-01). | Owner: Die E-Mail-Adresse des Projektbesitzers (jemand@userdomain.de). | Project Name: Der Name des Projekts (projectname-dea9e633-9a03-4130-900b-dfde07734bff). | Wert: Der Wert der Metrik (1). | . ",
    "url": "/edge/openstack/metrics/#metriken-aus-dem-operations-center",
    
    "relUrl": "/edge/openstack/metrics/#metriken-aus-dem-operations-center"
  },"59": {
    "doc": "Resourcen Metriken",
    "title": "Metriken",
    "content": "| Name | Beschreibung | Typ | . | libvirt_domain_block_meta | Block device metadata info. Device name, source file, serial. | gauge | . | libvirt_domain_block_stats_allocation | Offset of the highest written sector on a block device. | gauge | . | libvirt_domain_block_stats_capacity_bytes | Logical size in bytes of the block device backing image. | gauge | . | libvirt_domain_block_stats_flush_requests_total | Total flush requests from a block device. | counter | . | libvirt_domain_block_stats_flush_time_seconds_total | Total time in seconds spent on cache flushing to a block device | counter | . | libvirt_domain_block_stats_limit_burst_length_read_requests_seconds | Read requests per second burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_length_total_requests_seconds | Total requests per second burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_length_write_requests_seconds | Write requests per second burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_read_bytes | Read throughput burst limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_burst_read_bytes_length_seconds | Read throughput burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_read_requests | Read requests per second burst limit | gauge | . | libvirt_domain_block_stats_limit_burst_total_bytes | Total throughput burst limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_burst_total_bytes_length_seconds | Total throughput burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_total_requests | Total requests per second burst limit | gauge | . | libvirt_domain_block_stats_limit_burst_write_bytes | Write throughput burst limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_burst_write_bytes_length_seconds | Write throughput burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_write_requests | Write requests per second burst limit | gauge | . | libvirt_domain_block_stats_limit_read_bytes | Read throughput limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_read_requests | Read requests per second limit | gauge | . | libvirt_domain_block_stats_limit_total_bytes | Total throughput limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_total_requests | Total requests per second limit | gauge | . | libvirt_domain_block_stats_limit_write_bytes | Write throughput limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_write_requests | Write requests per second limit | gauge | . | libvirt_domain_block_stats_physicalsize_bytes | Physical size in bytes of the container of the backing image. | gauge | . | libvirt_domain_block_stats_read_bytes_total | Number of bytes read from a block device, in bytes. | counter | . | libvirt_domain_block_stats_read_requests_total | Number of read requests from a block device. | counter | . | libvirt_domain_block_stats_read_time_seconds_total | Total time spent on reads from a block device, in seconds. | counter | . | libvirt_domain_block_stats_size_iops_bytes | The size of IO operations per second permitted through a block device | gauge | . | libvirt_domain_block_stats_write_bytes_total | Number of bytes written to a block device, in bytes. | counter | . | libvirt_domain_block_stats_write_requests_total | Number of write requests to a block device. | counter | . | libvirt_domain_block_stats_write_time_seconds_total | Total time spent on writes on a block device, in seconds | counter | . | libvirt_domain_info_cpu_time_seconds_total | Amount of CPU time used by the domain, in seconds. | counter | . | libvirt_domain_info_maximum_memory_bytes | Maximum allowed memory of the domain, in bytes. | gauge | . | libvirt_domain_info_memory_usage_bytes | Memory usage of the domain, in bytes. | gauge | . | libvirt_domain_info_meta | Domain metadata | gauge | . | libvirt_domain_info_virtual_cpus | Number of virtual CPUs for the domain. | gauge | . | libvirt_domain_info_vstate | Virtual domain state. 0: no state, 1: the domain is running, 2: the domain is blocked on resource, 3: the domain is paused by user, 4: the domain is down, 5: the domain is shut off,6: the domain is crashed, 7: the domain is suspended by guest power management | gauge | . | libvirt_domain_interface_meta | Interfaces metadata. Source bridge, target device, interface uuid | gauge | . | libvirt_domain_interface_stats_receive_bytes_total | Number of bytes received on a network interface, in bytes. | counter | . | libvirt_domain_interface_stats_receive_drops_total | Number of packet receive drops on a network interface. | counter | . | libvirt_domain_interface_stats_receive_errors_total | Number of packet receive errors on a network interface. | counter | . | libvirt_domain_interface_stats_receive_packets_total | Number of packets received on a network interface. | counter | . | libvirt_domain_interface_stats_transmit_bytes_total | Number of bytes transmitted on a network interface, in bytes. | counter | . | libvirt_domain_interface_stats_transmit_drops_total | Number of packet transmit drops on a network interface. | counter | . | libvirt_domain_interface_stats_transmit_errors_total | Number of packet transmit errors on a network interface. | counter | . | libvirt_domain_interface_stats_transmit_packets_total | Number of packets transmitted on a network interface. | counter | . | libvirt_domain_memory_stats_actual_balloon_bytes | Current balloon value (in bytes). | gauge | . | libvirt_domain_memory_stats_available_bytes | The total amount of usable memory as seen by the domain. This value may be less than the amount of memory assigned to the domain if driver is in use or if the guest OS does not initialize all assigned pages. This value is expressed in bytes. | gauge | . | libvirt_domain_memory_stats_disk_cache_bytes | The amount of memory, that can be quickly reclaimed without additional I/O (in bytes).Typically these pages are used for caching disk | gauge | . | libvirt_domain_memory_stats_major_fault_total | Page faults occur when a process makes a valid access to virtual memory that is not available. When servicing the page fault, if is required, it is considered a major fault. | counter | . | libvirt_domain_memory_stats_minor_fault_total | Page faults occur when a process makes a valid access to virtual memory that is not available. When servicing the page not fault, IO is required, it is considered a minor fault. | counter | . | libvirt_domain_memory_stats_rss_bytes | Resident Set Size of the process running the domain. This value is in bytes | gauge | . | libvirt_domain_memory_stats_unused_bytes | The amount of memory left completely unused by the system. Memory that is available but used for reclaimable caches should NOT be free. This value is expressed in bytes. | gauge | . | libvirt_domain_memory_stats_usable_bytes | How much the balloon can be inflated without pushing the guest system to swap, corresponds to ‘Available’ in /proc/meminfo | gauge | . | libvirt_domain_memory_stats_used_percent | The amount of memory in percent, that used by domain. | gauge | . | libvirt_domain_vcpu_cpu | Real CPU number, or one of the values from virVcpuHostCpuState | gauge | . | libvirt_domain_vcpu_state | VCPU state. 0: offline, 1: running, 2: blocked | gauge | . | libvirt_domain_vcpu_time_seconds_total | Amount of CPU time used by the domain’s VCPU, in seconds. | counter | . | libvirt_domain_vcpu_wait_seconds_total | Vcpu’s wait_sum metric. CONFIG_SCHEDSTATS has to be enabled | counter | . | libvirt_up | Whether scraping libvirt’s metrics was successful. | gauge | . | libvirt_versions_info | Versions of virtualization components | gauge | . | sql_oc_membership | Project owner information of Operations Center | counter | . ",
    "url": "/edge/openstack/metrics/#metriken",
    
    "relUrl": "/edge/openstack/metrics/#metriken"
  },"60": {
    "doc": "Resourcen Metriken",
    "title": "Resourcen Metriken",
    "content": " ",
    "url": "/edge/openstack/metrics/",
    
    "relUrl": "/edge/openstack/metrics/"
  },"61": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/edge/faq/",
    
    "relUrl": "/edge/faq/"
  },"62": {
    "doc": "FAQ",
    "title": "Openstack",
    "content": "Wie werden User hinzugefügt? . Aktuell nur auf Anfrage per Jira Helpdesk Ticket da jeder Openstack User auch Admin ist. Wie kann man VM’s löschen von anderen Usern/Projekten? Im Horizon dashboard kann man über den Adminbereich auf alle VMs zugreifen und diese löschen. Sonst ist der Zugriff auch über die Openstack CLI möglich . Wie kann die Plattengröße einer bestehenden VM erweitert werden? . Dies ist in der aktuellen Openstack Version nicht möglich. Welche Subnetze gibt es . Die folgenden Subnetze sind immer vorhanden: . | public | shared | private/internal | . Auf Anfrage kann ein viertes Netzwerk, Labor, für eine direkte Verbindung ohne die Barracuda Firewall erstellt werden. Wie erfolgt die Trennung der Subnetze: . Subnetze werden über OpenVSwitch virtualisiert zur garantierten Trennung können Securityregeln angelegt werden. Diese Regeln sind durch IPFilter Regeln in OpenStack Neutron dar. Die Einhaltung von Zonen wird im Operations Center über Sicherheitsregeln und Sicherheitsgruppen verwaltet. ",
    "url": "/edge/faq/#openstack",
    
    "relUrl": "/edge/faq/#openstack"
  },"63": {
    "doc": "FAQ",
    "title": "Operations Center",
    "content": "Wie werden User hinzugefügt im Operations Center? . Der Zugriff auf das Operations Center wird über SSO realisiert. Nachdem Benutzer sich angemeldet haben können sie durch Projekteigentümer zu einem Projekt hinzugefügt werden. Jeder Nutzer kann eigene Projekte anlegen. Zusätzlich können im Operations Center externe Benutzer angelegt werden. Siehe Rolle Externe Benutzer . Gruppen vs Rollen Unterschied von Rollen und Gruppen . Das Operations Center hat aktuell 4 Benutzergruppen: . | Externe Benutzer: Externe Benutzer werden im Operations Center verwaltet. Sie können Zugriff per VPN auf VMs erhalten. Zusätzlich kann der öffentliche SSH Schlüssel zu VMS hinzugefügt werden. | Benutzer (SSO): Benutzer können Projekte anlegen und Verwalten. Auf Projektebene gibt es drei Rollen | . | Owner: Vollzugriff auf das Projekt. | . | . | Member: Vollen Zugriff auf die Projektresourcen ohne Benutzerverwaltung. | . | . | Viewer: Nur lesender Zugriff. Der Gast kann keine Passwörter für VMs einsehen. | . | Admin (SSO)*können externe Benutzer verwalten und haben Zugriff auf die Ressourcenübersicht und Globale Nachrichten Funktion. | Global Admin (SSO)*: Haben zusätzlichen Zugriff auf alle Projekte und können das VPNaaS für die Projekte aktivieren VPN können die normalen Admins auch anlegen | . *Anfrage an GEC notwendig nachdem der Benutzer sich das erste mal am Operations Center angemeldet hat. Admin Rechte nur auf einem Projekt? Wie? . Benutzer haben automatisch vollen Zugriff auf Projekte, die sie anlegen. Andere Projektbesitzer können das Recht weitergeben. Wie kann ich den Besitzer von einem Projekt ändern? . Jeder Projektbesitzer und Admin mit Zugriff auf das Projekt kann die Berechtigungen ändern. Was passiert wenn ein User das Unternehmen verlässt? . Durch die Integration von SSO verliert der Benutzer automatisch den Zugriff. Es ist ggfls. notwendig seine Projekte neu zuzuweisen. Siehe vorheriger Punkt. VPN Zugänge müssen manuell gelöscht werden. Können Vorgaben gemacht werden wie ein Projekt heißen soll? . Beim Anlegen. Eine Änderung ist nicht vorgesehen. Wie wird ein VPN angelegt? . Globale Administratoren können die Funktion für einzelne Projekte aktivieren. Projektbesitzer können weiter Benutzer die dem Projekt hinzugefügt sind freischalten. Benutzer brauchen Zugriff auf das Projekt um ihre eigene Konfiguration herunterzuladen. Externe Benutzer . Externe Benutzer können auch für VPN freigeschaltet werden. Die VPN Konfiguration für externe Benutzer muss durch einen Projektbesitzer Benutzer heruntergeladen und zur Verfügung gestellt werden. Wie werden Projektefreigaben angelegt bzw. erstellt? . Auf der Startseite jedes Projektes über die Mitgliederliste. Wie werden die Flavor verwaltet? . Flavor werden in Openstack verwaltet. Jedes “public” Flavor steht im OC zur Konfiguration bereit. Falls ein “public” Flavor nicht mehr im Operations Center beim Erstellen einer VM auswählbar sein soll, dann können die Metadaten des Flavors entsprechend geändert werden. Dazu in Horizon das Flavor auswählen, auf “Aktualisiere Metadaten” klicken und im Feld “Custom” visibility eintragen und mit “+” hinzufügen. Das Feld taucht jetzt unter “Existing Metadata” auf. Hier den Wert “false” eintragen und die Änderung speichern. Das Flavor kann jetzt nicht mehr beim erstellen einer neuen VM ausgewählt werden. Bestehende VMs die dieses Flavor benutzen werden nicht beeinflusst und können weiterhin im Operations Center verwaltet werden. Flavor können nicht auf einzelne Benutzer beschränkt werden. Dürfen Flavor gelöscht werden . Ja, aber nur wenn sichergestellt ist, dass keine VM das Flavor nutzt. Das Löschen erfolgt über Horizon. Sollte ein Flavor, welches noch in Nutzung ist, gelöscht werden, kann jedes Projekt mit einer betroffenen VM nicht mehr über das Operations Center verwaltet werden. Wo kann man Logs sehen, wenn eine VM angelegt wurde aber sie nicht erscheint? . Über Horizon ist es möglich den Startvorgang der VM zu beobachten und zu wiederholen. Wird die VM in Horizon nicht angelegt, ist aktuell ein Ticket im Helpdesk anzulegen. Wie kann man VMs löschen von anderen Usern/Projekten? . Nur Benutzer und Globale Administratoren mit Zugriff auf Projekte können VMs löschen. Projektliste Operations Center vs Horizon . Die Projekte vom Operations Center sind auch in Horizon sichtbar. Projekte, die in Horizon erzeugt werden sind nicht im Operations Center sichtbar. Wie können VMs gestartet werden? . Der Start von VMs ist über die Schaltfläche “Starten” im Operations Center möglich. Wie kann die Plattengröße einer bestehenden VM erweitert werden im Operations Center? . Es ist nicht möglich. Wie können externe/interne User gelöscht werden? . Interne User werden über das SSO authentifiziert. Werden sie dort deaktiviert, werden sie automatisch gesperrt. Externe Benutzer können in der Übersicht der Externen Benutzer im Operations Center von Administratoren gelöscht werden. Wie kann ich den Router bearbeiten und Konfigurieren? . Das Operations Center konfiguriert die Router entsprechend der Projektvorgaben automatisch. Wie können Routen bearbeitet werden im Operations Center? . Es können im Operations Center keine zusätzlichen Routen eingerichtet werden. Wie kann man einen SSH key einer vorhandenen VM hinzufügen? . Es ist nicht möglich SSH keys nach der Erstellung einer VM über das Operations Center hinzuzufügen. Dies müssen direkt in der VM hinzugefügt werden. Volumegrößen . Das Operations Center erlaubt das Anlegen von Volumes in unterschiedlichen Größen. Die Liste kann auf Anfrage erweitert werden. Kann das Operations Center per API konfiguriert werden . Nein, das ist nicht vorgesehen. Wie kann ich mich bei einer Windows VM anmelden? Der User operation funktioniert nicht mit dem eingegebenen Passwort. Anstelle des Benutzers “operation” muss für Windows VMs der Benutzer des ausgewählten images verwendet werden. Oft Administrator. ",
    "url": "/edge/faq/#operations-center",
    
    "relUrl": "/edge/faq/#operations-center"
  },"64": {
    "doc": "FAQ",
    "title": "General",
    "content": "Zonen . Sicherheitszonen sind in hierarchisch absteigender Reihenfolge strukturiert, beginnend mit der Private / Internal Zone, gefolgt von der Shared Zone und Public Zone. Auf dieser Grundlage können virtuelle Maschinen (VMs) durch die Zuweisung von Floating IPs hierarchisch absteigend kommunizieren. Wenn VMs in sich in der Private / Internal Zonen befinden ist eine Kommunikation mit den Floating IPs in der Shared und Public Zone möglich. Allerdings ist der umgekehrte Weg, also von der Shared Zone zurPrivate / Internal Zonen, nicht gestattet. Analog ist die Kommunikation von der Public Zone in die Shared Zone zur Private / Internal Zonen nicht gestattet. Die Kommunikation folgt dabei stets dem Prinzip von sicher zu unsicher. Diese hierarchische Struktur ermöglicht eine geordnete und sicherheitsbewusste Kommunikation zwischen den verschiedenen Zonen. Des weiteren sind Floating IPs aus der Private / Internal Zonen nur aus dem Instituts Netz erreichbar, Services mit einer Floating IP aus der Public IP können durch die Instituts IT ins Internet bereitgestellt werden. Als spezial Zone kann auf Anfrage eine Lab Zone eingerichtet werden mit direkt Verbindung an den Switchen. Die Zonen werden durch das Operations Center verwaltet. Bei Projekten, die in Openstack angelegt werden, ist der Nutzer selbst verantwortlich. Wie können die Hardware Server runter/hoch gefahren werden? z.B. bei einer kurzfristigen Stromabschaltung. Aktuell nur per Anfrage im Jira Helpdesk . Starten die Server nach Strom automatisch? . Die Server Starten automatisch aber die Edge ist nicht automatisch nutzbar, die bei einem geregelten Herunterfahren die CEPH Replikation deaktiviert wird. Diese muss bei einem Neustart wieder aktiviert werden. Wenn das beigefügte Windows Image ausgewählt ist, ist da bereits eine Lizenz mit dabei. Um was für eine Lizenz handelt es sich? . Es handelt sich um eine Trial Lizenz von Microsoft. Bei Windows VM’s, ist der Kunde für den Erwerb und das Einfügen einer Lizenz verantwortlich. NVLink und NVSwitches . NV Link und NVSwitches verbinden NVidia Grafikkarten zum direkten Austausch von Daten zwischen den Grafikkarten. Aktuell verbauen wir Supermicro GPU Server diese haben keine NVLink oder NVSwitch zwischen den einzelnen GPUs. Zum aktuellen Zeitpunkt werden NVLink und NVSwitches nur in NVidia eigenen Servern verbaut der Marke DGX oder HGX. ",
    "url": "/edge/faq/#general",
    
    "relUrl": "/edge/faq/#general"
  },"65": {
    "doc": "Guided Tour",
    "title": "Guided Tour",
    "content": "Aus dem Browser zum eigenen Stack per Heat . ",
    "url": "/optimist/guided_tour/",
    
    "relUrl": "/optimist/guided_tour/"
  },"66": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Schritt 1: Das Dashboard (Horizon)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/#schritt-1-das-dashboard-horizon",
    
    "relUrl": "/optimist/guided_tour/step01/#schritt-1-das-dashboard-horizon"
  },"67": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Vorwort",
    "content": "In diesem Schritt für Schritt Tutorial werden wir uns schrittweise der Bedienung von Openstack widmen. Den Anfang macht das Horizon(Dashboard), nach einer kleinen Einführung, wird dann auf die Konsole gewechselt und der Abschluss bildet die Erstellung eigener Heat-Templates. ",
    "url": "/optimist/guided_tour/step01/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step01/#vorwort"
  },"68": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Login",
    "content": "Nachdem die Zugangsdaten vorliegen, ist der erste Schritt der Login. WICHTIG: Es gibt keinen Reset-Knopf für das Passwort. Für ein neues Passwort, schreiben Sie uns bitte eine E-Mail an support@gec.io. Hierzu wechseln wir im Browser auf folgende URL: https://optimist.gec.io/ . Im sich öffnenden Fenster wählen wir bei Domain default, und tragen den zugesendeten Benutzer (User-Name) sowie das zugehörige Passwort(Password) ein und klicken auf Connect. Nun öffnet sich das Horizon(Dashboard). ",
    "url": "/optimist/guided_tour/step01/#login",
    
    "relUrl": "/optimist/guided_tour/step01/#login"
  },"69": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Passwort ändern",
    "content": "Da aus Sicherheitsgründen empfohlen wird das Passwort nach Erhalt zu ändern, klicken wir im Horizon(Dashboard) dafür rechts oben auf den Benutzernamen(1) und auf Settings(2). Im sich nun öffnenden Fenster sehen wir zuerst Settings, wo unter anderem auch die Sprache umgestellt werden kann. Um das Passwort zu ändern, klicken wir rechts auf Change Password(1). Hier können nun das Passwort geändert werden. Dafür geben wir zunächst unser bisheriges Passwort ein(2), geben dann das neue an(3) und bestätigen es in der neuen Zeile (4). Damit das neue Passwort auch übernommen wird, fehlt noch ein Klick auf Change(5). ",
    "url": "/optimist/guided_tour/step01/#passwort-%C3%A4ndern",
    
    "relUrl": "/optimist/guided_tour/step01/#passwort-ändern"
  },"70": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "Abschluss",
    "content": "Sie haben Ihre ersten Schritte im Dashboard ausgeführt und Ihr Passwort geändert! . ",
    "url": "/optimist/guided_tour/step01/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step01/#abschluss"
  },"71": {
    "doc": "01: Das Dashboard (Horizon)",
    "title": "01: Das Dashboard (Horizon)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/",
    
    "relUrl": "/optimist/guided_tour/step01/"
  },"72": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Schritt 2: SSH-Key per Horizon anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step02/#schritt-2-ssh-key-per-horizon-anlegen",
    
    "relUrl": "/optimist/guided_tour/step02/#schritt-2-ssh-key-per-horizon-anlegen"
  },"73": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Vorwort",
    "content": "Um im nächsten Schritt einen Stack inkl. einer Instanz zu starten, wird ein SSH Keypair (Schlüsselpaar) benötigt. Für den Fall, dass bereits ein Keypair vorhanden ist und der Umgang damit bekannt ist, kann dieser Schritt übersprungen werden. ",
    "url": "/optimist/guided_tour/step02/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step02/#vorwort"
  },"74": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Installation",
    "content": "Es gibt verschiedene Wege, um ein Keypair zu erzeugen. Einer der späteren Schritte erklärt den Weg zu einem selbst erstellten Keypair. Hier wird der Schlüssel direkt im Horizon(Dashboard) erstellt, um im nächsten Schritt den Stack zu erstellen. Um nun den Schlüssel zu erstellen, wechseln wir im Horizon(Dashboard) in der Navigation auf Compute → Key Pairs und klicken dort auf Create Key Pair. Im sich öffnenden Fenster kann nun ein Name für den Key vergeben werden, in dem Beispiel wird BeispielKey verwendet, anschließend klicken wir auf Create Key Pair. ",
    "url": "/optimist/guided_tour/step02/#installation",
    
    "relUrl": "/optimist/guided_tour/step02/#installation"
  },"75": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "Abschluss",
    "content": "Wir haben jetzt unser SSH Keypair erstellt und sind bereit für den Rest des Tutorials! . ",
    "url": "/optimist/guided_tour/step02/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step02/#abschluss"
  },"76": {
    "doc": "02: SSH-Key per Horizon anlegen",
    "title": "02: SSH-Key per Horizon anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step02/",
    
    "relUrl": "/optimist/guided_tour/step02/"
  },"77": {
    "doc": "03: Einen Stack starten",
    "title": "Schritt 3: Einen Stack starten",
    "content": " ",
    "url": "/optimist/guided_tour/step03/#schritt-3-einen-stack-starten",
    
    "relUrl": "/optimist/guided_tour/step03/#schritt-3-einen-stack-starten"
  },"78": {
    "doc": "03: Einen Stack starten",
    "title": "Vorwort",
    "content": "In diesem Schritt beschäftigen wir uns damit, im Horizon Dashboard einen Stack zu starten und damit auch das Horizon Dashboard besser kennenzulernen. Wichtige Voraussetzung ist an dieser Stelle ein SSH-Key, den wir in Schritt 2 erzeugt haben. ",
    "url": "/optimist/guided_tour/step03/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step03/#vorwort"
  },"79": {
    "doc": "03: Einen Stack starten",
    "title": "Start",
    "content": "Um einen Stack zu starten, loggen wir uns zunächst im Horizon Dashboard mit denen in Schritt 1 geänderten Zugangsdaten ein. Hier navigieren wir über Orchestration zu Stacks und klicken auf Launch Stack. Um den Stack auch zu starten, benötigen wir zunächst ein Template, welches in dem Stack eine Instanz startet. Hierfür nutzen wir die SingleServer.yaml aus dem GECio Github Repository. In dem sich nun öffnenden Fenster, wählen wir bei Template Source File aus und nehmen bei Template File, die eben heruntergeladene SingleServer.yaml. Den Rest belassen wir so wie es ist und klicken auf Next. Nun werden weitere Eingaben benötigt, genauer sind das folgende und am Ende klicken wir auf Launch: . | Stack Name: BeispielServer | Creation Timeout: 60 | Password for User: Bitte das eigene Passwort eintragen | availability_zone: ix1 | flavor_name: m1.micro | key_name: BeispielKey | machine_name: singleserver | public_network_id: provider | . Nun wird der Stack auch direkt gestartet und das Horizon Dashboard sieht dann so aus: . Um nun zu überprüfen ob die Instanz korrekt gestartet wurde, wechseln wir in der Navigation auf Compute → Instances und die Übersicht sieht dann wie folgt aus: . Nachdem nun also der Stack und auch die darin enthaltene Instanz gestartet wurden, löschen wir jetzt wieder den Stack inklusive Instanz. Wir könnten auch die Instanz alleine löschen, das kann aber im Nachgang zu Problemen beim löschen des Stacks führen. Um den Stack zu löschen, wechseln wir in der Navigation wieder auf Orchestration → Stacks. Klicken hinter dem Stack, unter Actions, auf den Pfeil nach unten und wählen dort Delete Stack. ",
    "url": "/optimist/guided_tour/step03/#start",
    
    "relUrl": "/optimist/guided_tour/step03/#start"
  },"80": {
    "doc": "03: Einen Stack starten",
    "title": "Abschluss",
    "content": "Wir haben unseren ersten Stack erstellt … und ihn dann gelöscht! . ",
    "url": "/optimist/guided_tour/step03/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step03/#abschluss"
  },"81": {
    "doc": "03: Einen Stack starten",
    "title": "03: Einen Stack starten",
    "content": " ",
    "url": "/optimist/guided_tour/step03/",
    
    "relUrl": "/optimist/guided_tour/step03/"
  },"82": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Schritt 4: Der Weg vom Horizon auf die Kommandozeile",
    "content": " ",
    "url": "/optimist/guided_tour/step04/#schritt-4-der-weg-vom-horizon-auf-die-kommandozeile",
    
    "relUrl": "/optimist/guided_tour/step04/#schritt-4-der-weg-vom-horizon-auf-die-kommandozeile"
  },"83": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Vorwort",
    "content": "Auf den ersten Blick kann es komfortabel erscheinen, seine OpenStack Umgebung mit dem Horizon Dashboard zu verwalten. Für einfache, nicht wiederkehrende Aufgaben, kann das Horizon Dashboard mit seinen grafische Ansichten wirklich hilfreich sein. Sobald Aufgaben regelmäßig wiederholt werden oder ein komplexerer Stack verwaltet werden soll, ist es sinnvoller, den OpenStack Client und auch Heat(welches in den späteren Schritten mit erklärt wird) zu verwenden. Anfangs mag die Handhabung ungewohnt sein, mit ein wenig Übung kann die Arbeit an den eigenen Stacks schnell und effizient erledigt werden. Der OpenStack Client ist sehr hilfreich bei der Administration der OpenStack Umgebung, da dort bereits Komponenten wie Nova, Glance, Heat, Cinder, Neutron enthalten sind. Da wir auch im weiteren Verlauf der Dokumentation den Client nutzen, installieren wir ihn in diesem Schritt. ",
    "url": "/optimist/guided_tour/step04/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step04/#vorwort"
  },"84": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Installation",
    "content": "Um den OpenStackClient installieren zu können, wird mindestens Python 2.7 noch die Python Setuptools (diese sind bei macOS bereits vorinstalliert). Es gibt verschiedene Optionen die Installation durchzuführen, pip hat sich hierbei als eine gute Lösung herausgestellt und wird als Grundlage in der Dokumentation verwendet. Selbiges ist einfach zu bedienen, stellt sicher das die aktuellste Version der Pakete genutzt wird und kann im Nachhinein Updates einspielen. Man kann den Client ohne weiteres in root/admin installieren, das kann aber zu weiteren Problem führen, daher nutzen wir eine virtuelle Umgebung für den Clienten. macOS . Damit nun der OpenStack Client installieren werden kann, wird zunächst pip benötigt. Um pip zu installieren, wird zunächst die Konsole geöffnet (diese kann zum Beispiel über das Launchpad → Konsole geöffnet werden)  und dann folgender Befehl ausgeführt: . $ easy_install pip Searching for pip Best match: pip 9.0.1 Adding pip 9.0.1 to easy-install.pth file Installing pip script to /usr/local/bin Installing pip2.7 script to /usr/local/bin Installing pip2 script to /usr/local/bin Using /usr/local/lib/python2.7/site-packages Processing dependencies for pip Finished processing dependencies for pip . Sobald die Installation von pip abgeschlossen ist, wird nun die Virtuelle Umgebung angelegt: . $ pip install virtualenv Collecting virtualenv Downloading virtualenv-15.1.0-py2.py3-none-any.whl (1.8MB) 100% |????????????????????????????????| 1.8MB 619kB/s Installing collected packages: virtualenv Successfully installed virtualenv-15.1.0 . Nachdem wir nun mit virtualenv eine virtuelle Umgebung nutzen können, erstellen wir direkt eine: . $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Jetzt wechseln wir in die erzeugte virtuelle Umgebung: . $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Nachdem dieser Wechsel funktioniert hat, können wir in der geschaffenen Umgebung auch direkt den OpenStackClient installieren: . (openstack) $ pip install python-openstackclient . Da wir im Verlauf der Dokumentation auch andere Services benutzen, installieren wir die entsprechenden Clienten gleich mit: . (openstack) $ pip install python-heatclient python-designateclient python-octaviaclient . Nun verlassen wir die Umgebung auch direkt wieder: . (openstack) $ deactivate . Damit wir den OpenStackClient auch nutzen können, ist es nun notwendig dies in die Path Variablen aufzunehmen. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Um zu sehen ob alles korrekt funktioniert hat, testen wir die Ausgabe: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . Windows . Um pip zu nutzen, ist zuerst der Wechsel in den Ordner der Python Installation notwendig (Speicherort der Standart Installation: C:\\Python27\\Scripts). pip wird dann mit dem Befehl easy_install pip installiert: . C:\\Python27\\Scripts&gt;easy_install pip Searching for pip Best match: pip 9.0.1 Adding pip 9.0.1 to easy-install.pth file Installing pip-script.py script to c:\\python27\\scripts Installing pip.exe script to c:\\python27\\scripts Installing pip.exe.manifest cript to c:\\python27\\scripts Installing pip3.5-script.py script to c:\\python27\\scripts Installing pip3.5.exe script to c:\\python27\\scripts Installing pip3.5.exe.manifest script to c:\\python27\\scripts Installing pip3-script.py script to c:\\python27\\scripts Installing pip3.exe script to c:\\python27\\scripts Installing pip3.exe.manifest script to c:\\python27\\scripts Using c:\\python27\\lib\\site-packages Processing dependencies for pip Finished processing dependencies for pip . Nach der erfolgreichen Installation von pip, kann direkt mit pip install python-openstackclient der OpenStackClient auch installiert werden: . C:\\Python27\\Scripts&gt;pip install python-openstackclient Collecting python-openstackclient Downloading python_openstackclient-3.12.0-py2.py3-none-any.whl (772kB) 100% |################################| 778kB 1.1MB/s . Linux (in diesem Beispiel Ubuntu) . Zunächst wird auch hier pip benötigt, dafür wird apt-get genutzt: . $ sudo apt-get install python3-pip Reading package lists... Done Building dependency tree Reading state information... Done . Sobald die Installation von pip abgeschlossen ist, wird nun die Virtuelle Umgebung angelegt: . $ sudo apt-get install python3-virtualenv Reading package lists... Done Building dependency tree Reading state information... Done . Nachdem wir nun mit virtualenv eine virtuelle Umgebung nutzen können, erstellen wir direkt eine: . $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Jetzt wechseln wir in die erzeugte virtuelle Umgebung: . $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Nachdem dieser Wechsel funktioniert hat, können wir in der geschaffenen Umgebung auch direkt den OpenStackClient installieren: . (openstack) $ pip install python-openstackclient . Da wir im Verlauf der Dokumentation auch Heat benutzen, installieren wir den entsprechenden Heat Clienten gleich mit: . (openstack) $ pip install python-heatclient . Nun verlassen wir die Umgebung auch direkt wieder: . (openstack) $ deactivate . Damit wir den OpenStackClient auch nutzen können, ist es nun notwendig dies in die Path Variablen aufzunehmen. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Um zu sehen ob alles korrekt funktioniert hat, testen wir die Ausgabe: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . ",
    "url": "/optimist/guided_tour/step04/#installation",
    
    "relUrl": "/optimist/guided_tour/step04/#installation"
  },"85": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Zugangsdaten",
    "content": "Nachdem der OpenStackClient nun installiert ist, werden noch die Zugangsdaten für Openstack benötigt. Diese können direkt im Horizon Dashboard heruntergeladen werden. Dafür loggen wir uns ein und klicken dann rechts oben in der Ecke auf die E-Mail-Adresse und dann auf Download OpenStack RC File v3. Die heruntergeladene Datei trägt den Projektnamen (Projektname.sh), in unserem Beispiel nennen wir sie Beispiel.sh . macOS | Linux . Um die Zugangsdaten in den OpenStackClienten einzulesen, führen wir nun folgenden Befehl aus: . source Beispiel.sh . Windows . Um unter Windows die Zugangsdaten einzulesen, ist es notwendig entweder PowerShell, Git for Windows oder Linux on Windows zu nutzen. Bei Linux on Windows und Git for Windows via Git Bash, wird der gleiche Befehl wie im Beispiel für macOS | Linux genutzt: . source Beispiel.sh . Bei der Nutzung von PowerShell müssen die Variablen einzeln gesetzt werden. Alle notwendigen Variablen befinden sich in der Datei Beispiel.sh und diese kann mit einem Editor geöffnet werden. Um die Variablen zu setzen, kann folgender Befehl genutzt werden: . set-item env:OS_AUTH_URL -value \"https://identity.optimist.gec.io/v3\" set-item env:OS_PROJECT_ID -value \"Projekt ID eintragen\" set-item env:OS_PROJECT_NAME -value \"Namen eintrage\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_USERNAME -value \"Usernamen eintragen\" set-item env:OS_PASSWORD -value \"Passwort eingeben\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_REGION_NAME -value \"fra\" set-item env:OS_INTERFACE -value \"public\" set-item env:OS_IDENTITY_API_VERSION -value \"3\" . ",
    "url": "/optimist/guided_tour/step04/#zugangsdaten",
    
    "relUrl": "/optimist/guided_tour/step04/#zugangsdaten"
  },"86": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "Ziel",
    "content": "Die Installation des OpenStackClienten ist abgeschlossen und die ersten Befehle können damit getestet werden. Eine Übersicht über alle Befehle, kann mit folgendem Kommando abgerufen werden: . openstack --help . ",
    "url": "/optimist/guided_tour/step04/#ziel",
    
    "relUrl": "/optimist/guided_tour/step04/#ziel"
  },"87": {
    "doc": "04: Der Weg vom Horizon auf die Kommandozeile",
    "title": "04: Der Weg vom Horizon auf die Kommandozeile",
    "content": " ",
    "url": "/optimist/guided_tour/step04/",
    
    "relUrl": "/optimist/guided_tour/step04/"
  },"88": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Schritt 5: Die wichtigsten Befehle des OpenStackClients",
    "content": " ",
    "url": "/optimist/guided_tour/step05/#schritt-5-die-wichtigsten-befehle-des-openstackclients",
    
    "relUrl": "/optimist/guided_tour/step05/#schritt-5-die-wichtigsten-befehle-des-openstackclients"
  },"89": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Vorwort",
    "content": "Nachdem in Schritt 4 der OpenStack Client installiert wurde, werden wir in diesem Schritt alle wichtigen Befehle einmal auflisten. Die Übersicht der spezifischen Subbefehle kann in der Kommandozeile mit einem --help hinter dem eigentlichen Befehl separat angezeigt werden. Um alle Befehle aufzulisten, kann der Schalter --help auch ohne weitere Angaben von einem Bestandteil  verwendet werden: . openstack --help . ",
    "url": "/optimist/guided_tour/step05/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step05/#vorwort"
  },"90": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Server",
    "content": "Mit dem Kommando openstack server ist es mögliche eigene Instanz zu erstellen, diese zu verwalten, zu löschen und andere} administrative Aufgaben durchzuführen. Hier eine Liste der wichtigsten Kommandos:} . | openstack server add Einer bestehenden Instanz können verschiedene Bestandteile (Fixed IP, Floating IP, Security Group, Volume) zugewiesen werden | openstack server create Mit diesem Befehl kann eine neue Instanz erstellt werden | openstack server delete Löscht die im Befehl angegebene Instanz | openstack server list Listet alle bestehenden Instanzen auf | openstack server remove Kann verschiedene Bestandteile (Fixed IP, Floating IP, Security Group, Volume) wieder entfernen | openstack server show Zeigt alle verfügbaren Informationen zu der im Befehl genannten Instanz an | . ",
    "url": "/optimist/guided_tour/step05/#server",
    
    "relUrl": "/optimist/guided_tour/step05/#server"
  },"91": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Stack (Heat)",
    "content": "Genauso wie mit openstack server ... Befehlen einzelne Instanzen administriert werden, kann man mit openstack stack ... ganze Stacks verwalten. Auch hier eine kurze Auflistung der wichtigsten Befehle: . | openstack stack create Kann einen neuen Stack erstellen | openstack stack list Listet alle bestehenden Stacks auf | openstack stack show Zeigt alle Informationen zu dem im Befehl angegebenen Stack | openstack stack delete Löscht den im Befehl angegebenen Stack | . ",
    "url": "/optimist/guided_tour/step05/#stack-heat",
    
    "relUrl": "/optimist/guided_tour/step05/#stack-heat"
  },"92": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Security Group",
    "content": "Security Groups werden verwendet, um Instanzen eingehende und ausgehende Netzwerk-Verbindungen basierend auf IP-Adressen und Ports zu erlauben oder zu verbieten. Auch Security Groups kann man mit dem OpenStackClienten verwalten. Hier eine beispielhafte Liste üblicher Aufrufe: . | openstack security group create Erstellt eine neue Security Group | openstack security group delete Löscht die im Befehl angegebene Security Group | openstack security group list Listet alle bestehenden Gruppen auf | openstack security group show Zeigt alle verfügbaren Informationen zu der im Befehl angegebenen Security Group | openstack security group rule create Fügt eine Regel zu einer Security Group hinzu | openstack security group rule delete Löscht die im Befehl angegeben Regel | . ",
    "url": "/optimist/guided_tour/step05/#security-group",
    
    "relUrl": "/optimist/guided_tour/step05/#security-group"
  },"93": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Network",
    "content": "Um später auch Instanzen sinnvoll nutzen zu können, benötigen diesen ein Netzwerk, hier eine kurze Auflistung der wichtigsten Befehle um ein Netzwerk zu erstellen: . | openstack network create Erstellt ein neues Netzwerk | openstack nerwork list Listet alle bestehenden Netzwerke auf | openstack network show Zeigt alle Informationen zu dem im Befehl angegebenen Netzwerk | openstack network delete Löscht das im Befehl angegebene Netzwerk | . ",
    "url": "/optimist/guided_tour/step05/#network",
    
    "relUrl": "/optimist/guided_tour/step05/#network"
  },"94": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Router",
    "content": "Damit eine Instanz mit einem Netzwerk verbunden werden kann, ist ein virtueller Router notwendig und lässt sich mit diesen Kommando administrieren. Hier eine kurze Liste der möglichen Befehle: . | openstack router create Erstellt einen neuen Router | openstack router delete Löscht einen bestehenden Router | openstack router add port Weist dem angegebenen Router, den angegebenen Port zu | openstack router add subnet Weist dem angegeben Router, das angegebene Subnet zu | . ",
    "url": "/optimist/guided_tour/step05/#router",
    
    "relUrl": "/optimist/guided_tour/step05/#router"
  },"95": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Subnet",
    "content": "Um den virtuellen Router korrekt zu betreiben, wird auch ein Subnet benötigt, welches mit dem Kommando openstack subnet administriert werden kann. Möglich Befehle sind: . | openstack subnet create Erstellt ein neues Subnet | openstack subnet delete Löscht ein bestehendes Subnet | openstack subnet show Zeigt alle verfügbaren Informationen zu einem Subnet an | . ",
    "url": "/optimist/guided_tour/step05/#subnet",
    
    "relUrl": "/optimist/guided_tour/step05/#subnet"
  },"96": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Port",
    "content": "Nachdem nun bereits virtuelle Router und Subnets bekannt sind, darf der Port nicht fehlen. Die wichtigsten Befehle: . | openstack port create Erstellt einen neuen Port | openstack port delete Löscht einen bestehenden Port | openstack port show Zeigt alle verfügbaren Informationen zu einem Port an | . ",
    "url": "/optimist/guided_tour/step05/#port",
    
    "relUrl": "/optimist/guided_tour/step05/#port"
  },"97": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Volume",
    "content": "Volumes sind persistente Speicherorte, die über die Existenz von einzelnen Instanzen hinaus erhalten bleiben. Wichtige Befehle sind: . | openstack volume create Erstellt ein neues Volume | openstack volume delete Löscht ein bestehendes Volume | openstack volume show Zeigt alle verfügbaren Informationen zu einem Volume an | . ",
    "url": "/optimist/guided_tour/step05/#volume",
    
    "relUrl": "/optimist/guided_tour/step05/#volume"
  },"98": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "Abschluss",
    "content": "In diesem Schritt wurden die wichtigsten Befehle einmal aufgelistet und auch mit einer kleinen Erklärung versehen. Die genannten Befehle werden in den nächsten Schritten benötigti und bilden somit die Grundlage für die Guided Tour. In Schritt 6 wird das Thema ein selbst erstelltes SSH Key Pair sein. ",
    "url": "/optimist/guided_tour/step05/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step05/#abschluss"
  },"99": {
    "doc": "05: Die wichtigsten Befehle des OpenStackClients",
    "title": "05: Die wichtigsten Befehle des OpenStackClients",
    "content": " ",
    "url": "/optimist/guided_tour/step05/",
    
    "relUrl": "/optimist/guided_tour/step05/"
  },"100": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Schritt 6: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "content": " ",
    "url": "/optimist/guided_tour/step06/#schritt-6-einen-eigenen-ssh-key-per-konsole-erstellen-und-nutzen",
    
    "relUrl": "/optimist/guided_tour/step06/#schritt-6-einen-eigenen-ssh-key-per-konsole-erstellen-und-nutzen"
  },"101": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Vorwort",
    "content": "Um später Zugriff auf den ersten deployten Stack per SSH zu erhalten, ist es notwendig, ein Key Pair zu erzeugen und dieses im Gegensatz zu Schritt 2 auch zu nutzen. Sollte bereits ein Keypair vorhanden sein, ist es nicht notwendig einen neuen zu erstellen. ",
    "url": "/optimist/guided_tour/step06/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step06/#vorwort"
  },"102": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Installation",
    "content": "Wie in Schritt 2 erwähnt, gibt es mehrere Optionen um einen Key zu erstellen. Da wir bereits per Horizon(Dashboard) einen Key erzeugt haben, wird in diesem Schritt er direkt über einen Befehl in der Kommandozeile erstellt. $ ssh-keygen -t rsa -f Beispiel.key Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in Beispiel.key. Your public key has been saved in Beispiel.key.pub. The key fingerprint is: SHA256:UKSodmr6MFCO1fSqNYAoyM7uX8n/O5a43cPEV5vJXW8 The key's randomart image is: +---[RSA 2048]----+ | .o |+. o o o |=.+ o + |+= o ..|oo+ = S . o B|o. =... o . =E|o.+ + . + . |.= ...+.o |.oo. o++o.. | +----[SHA256]-----+ . Mit dem oben genutzten Befehl (ssh-keygen -t rsa -f Beispiel.key) werden zwei Dateien erzeugt, also das vorher genannte Keypair. Zum einen die Beispiel.key Datei und die Beispiel.key.pub, dabei ist Beispiel.key der private Teil, der nur uns bekannt sein soll und Beispiel.key.pub wird als öffentlicher Teil genutzt. ",
    "url": "/optimist/guided_tour/step06/#installation",
    
    "relUrl": "/optimist/guided_tour/step06/#installation"
  },"103": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Einsatzort",
    "content": "Um den gerade erstellten Key zu nutzen, muss dieser eingebunden und für später erstellte Instanzen/Stacks bereit gestellt werden. Dies geht direkt mit dem vorher installierten OpenStackClient. In der Dokumentation gehen wir davon aus, dass der erzeugte Key in ~/.ssh/ liegt, sollte sich dieser an einer anderen Stelle befinden, muss das Keypair dorthin kopiert werden oder der Befehl entsprechend angepasst werden: . $ openstack keypair create --public-key ~/.ssh/Beispiel.key.pub Beispiel +-------------+-------------------------------------------------+ | Field | Value | +-------------+-------------------------------------------------+ | fingerprint | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | name | Beispiel | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | +-------------+-------------------------------------------------+ . Da im weiteren Verlauf der SSH-Key genutzt wird, sollte der Name, der statt Beispiel vergeben wird, leicht merkbar sein. Um zu überprüfen, ob der Key korrekt abgelegt wurde oder um sich den Namen erneut anzeigen zu lassen, nutzt man folgenden Befehl: . $ openstack keypair list +----------+-------------------------------------------------+ | Name | Fingerprint | +----------+-------------------------------------------------+ | Beispiel | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | +----------+-------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step06/#einsatzort",
    
    "relUrl": "/optimist/guided_tour/step06/#einsatzort"
  },"104": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "Abschluss",
    "content": "Da der SSH Key jetzt genutzt werden kann, wird es Zeit weiter vorzugehen und eine eigene Instanz zu erstellen. Wie das genau funktioniert, erklären wir in Schritt 7. ",
    "url": "/optimist/guided_tour/step06/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step06/#abschluss"
  },"105": {
    "doc": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "title": "06: Einen eigenen SSH-Key per Konsole erstellen und nutzen",
    "content": " ",
    "url": "/optimist/guided_tour/step06/",
    
    "relUrl": "/optimist/guided_tour/step06/"
  },"106": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Schritt 7: Die erste eigene Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step07/#schritt-7-die-erste-eigene-instanz",
    
    "relUrl": "/optimist/guided_tour/step07/#schritt-7-die-erste-eigene-instanz"
  },"107": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Vorwort",
    "content": "Wir wissen jetzt alles, was nötig ist, um die erste eigene Instanz anzulegen und zu starten. Es ist am sinnvollsten, das gleich in einem Stack zu organisieren und diesen mit einem Template zu beschreiben, anstatt alle notwendigen Arbeitsschritte von Hand durchzuführen. Trotzdem erzeugen wir im allerersten Schritt erst mal eine Instanz von Hand. ",
    "url": "/optimist/guided_tour/step07/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step07/#vorwort"
  },"108": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Installation",
    "content": "Der Grundbefehl für das Erstellen einer Instanz in der Kommandozeile lautet: . openstack server create test . Wenn der Befehl ohne weitere Zusätze ausgeführt wird, erscheint direkt eine Fehlermeldung: . usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; openstack server create: error: argument --flavor is required . Der Fehler besagt, dass kein Flavor angegeben ist. Damit nun eine Instanz gestartet werden kann, wird dem Befehl noch der entsprechende Flavor hinzugefügt. Um zu sehen welche Flavors bereit stehen, führen wir folgenden Befehl aus: . $ openstack flavor list +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | 090bcc91-6207-465d-aff0-bfcc10a9e063 | m1.medium | 8192 | 20 | 0 | 4 | True | 4ade7a50-f829-4bf6-af15-266798ea8d6f | win.large | 32768 | 80 | 0 | 8 | True | 5dd72380-088e-48cd-9a18-112cb5a9cab5 | win.small | 8192 | 80 | 0 | 2 | True | 884d5b93-1467-4bc1-a445-ff7c74271cbd | m1.micro | 1024 | 20 | 0 | 1 | True | b7c4fa0b-7960-4311-a86b-507dbf58e8ac | m1.small | 4096 | 20 | 0 | 2 | True | d45e3029-8364-4e4c-beab-242e8b4622a3 | win.medium | 16384 | 80 | 0 | 4 | True | dfead62e-96a8-46e9-bdae-342ecce32d41 | win.micro | 2048 | 80 | 0 | 1 | True | ed18c320-324a-487f-88e1-3e9eb9244509 | m1.large | 16384 | 20 | 0 | 8 | True | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ . Sollte man nun den Befehl openstack stack create Beispiel mit --flavor m1.micro starten, würde erneut eine Fehlermeldung angezeigt werden, da weitere Parameter fehlen. Um eine Instanz über diesen Weg zu starten, wird neben dem Flavor(--flavor) auch noch der SSH-Key (--key-name), das Image (--image), das verfügbare Netz (--network, in alten Versionen des Clients muss --nic net_id= verwendet werden) und eine SecurityGroup (--security-group) benötigt. Der SSH-Key wurde im letzten Schritt bereits erstellt und braucht so nicht erneut angelegt werden. Damit fehlen noch das Image (--image) und das Netz. Starten wir zunächst mit dem Image, wie bereits bei dem Flavor, kann auch hier eine Übersicht der möglichen Images mit folgendem Befehl angezeigt werden: . $ openstack image list +--------------------------------------+---------------------------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------------------------+--------+ | fd8ad5aa-6b33-4198-a05d-8be42fc0f20e | CentOS 7 - Latest | active | 82242d21-d990-4fc2-92a5-c7bd7820e790 | Ubuntu 16.04 Xenial Xerus - Latest | active | 8e82fd42-3d6f-44a7-9f20-92f5661823cf | Windows Server 2012 R2 Std - Latest | active | 536c086c-d2a4-43dd-80ea-a9d05ee2b97f | Windows Server 2016 Std - Latest | active | c94ced87-a03e-4eec-89f7-48f2c0ec6cd2 | debian-9.1.5-20170910-openstack-amd64 | active | b1195ddf-9336-42a7-a134-4f2e7ea57710 | iNNOVO-OPNsense-17.7.8 | active | 9134b6ed-8c5a-4a9a-907e-733dc2b5f0ef | iNNOVO_pfSense 2.3.4 | active | +--------------------------------------+---------------------------------------+--------+ . Nun fehlt noch ein Netzwerk. An dieser Stelle gibt es 2 Möglichkeiten für das Netzwerk, zum einen kann man ein sehr simples Netzwerk anlegen und so die Instanz starten, dafür nutzt man folgenden Befehl: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T08:32:44Z | description | | dns_domain | None | id | a783d691-7efe-4f67-9226-99a014fa8926 | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T08:32:44Z | +---------------------------+--------------------------------------+ . Der Nachteil an diesem Netzwerk ist, dass man die Instanz nicht erreichen kann. Soll die Instanz nutzbar sein, wird ein funktionierendes Netz benötigt, welches in Schritt 10 komplett angelegt wird. Nachdem alle Bestandteile jetzt bekannst sind, kann die erste Instanz erstellt werden. Dafür wird das --flavor m1.small, der SSH-Key aus Schritt 6, das Netzwerk von weiter oben, das --image \"Ubuntu 16.04 Xenial Xerus - Latest\" und die --security-group default: . $ openstack server create BeispielServer --flavor m1.small --key-name Beispiel --image 82242d21-d990-4fc2-92a5-c7bd7820e790 --network=BeispielNetzwerk --security-group default +-----------------------------+--------------------------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | adminPass | 6MotuEMy4c3t | config_drive | | created | 2017-12-06T14:15:02Z | flavor | m1.small (676d2587-b5aa-49eb-998d-d91c1bd6c056) | hostId | | id | 44ff2688-4ce5-417d-962b-3a80199bf1bc | image | cirros-tempest1 (2fbe66ef-adc8-44d0-b2e2-03d95dc36936) | key_name | cg | name | BeispielServer | progress | 0 | project_id | 1e775e2cc71a461991be42d4fad8a5cb | properties | | security_groups | name='3265503b-ac24-4f60-a8d0-466b7c812916' | status | BUILD | updated | 2017-12-06T14:15:02Z | user_id | b54fda3f4d1a484797b3ad4de9b3f4f9 | volumes_attached | +-----------------------------+--------------------------------------------------------+ . Weitere mögliche Parameter für die Erstellung einer Instanz können mit --help abgefragt werden: . $ openstack server create --help usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; Create a new server positional arguments: &lt;server-name&gt; New server name optional arguments: -h, --help show this help message and exit --image &lt;image&gt; Create server boot disk from this image (name or ID) --volume &lt;volume&gt; Create server using this volume as the boot disk (name or ID) --flavor &lt;flavor&gt; Create server with this flavor (name or ID) --security-group &lt;security-group-name&gt; Security group to assign to this server (name or ID) (repeat option to set multiple groups) --key-name &lt;key-name&gt; Keypair to inject into this server (optional extension) --property &lt;key=value&gt; Set a property on this server (repeat option to set multiple values) --file &lt;dest-filename=source-filename&gt; File to inject into image before boot (repeat option to set multiple files) --user-data &lt;user-data&gt; User data file to serve from the metadata server --availability-zone &lt;zone-name&gt; Select an availability zone for the server --block-device-mapping &lt;dev-name=mapping&gt; Map block devices; map is &lt;id&gt;:&lt;type&gt;:&lt;size(GB)&gt;:&lt;delete_on_terminate&gt; (optional extension) --nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt; Create a NIC on the server. Specify option multiple times to create multiple NICs. Either net-id or port- id must be provided, but not both. net-id: attach NIC to network with this UUID, port-id: attach NIC to port with this UUID, v4-fixed-ip: IPv4 fixed address for NIC (optional), v6-fixed-ip: IPv6 fixed address for NIC (optional), none: (v2.37+) no network is attached, auto: (v2.37+) the compute service will automatically allocate a network. Specifying a --nic of auto or none cannot be used with any other --nic value. --hint &lt;key=value&gt; Hints for the scheduler (optional extension) --config-drive &lt;config-drive-volume&gt;|True Use specified volume as the config drive, or 'True' to use an ephemeral drive --min &lt;count&gt; Minimum number of servers to launch (default=1) --max &lt;count&gt; Maximum number of servers to launch (default=1) --wait Wait for build to complete output formatters: output formatter options -f {json,shell,table,value,yaml}, --format {json,shell,table,value,yaml} the output format, defaults to table -c COLUMN, --column COLUMN specify the column(s) to include, can be repeated table formatter: --max-width &lt;integer&gt; Maximum display width, &lt;1 to disable. You can also use the CLIFF_MAX_TERM_WIDTH environment variable, but the parameter takes precedence. --print-empty Print empty table if there is no data to show. json formatter: --noindent whether to disable indenting the JSON shell formatter: a format a UNIX shell can parse (variable=\"value\") --prefix PREFIX add a prefix to all variable names . ",
    "url": "/optimist/guided_tour/step07/#installation",
    
    "relUrl": "/optimist/guided_tour/step07/#installation"
  },"109": {
    "doc": "07: Die erste eigene Instanz",
    "title": "Abschluss",
    "content": "Nachdem wir in diesem Schritt nicht nur eine neue Instanz erstellt , sondern auch noch einige Basis Befehle für OpenStack angewedet haben. Werden wir im Schritt 8 diese VM wieder löschen. ",
    "url": "/optimist/guided_tour/step07/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step07/#abschluss"
  },"110": {
    "doc": "07: Die erste eigene Instanz",
    "title": "07: Die erste eigene Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step07/",
    
    "relUrl": "/optimist/guided_tour/step07/"
  },"111": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Schritt 8: Löschen der ersten eigenen Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step08/#schritt-8-l%C3%B6schen-der-ersten-eigenen-instanz",
    
    "relUrl": "/optimist/guided_tour/step08/#schritt-8-löschen-der-ersten-eigenen-instanz"
  },"112": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Vorwort",
    "content": "Nachdem in Schritt 7: Die erste eigene Instanz die Instanz in der Kommandozeile angelegt wurde, wird in diesem Schritt erklärt, wie man eine Instanz wieder löscht. ",
    "url": "/optimist/guided_tour/step08/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step08/#vorwort"
  },"113": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Vorgehen",
    "content": "Damit eine Instanz generell gelöscht werden kann, wird entweder der Name oder die ID der zu löschenden Instanz benötigt. Bei wenigen Instanzen in einem Stack, kann der Name für das Löschen verwendet werden. Sobald allerdings mehrere Instanzen verwendet werden, wird von uns empfohlen für das Löschen die ID zu nutzen, da Namen im Gegensatz zu IDs nicht einzigartig sind. Der OpenStackClient zeigt einem sonst an, dass es mehrere Instanzen mit dem entsprechenden Namen gibt. Um nun eine Liste aller verfügbaren Instanzen zu erhalten, kann openstack server list als Befehl ausgeführt werden: . $ openstack server list +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | ID | Name | Status | Networks | Image Name | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | 801b3021-0c00-4566-881e-b50d47152e63 | singleserver | ACTIVE | single_internal_network=10.0.0.12, 185.116.245.39 | Ubuntu 16.04 Xenial Xerus - Latest | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ . Die angezeigte Liste listet alle verfügbaren Instanzen auf und enthält neben dem Namen der jeweiligen Instanz, auch die zugehörige ID. Um die im vorigen Schritt erstelle Instanz zu löschen, wird der Befehl openstack server delete ID verwendet, wobei “ID” durch die korrekte ID der Instanz ausgetauscht wird. In unserem Beispiel lautet der Befehl also wie folgt: . openstack server delete 801b3021-0c00-4566-881e-b50d47152e63 . Bei einer erneuten Ausgabe von openstack server list, sollte kein Server mehr angezeigt werden: . $ openstack server list $ . ",
    "url": "/optimist/guided_tour/step08/#vorgehen",
    
    "relUrl": "/optimist/guided_tour/step08/#vorgehen"
  },"114": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "Abschluss",
    "content": "Nachdem im vorigen Schritt eine Instanz per Hand erstellt wurde, haben wir diese Instanz hier gelöscht. Außerdem konnte mit dem Befehl openstack server list eine Übersicht über alle Instanzen gewonnen werden. In Schritt 9: Die erste Security-Group wird an den bisherigen Erfahrungen angeknüpft und das gewonnene Wissen um das Thema Security-Groups erweitert. ",
    "url": "/optimist/guided_tour/step08/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step08/#abschluss"
  },"115": {
    "doc": "08: Löschen der ersten eigenen Instanz",
    "title": "08: Löschen der ersten eigenen Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step08/",
    
    "relUrl": "/optimist/guided_tour/step08/"
  },"116": {
    "doc": "09: Die erste Security-Group",
    "title": "Schritt 9: Die erste Security-Group",
    "content": " ",
    "url": "/optimist/guided_tour/step09/#schritt-9-die-erste-security-group",
    
    "relUrl": "/optimist/guided_tour/step09/#schritt-9-die-erste-security-group"
  },"117": {
    "doc": "09: Die erste Security-Group",
    "title": "Vorwort",
    "content": "Standardmäßig ist jeglicher Zugriff auf eine Instanz von außerhalb verboten. Um Zugriff auf eine Instanz zu erlauben, muss (mindestens) eine Security Group definiert und der Instanz zugewiesen werden. Es ist möglich, alle Zugriffsregeln in einer Security Group zusammenzufassen, doch für komplexe Stacks macht es Sinn, die Regeln nach Aufgabe einzelner Instanzen in eigenen Security Groups zu hinterlegen. ",
    "url": "/optimist/guided_tour/step09/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step09/#vorwort"
  },"118": {
    "doc": "09: Die erste Security-Group",
    "title": "Vorgehen",
    "content": "Der Grundbefehl für das erstellen einer Security Group lautet openstack security group create allow-ssh-from-anywhere --description Beispiel: . $ openstack security group create allow-ssh-from-anywhere --description Beispiel +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 2 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:01:42Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . Damit eine Security Group nicht nur eine leere Hülle ist, kann der Befehl durch weitere Zusätze sinnvoll erweitert werden. Hier eine kurze Übersicht der wichtigsten Optionen: . | --protocol = Definition des genutzten Protokolls (mögliche Optionen: icmp, tcp, udp) | --dst-port = Gibt den Port oder die Range der Ports an. (22:22 ist Port 22, 1:[65535 würde alle Ports definieren)]{style=”color: rgb(34,34,34);”} | --remote-ip = Kann eine IP oder IP-Range definieren. (Default um den Zugang über alle IPs zu gewähren ist 0.0.0.0/0 | --ingress bzw. --egress = ingress definiert den eingehenden Verkehr, egress den ausgehenden | . Da die wichtigsten Optionen nun bekannt sind, kann jetzt eine Security Group erstellt werden, die es erlaubt theoretisch Zugriff per SSH zu erhalten. Der Befehl lautet openstack security group rule create allow-ssh-from-anywhere --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 . $ openstack security group rule create allow-ssh-from-anywhere --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:02:15Z | description | | direction | ingress | ether_type | IPv4 | id | 694a0573-b4c3-423c-847d-550f79e83f2b | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | 0.0.0.0/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:02:15Z | +-------------------+--------------------------------------+ . Um zu prüfen, ob die Security Group korrekt angelegt wurde und um eine Übersicht über alle zu erhalten, kann folgender Befehl genutzt werden, openstack security group show allow-ssh-from-anywhere . $ openstack security group show allow-ssh-from-anywhere +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 3 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:02:15Z', direction='ingress', ethertype='IPv4', id='694a0573-b4c3-423c-847d-550f79e83f2b', port_range_max='22', | | port_range_min='22', protocol='tcp', remote_ip_prefix='0.0.0.0/0', updated_at='2017-12-08T12:02:15Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:02:15Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step09/#vorgehen",
    
    "relUrl": "/optimist/guided_tour/step09/#vorgehen"
  },"119": {
    "doc": "09: Die erste Security-Group",
    "title": "Abschluss",
    "content": "Nach dem erfolgreichen erstellen der Security-Group, ist der nächste Schritt ein Netzwerk hinzuzufügen. Dies erfolgt im Schritt 10: Zugriff aus dem Internet vorbereiten: Ein Netzwerk anlegen. ",
    "url": "/optimist/guided_tour/step09/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step09/#abschluss"
  },"120": {
    "doc": "09: Die erste Security-Group",
    "title": "09: Die erste Security-Group",
    "content": " ",
    "url": "/optimist/guided_tour/step09/",
    
    "relUrl": "/optimist/guided_tour/step09/"
  },"121": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Schritt 10: Zugriff aus dem Internet vorbereiten: Ein Netzwerk anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step10/#schritt-10-zugriff-aus-dem-internet-vorbereiten-ein-netzwerk-anlegen",
    
    "relUrl": "/optimist/guided_tour/step10/#schritt-10-zugriff-aus-dem-internet-vorbereiten-ein-netzwerk-anlegen"
  },"122": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Vorwort",
    "content": "In Schritt 7 wurde zuerst eine Instanz manuell erstellt und in Schritt 9 dann eine Security Group. Nun ist der nächste Schritt ein virtuelles Netzwerk zu erstellen. ",
    "url": "/optimist/guided_tour/step10/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step10/#vorwort"
  },"123": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Netzwerk",
    "content": "Den Start dafür macht das eigentliche Netzwerk. Wie bisher gibt es mehrere zusätzliche Optionen, die wie gewohnt mit dem Zusatz --help aufgelistet werden können. Um das Netzwerk zu erstellen, nutzen wir den Befehl: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:06:38Z | description | | dns_domain | None | id | ff6d8654-66d6-4881-9528-2686bddcb6dc | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T12:06:38Z | +---------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#netzwerk",
    
    "relUrl": "/optimist/guided_tour/step10/#netzwerk"
  },"124": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Subnet",
    "content": "Da das Netzwerk nun angelegt wurde, ist der nächste logische Schritt, ein zugehöriges Subnet. Auch das Subnet hat sehr viele zusätzliche Optionen, für das Beispiel werden folgende genutzt: . | --network = Gibt an, in welchem Netzwerk das Subnet angelegt werden soll | --subnet-range = CIDR des Subnets. Im Beispiel wird 192.168.2.0/24 verwendet | . Um das Subnet in das vorher erstellte Netzwerk zu integrieren und die CIDR zu definieren lautet der korrekte Befehl: . $ openstack subnet create BeispielSubnet --network BeispielNetzwerk --subnet-range 192.168.2.0/24 +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | allocation_pools | 192.168.2.2-192.168.2.254 | cidr | 192.168.2.0/24 | created_at | 2017-12-08T12:09:07Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 192.168.2.1 | host_routes | | id | 984b24bf-db60-46a9-83c3-d68f6f1062e4 | ip_version | 4 | ipv6_address_mode | None | ipv6_ra_mode | None | name | BeispielSubnet | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | updated_at | 2017-12-08T12:09:07Z | use_default_subnet_pool | None | +-------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#subnet",
    
    "relUrl": "/optimist/guided_tour/step10/#subnet"
  },"125": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Router",
    "content": "Damit das Subnet auch sinnvoll genutzt werden kann, wird noch ein virtueller Router benötigt: . $ openstack router create BeispielRouter +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:09:49Z | description | | distributed | False | external_gateway_info | None | flavor_id | None | ha | False | id | bfb91c7f-acca-450a-aae0-c519ab563d38 | name | BeispielRouter | project_id | b15cde70d85749689e08106f973bb002 | revision_number | None | routes | | status | ACTIVE | updated_at | 2017-12-08T12:09:49Z | +-------------------------+--------------------------------------+ . Um eine Verbindung ins Internet zu ermöglichen, benötigt der Router ein externes Gateway, welches mit diesem Befehl gesetzt wird: . openstack router set BeispielRouter --external-gateway provider . Da nun schon die Verbindung hergestellt ist, wird dem Router nun noch das Subnet zugewiesen: . openstack router add subnet BeispielRouter BeispielSubnet . ",
    "url": "/optimist/guided_tour/step10/#router",
    
    "relUrl": "/optimist/guided_tour/step10/#router"
  },"126": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Port",
    "content": "Nachdem nun bereits der Router und das Subnet erstellt wurden, fehlt im letzten Schritt noch der zugehörige Port. Bei der Erstellung wird mit --network definiert, in welchem Netzwerk der Port verwendet werden soll: . $ openstack port create BeispielPort --network BeispielNetzwerk +-----------------------+----------------------------------------------------------------------------+ | Field | Value | +-----------------------+----------------------------------------------------------------------------+ | admin_state_up | UP | allowed_address_pairs | | binding_host_id | None | binding_profile | None | binding_vif_details | None | binding_vif_type | None | binding_vnic_type | normal | created_at | 2017-12-08T12:12:13Z | description | | device_id | | device_owner | | dns_assignment | None | dns_name | None | extra_dhcp_opts | | fixed_ips | ip_address='192.168.2.8', subnet_id='984b24bf-db60-46a9-83c3-d68f6f1062e4' | id | 31777c0a-a952-43ca-bb7f-11ad33926dae | ip_address | None | mac_address | fa:16:3e:09:88:c8 | name | BeispielPort | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | option_name | None | option_value | None | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 3 | security_group_ids | 3d3e3074-3087-4965-9a64-34a6d56193b9 | status | DOWN | subnet_id | None | updated_at | 2017-12-08T12:12:13Z | +-----------------------+----------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#port",
    
    "relUrl": "/optimist/guided_tour/step10/#port"
  },"127": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "Abschluss",
    "content": "Nachdem Router, Subnet und Port angelegt und diese miteinander verknüpft wurden, ist die Einrichtung des Beispielnetzwerks abgeschlossen und im nächsten Schritt fügen wir noch den Zugriff per IPv6 hinzu. ",
    "url": "/optimist/guided_tour/step10/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step10/#abschluss"
  },"128": {
    "doc": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "title": "10: Zugriff aus dem Internet vorbereiten; Ein Netzwerk anlegen",
    "content": " ",
    "url": "/optimist/guided_tour/step10/",
    
    "relUrl": "/optimist/guided_tour/step10/"
  },"129": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Schritt 11: Zugriff aus dem Internet vorbereiten: Wir ergänzen IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step11/#schritt-11-zugriff-aus-dem-internet-vorbereiten-wir-erg%C3%A4nzen-ipv6",
    
    "relUrl": "/optimist/guided_tour/step11/#schritt-11-zugriff-aus-dem-internet-vorbereiten-wir-ergänzen-ipv6"
  },"130": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Vorwort",
    "content": "In Schritt 10 wurde von uns bereits ein Netzwerk angelegt und in diesem Schritt erweitern wir selbiges um IPv6. Dabei nutzen wir die bereits bestehenden Router etc. Wichtig ist, dass die IPv4-Adresse auf dem ersten Interface läuft. Die Cloud Images sind so konzipiert, dass das primäre Interface mit DHCP vorkonfiguriert ist. Erst wenn das erfolgt ist, wird auf den Metadata Service zugegriffen um IPv6 überhaupt hoch zu fahren. ",
    "url": "/optimist/guided_tour/step11/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step11/#vorwort"
  },"131": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Subnet",
    "content": "Für die IPv6 Netze gibt es bereits einen Pool, aus dem man sich einfach ein eigenes Subnetz generieren lassen kann. Welche Pools es gibt, findet man mit diesem Befehl heraus: . $ openstack subnet pool list +--------------------------------------+---------------+---------------------+ | ID | Name | Prefixes | +--------------------------------------+---------------+---------------------+ | f541f3b6-af22-435a-9cbb-b233d12e74f4 | customer-ipv6 | 2a00:c320:1000::/48 | +--------------------------------------+---------------+---------------------+ . Aus diesem Pool kann man sich nun eigene Subnetze generieren lassen und die Prefixlänge von 64Bit ist dabei pro generiertem Subnet fest vorgegeben. Bei der Erstellung der Pools kann man die Subnets direkt mit angeben oder man überlässt es OpenStack. Dafür wird im Befehl openstack subnet create --network BeispielNetzwerk --ip-version 6 --use-default-subnet-pool --ipv6-address-mode dhcpv6-stateful --ipv6-ra-mode dhcpv6-stateful BeispielSubnetIPv6 einfach der Zusatz --use-default-subnet-pool genutzt. $ openstack subnet create --network BeispielNetzwerk --ip-version 6 --use-default-subnet-pool --ipv6-address-mode dhcpv6-stateful --ipv6-ra-mode dhcpv6-stateful BeispielSubnetIPv6 +-------------------------+----------------------------------------------------------+ | Field | Value | +-------------------------+----------------------------------------------------------+ | allocation_pools | 2a00:c320:1000:2::2-2a00:c320:1000:2:ffff:ffff:ffff:ffff | cidr | 2a00:c320:1000:2::/64 | created_at | 2017-12-08T12:41:42Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 2a00:c320:1000:2::1 | host_routes | | id | 0046c29b-a9b0-47c3-b5dd-704aa801704d | ip_version | 6 | ipv6_address_mode | dhcpv6-stateful | ipv6_ra_mode | dhcpv6-stateful | name | BeispielSubnetIPv6 | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | f541f3b6-af22-435a-9cbb-b233d12e74f4 | updated_at | 2017-12-08T12:41:42Z | use_default_subnet_pool | True | +-------------------------+----------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#subnet",
    
    "relUrl": "/optimist/guided_tour/step11/#subnet"
  },"132": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Router",
    "content": "Da nun das IPv6 Netz auch erstellt ist, werden wir in diesem Schritt das neue Netz mit dem in Schritt 10 erstellten Router verbinden. Dafür nutzen wir den Befehl: . openstack router add subnet BeispielRouter BeispielSubnetIPv6 . ",
    "url": "/optimist/guided_tour/step11/#router",
    
    "relUrl": "/optimist/guided_tour/step11/#router"
  },"133": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Security Group",
    "content": "Die Regeln die wir zuvor in Schritt 9 angelegt haben, beziehen sich nur auf IPv4. Damit auch IPv6 genutzt werden kann, legen wir noch zwei weitere Regeln in den schon bestehenden SecurityGroups an. Um auch per IPv6 Zugriff per SSH auf die VM zu erlangen nutzen wir den Befehl: . $ openstack security group rule create --remote-ip \"::/0\" --protocol tcp --dst-port 22:22 --ethertype IPv6 --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:04Z | description | | direction | ingress | ether_type | IPv6 | id | 7d871e85-05fa-4620-b558-c6fc64076cde | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:04Z | +-------------------+--------------------------------------+ . Nun fehlt noch der Zugriff per ICMP, damit wir die VM auch über IPv6 per Ping erreichen können. Dies geht mit diesem Befehl: . $ openstack security group rule create --remote-ip \"::/0\" --protocol ipv6-icmp --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:44Z | description | | direction | ingress | ether_type | IPv6 | id | f63e4787-9965-4732-b9d2-20ce0fedc974 | name | None | port_range_max | None | port_range_min | None | project_id | b15cde70d85749689e08106f973bb002 | protocol | ipv6-icmp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:44Z | +-------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#security-group",
    
    "relUrl": "/optimist/guided_tour/step11/#security-group"
  },"134": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Anpassungen am Betriebssystem",
    "content": "Startet man nun eine VM innerhalb des angelegten Netzes, bekommt diese eine IPv4, als auch eine IPv6 Adresse. Die Standardimages der Hersteller sind aber leider noch nicht für IPv6 vorkonfiguriert, weshalb tatsächlich nur die IPv4 Adresse in der VM ankommt. Nutzt man unsere bereitgestellten Heat Templates, sind die notwendigen Anpassungen bereits im Template enthalten. Um dies auch bei bestehenden Instanzen nachträglich auch zu ermöglichen, gibt es hier für verschiedene Distributionen einen Anleitung. Ubuntu 16.04 . Um IPv6 korrekt nutzen zu können, müssen folgende Dateien, mit dem angegeben Inhalt erstellt werden. | /etc/dhcp/dhclient6.conf . timeout 30; . | /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg . network: {config: disabled} . | /etc/network/interfaces.d/lo.cfg . auto lo iface lo inet loopback . | /etc/network/interfaces.d/ens3.cfg . iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true . | . Im Anschluss wird das entsprechende Interface neugestartet: . sudo ifdown ens3 &amp;&amp; sudo ifup ens3 . Die VM hat jetzt eine weitere IPv6 Adresse auf dem Interface, auf dem vorher nur die IPv4 Adresse existierte und kann somit auch per IPv6 korrekt erreicht werden. Damit man die beschriebenen Punkte nicht jedes mal manuell abarbeiten muss, kann man folgende cloud-init Konfiguration verwenden (Was cloud-init genau ist, erklären wir in Schritt 19: Unsere Instanz lernt IPv6): . #cloud-config write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] . CentOS 7 . Die genannten Parameter müssen den angegebenen Dateien neu hinzugefügt oder falls diese bereits vorhanden sind ergänzt werden: . | /etc/sysconfig/network . NETWORKING_IPV6=yes . | /etc/sysconfig/network-scripts/ifcfg-eth0 . IPV6INIT=yes DHCPV6C=yes . | . Anschließend wird das entsprechende Interface neugestartet: . sudo ifdown eth0 &amp;&amp; sudo ifup eth0 . Die VM hat jetzt eine weitere IPv6 Adresse auf dem Interface, auf dem vorher nur die IPv4 Adresse existierte und kann somit auch per IPv6 korrekt erreicht werden. Damit man die beschriebenen Punkte nicht jedes mal manuell abarbeiten muss, kann man folgende cloud-init Konfiguration verwenden(Was cloud-init genau ist, erklären wir für Ubuntu 16.04 in Schritt 19: Unsere Instanz lernt IPv6: . #cloud-config write_files: - path: /etc/sysconfig/network owner: root:root permissions: '0644' content: | NETWORKING=yes NOZEROCONF=yes NETWORKING_IPV6=yes - path: /etc/sysconfig/network-scripts/ifcfg-eth0 owner: root:root permissions: '0644' content: | DEVICE=\"eth0\" BOOTPROTO=\"dhcp\" ONBOOT=\"yes\" TYPE=\"Ethernet\" USERCTL=\"yes\" PEERDNS=\"yes\" PERSISTENT_DHCLIENT=\"1\" IPV6INIT=yes DHCPV6C=yes runcmd: - [ ifdown, eth0] - [ ifup, eth0] . ",
    "url": "/optimist/guided_tour/step11/#anpassungen-am-betriebssystem",
    
    "relUrl": "/optimist/guided_tour/step11/#anpassungen-am-betriebssystem"
  },"135": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Externer Zugriff",
    "content": "Wichtig: Diese VM ist ab sofort von überall auf der Welt über ihre IPv6 Adresse zu erreichen. Natürlich nur auf den Ports, die wir auch in den Security Groups aktiviert haben. Wir benötigen also keine weitere Floating IP um externen Zugriff auf diese VM zu ermöglichen. Es ist deshalb so wichtig zu erwähnen, da wir hier ein anderes Verhalten haben, als mit den IPv4 Adressen. Möchte man per IPv4 aus dem Internet auf diese VM zugreifen, muss man weiterhin auf die Floating IPs zurückgreifen. Hat man selbst lokal kein IPv6, möchte aber testen ob seine VM prinzipiell erreichbar ist, kann man auf Online Tools zurückgreifen, wie z.B. https://www.subnetonline.com/pages/ipv6-network-tools/online-ipv6-ping.php . ",
    "url": "/optimist/guided_tour/step11/#externer-zugriff",
    
    "relUrl": "/optimist/guided_tour/step11/#externer-zugriff"
  },"136": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "Abschluss",
    "content": "Nachdem im letzten Schritt bereits eine Verbindung per IPv4 erfolgte, wurde nun auch noch der Zugriff per IPv6 hinzugefügt. Im nächsten Schritt wird dann die Instanz aus Schritt 7 als Vorlage genutzt und erreichbar von außen. ",
    "url": "/optimist/guided_tour/step11/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step11/#abschluss"
  },"137": {
    "doc": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "title": "11: Zugriff aus dem Internet vorbereiten; Wir ergänzen IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step11/",
    
    "relUrl": "/optimist/guided_tour/step11/"
  },"138": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Schritt 12: Eine nutzbare Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step12/#schritt-12-eine-nutzbare-instanz",
    
    "relUrl": "/optimist/guided_tour/step12/#schritt-12-eine-nutzbare-instanz"
  },"139": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Vorwort",
    "content": "In Schritt 7 wurde bereits eine Instanz erstellt, diese konnte jedoch nur genutzt werden, wenn man ein paar Schritte übersprungen hat und das entsprechende Netzwerk mit erstellt. Es gab nur so die Möglichkeit eine Verbindung zu dieser herzustellen. Daher werden wir in diesem Schritt eine Instanz erstellen, die diese Problematik nicht mehr hat. ",
    "url": "/optimist/guided_tour/step12/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step12/#vorwort"
  },"140": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Installation",
    "content": "Damit die Instanz all die fehlenden Einstellungen enthält, wird der Befehl aus Schritt 7 modifiziert: . openstack server create BeispielInstanz --flavor m1.small --key-name Beispiel --image \"Ubuntu 16.04 Xenial Xerus - Latest\" --security-group allow-ssh-from-anywhere --network=BeispielNetzwerk . $ openstack server create BeispielInstanz --flavor m1.small --key-name Beispiel --image \"Ubuntu 16.04 Xenial Xerus - Latest\" --security-group allow-ssh-from-anywhere --network=BeispielNetzwerk +-----------------------------+---------------------------------------------------------------------------+ | Field | Value | +-----------------------------+---------------------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | adminPass | jkSdvP3A9yo6 | config_drive | | created | 2017-12-08T12:52:37Z | flavor | m1.small (b7c4fa0b-7960-4311-a86b-507dbf58e8ac) | hostId | | id | 1de98aa4-7d2b-4427-a8a5-d369ea8bdaf5 | image | Ubuntu 16.04 Xenial Xerus - Latest (82242d21-d990-4fc2-92a5-c7bd7820e790) | key_name | Beispiel | name | BeispielInstanz | progress | 0 | project_id | b15cde70d85749689e08106f973bb002 | properties | | security_groups | name='allow-ssh-from-anywhere' | status | BUILD | updated | 2017-12-08T12:52:37Z | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | volumes_attached | +-----------------------------+---------------------------------------------------------------------------+ . Genutzt wurden die folgenden Parameter: . | --flavor = Gibt den Flavor (Größe) der Instanz an. Eine Übersicht aller verfügbaren Flavors kann mit  openstack flavor list aufgerufen werden | --key-name = Der Name des zu verwendenden SSH-Keys | --image = Gibt an welches Image für die Instanz genutzt wird. Auch ist es möglich, sich im Vorfeld eine Liste aller verfügbaren Images anzusehen \"openstack image list\" | --security-group = Gibt an, welche Security-Groups genutzt wird | --network = Mit diesem Parameter kann unter anderem das gewünscht Netzwerk angeben werden (in alten Versionen des clients --nic net-id=&lt;network&gt;) | . Damit die erstellte Instanz über das Internet erreichbar ist, wird noch eine IP benötigt, welche zuerst angelegt wird. $ openstack floating ip create provider +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2017-12-08T12:53:37Z | description | | fixed_ip_address | None | floating_ip_address | 185.116.245.65 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 84eca140-9ac1-42c3-baf6-860ba920a23c | name | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | router_id | None | status | DOWN | updated_at | 2017-12-08T12:53:37Z | +---------------------+--------------------------------------+ . Die gerade erstellte IP wird im nächsten Schritt mit der vorher erstellten Instanz verbunden. openstack server add floating ip BeispielInstanz 185.116.245.145 . ",
    "url": "/optimist/guided_tour/step12/#installation",
    
    "relUrl": "/optimist/guided_tour/step12/#installation"
  },"141": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Nutzung",
    "content": "Die erstellte Instanz ist nun erreichbar. Um zu testen, ob alle Schritte funktionieren, stellen wir nun eine Verbindung per SSH her. Wichtig ist hierbei, dass eine Verbindung nur funktioniert, wenn der weiter oben genutzte SSH Key auch existiert und verwendet wird (Siehe Schritt 6: Einen eigenen SSH-Key per Konsole erstellen und nutzen): . $ ssh ubuntu@185.116.245.145 The authenticity of host '185.116.245.145 (185.116.245.145)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.145' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step12/#nutzung",
    
    "relUrl": "/optimist/guided_tour/step12/#nutzung"
  },"142": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Clean-Up",
    "content": "Für den Fall, dass die in den vorigen Schritten erstellten Bestandteile wieder gelöscht werden sollen, muss das in folgender Reihenfolge, mit dem entsprechenden Befehl geschehen. Sollte man dies nicht befolgen, kann es dazu führen, dass Bestandteile sich nicht löschen lassen. | Instanz . | openstack server delete BeispielInstanz | . | Floating-IP . | openstack floating ip delete 185.116.245.145 | . | Port . | openstack port delete BeispielPort | . | Router . | openstack router delete BeispielRouter | . | Subnet . | openstack subnet delete BeispielSubnet | . | Netzwerk . | openstack network delete BeispielNetzwerk | . | . ",
    "url": "/optimist/guided_tour/step12/#clean-up",
    
    "relUrl": "/optimist/guided_tour/step12/#clean-up"
  },"143": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "Abschluss",
    "content": "In den Schritten 7 bis 11 wurde eine Instanz Schritt für Schritt erstellt und jeder Schritt hat einen Teilbereich hinzugefügt (inklusive Netzwerk und einer eigenen Security-Group). Im nächsten Schritt lösen wir uns von einzelnen Instanzen und erstellen einen Stack. ",
    "url": "/optimist/guided_tour/step12/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step12/#abschluss"
  },"144": {
    "doc": "12: Eine nutzbare Instanz",
    "title": "12: Eine nutzbare Instanz",
    "content": " ",
    "url": "/optimist/guided_tour/step12/",
    
    "relUrl": "/optimist/guided_tour/step12/"
  },"145": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Schritt 13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/#schritt-13-der-strukturierte-weg-zu-einer-instanz-mit-stacks",
    
    "relUrl": "/optimist/guided_tour/step13/#schritt-13-der-strukturierte-weg-zu-einer-instanz-mit-stacks"
  },"146": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Vorwort",
    "content": "Nachdem die erste Instanz, inklusive einer Security Group und virtuellem Netzwerk, in einem sehr aufwendigen Prozess per Hand angelegt wurde, wird in diesem Schritt eine Alternative aufgezeigt. Wie diese funktioniert und was es hierbei zu beachten gibt, wird auch wie gewohnt Schritt für Schritt erklärt. Voraussetzung für die folgenden Schritte ist die Installation des Paketes python-heatclient. Siehe Schritt 4: Der Weg vom Horizon auf die Kommandozeile. ",
    "url": "/optimist/guided_tour/step13/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step13/#vorwort"
  },"147": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Installation",
    "content": "Anstatt einzelne Instanzen von Hand anzulegen, kann man beliebige OpenStack Resourcen (z.B. Instanzen, Netzwerke, Router, Security Groups) auch in einem definierten Verbund, einem so genannten Stack (oder Heat Stack) betreiben. Dadurch werden sie logisch zusammengefaßt und können einfach erstellt und gelöscht werden – je nach Verwendungszweck. Wir verwenden in diesem Schritt Heat-Templates, die auch Grundlage für folgende Schritte sind. Die bisherigen Schritte 9 bis 11, lassen sich einfach in einem Template zusammenfassen. Um nicht zu theoretisch zu bleiben, gibt es ein Template unter BeispielTemplates. Dieses Template erstellt einen Stack, in diesem ist eine Instanz, zwei Security Groups, ein virtuelles Netzwerk (inkl. Router, Port, Subnet) und eine Floating-IP enthalten. Um den Stack zu erstellen, ist es notwendig, sich im Verzeichnis des Templates zu befinden und dann folgenden Befehl zu nutzen: . $ openstack stack create -t SingleServer.yaml --parameter key_name=Beispiel SingleServer --wait 2017-12-08 13:13:43Z [SingleServer]: CREATE_IN_PROGRESS Stack CREATE started 2017-12-08 13:13:44Z [SingleServer.router]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:46Z [SingleServer.router]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_COMPLETE state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.subnet]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_COMPLETE state changed 2017-12-08 13:13:48Z [SingleServer.start-config]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:48Z [SingleServer.subnet]: CREATE_COMPLETE state changed 2017-12-08 13:13:49Z [SingleServer.router_subnet_bridge]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:49Z [SingleServer.port]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:50Z [SingleServer.start-config]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.port]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.host]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:52Z [SingleServer.router_subnet_bridge]: CREATE_COMPLETE state changed 2017-12-08 13:13:53Z [SingleServer.floating_ip]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:55Z [SingleServer.floating_ip]: CREATE_COMPLETE state changed 2017-12-08 13:14:05Z [SingleServer.host]: CREATE_COMPLETE state changed 2017-12-08 13:14:06Z [SingleServer]: CREATE_COMPLETE Stack CREATE completed successfully +---------------------+-------------------------------------------------+ | Field | Value | +---------------------+-------------------------------------------------+ | id | 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a | stack_name | SingleServer | description | A simple template to deploy your first instance | creation_time | 2017-12-08T13:13:42Z | updated_time | None | stack_status | CREATE_COMPLETE | stack_status_reason | Stack CREATE completed successfully | +---------------------+-------------------------------------------------+ . Der Befehl openstack stack create erstellt dabei den Stack, mit -t SingleServer.yaml wird festgelegt, dass das angegebene Template verwendet werden soll. Außerdem wird mit --parameter key_name=BEISPIEL noch ein SSH-Schlüssel angegeben und mit SingleServer wird der Name des Stacks festgelegt. Der letzte Bestandteil --wait zeigt alle Zwischenschritte der Erstellung an. (Siehe das obere Bild) . Nach kurzer Zeit ist der Stack inkl. Instanz erstellt und kann per SSH erreicht werden. Die notwendige IP lässt sich mit folgendem Befehl herausfinden, wichtig ist dabei die korrekte ID zu nutzen: . $ openstack stack output show 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a instance_fip +--------------+---------------------------------+ | Field | Value | +--------------+---------------------------------+ | description | External IP address of instance | output_key | instance_fip | output_value | 185.116.245.70 | +--------------+---------------------------------+ . Nun stellen wir noch die Verbindung per SSH her: . $ ssh ubuntu@185.116.245.70 The authenticity of host '185.116.245.70 (185.116.245.70)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.70' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step13/#installation",
    
    "relUrl": "/optimist/guided_tour/step13/#installation"
  },"148": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "Abschluss",
    "content": "Wir haben die Schritte 9 bis 11 in einem Heat-Template zusammengefaßt und können sie nun leicht wiederholen. In den folgenden Schritten gehen wir weiter auf Heat ein und zeigen weiterführende Beispiele. ",
    "url": "/optimist/guided_tour/step13/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step13/#abschluss"
  },"149": {
    "doc": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "title": "13: Der strukturierte Weg zu einer Instanz (mit Stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/",
    
    "relUrl": "/optimist/guided_tour/step13/"
  },"150": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Schritt 14: Unsere ersten Schritte mit Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step14/#schritt-14-unsere-ersten-schritte-mit-heat",
    
    "relUrl": "/optimist/guided_tour/step14/#schritt-14-unsere-ersten-schritte-mit-heat"
  },"151": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Vorwort",
    "content": "Nachdem im letzten Schritt die erste Berührung mit einem Template erfolgte, ist der nächste Schritt, zu verstehen, wie Templates mit Heat aufgebaut sind und funktionieren. Dieser Schritt erklärt nur die einzelnen Punkte eines Templates und soll diese näher bringen. ",
    "url": "/optimist/guided_tour/step14/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step14/#vorwort"
  },"152": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Das Template",
    "content": "Jedes Heat-Template folgt der gleichen Struktur und diese ist wie folgt aufgebaut: . heat_template_version: 2016-10-14   description: # Die Beschreibung des Templates (optional)   parameter_groups: # Die Definition der Eingabeparameter Gruppen und deren Reihenfolge   parameters: # Die Definition der Eingabeparameter   resources: # Die Definition der Ressourcen des Templates   outputs: # Die Definition der Ausgangsparameter   conditions: # Die Definition der Bedingungen . ",
    "url": "/optimist/guided_tour/step14/#das-template",
    
    "relUrl": "/optimist/guided_tour/step14/#das-template"
  },"153": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Heat Template Version",
    "content": "Die Template Version kann tatsächlich nicht willkürlich gewählt werden, sondern hat feste Vorgaben. Diese unterscheiden sich in den möglichen Befehlen und aktuell sind folgende Daten möglich: . | 2013-05-23 | 2014-10-16 | 2015-04-30 | 2015-10-15 | 2016-04-08 | 2016-10-14 | 2017-02-24 | . ",
    "url": "/optimist/guided_tour/step14/#heat-template-version",
    
    "relUrl": "/optimist/guided_tour/step14/#heat-template-version"
  },"154": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Description",
    "content": "Die Description oder auch Beschreibung ist ein komplett optionales Feld. Das bedeutet, dass das Feld nicht genutzt werden muss. Es bietet sich allerdings an, denn damit kann das Template in seinen Grundzügen beschrieben und direkt auf mögliche Besonderheiten hingewiesen werden. Auch besteht die Möglichkeit jederzeit eine Zeile mit dem Zeichen # auszukommentieren und dadurch das Template nach den jeweiligen Bedürfnissen zu verändern oder mehr Kommentare zu verfassen. ",
    "url": "/optimist/guided_tour/step14/#description",
    
    "relUrl": "/optimist/guided_tour/step14/#description"
  },"155": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Parameter Groups",
    "content": "In diesem Bereich ist es möglich zu spezifizieren, wie Parameter gruppiert werden sollen und die Reihenfolge für die Gruppierung festzulegen. Die Gruppen sind in eine Liste aufgegliedert, welche wiederum die einzelnen Parameter enthält. Jeder Parameter sollte nur einer Gruppe zugeordnet sein, damit es später zu keinen Problemen führt. Jede Parameter Group ist dabei wie folgt aufgebaut: . parameter_groups: - label: &lt;Name der Gruppe&gt; description: &lt;Beschreibung der Gruppe&gt; parameters: - &lt;Name des Parameters&gt; - &lt;Name des Parameters&gt; . | label: Name der Gruppe. | description: Dieses Attribut gibt  die Möglichkeit die Parameter Gruppe zu beschreiben und so für jeden verständlich zu machen, wofür diese genutzt wird. | parameter: Eine Auflistung aller Parameter die für diese Parameter Gruppe gelten. | Name des Parameters: Der in der Parameter Sektion definiert wurde. | . ",
    "url": "/optimist/guided_tour/step14/#parameter-groups",
    
    "relUrl": "/optimist/guided_tour/step14/#parameter-groups"
  },"156": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Parameter",
    "content": "Diese Sektion erlaubt es die eingegebenen Parameter zu spezifizieren, welche für die Ausführung benötigt werden. Die Parameter werden typischerweise dafür genutzt, jedes deployment zu individualisieren. Dabei wird jeder Parameter in einem separaten Block definiert, wobei am Anfang immer der Parameter genannt wird und dann weitere Attribute diesem zugeordnet werden.  parameters: &lt;Parameter Name&gt;: type: &lt;string | number | json | comma_delimited_list | boolean&gt; label: &lt;Name des Parameters&gt; description: &lt;Beschreibung des Parameters&gt; default: &lt;Standardwert des Parameters&gt; hidden: &lt;true | false&gt; constraints: &lt;Vorgaben für den Parameter&gt; immutable: &lt;true | false&gt; . | Parameter Name: Der Name des Parameters. | type: Der Typ des Parameters. Unterstützte Typen: string, number, json, comma_delimited_list, boolean. | label: Name des Parameters. (optional) | description: Dieses Attribut gibt die Möglichkeit den Parameter zu beschreiben und so für jeden verständlich zu machen, wofür dieser genutzt wird. (optional) | default: Der vorgegebene Wert des Parameters. Dieser Wert wird genutzt, wenn durch den User kein spezifischer Wert festgelegt werden soll. (optional) | hidden: Gibt an ob der Parameter bei einer Abfrage nach der Erstellung angezeigt wird oder versteckt ist. (optional und per Default auf false gesetzt) | constraints: Hier kann eine Liste von Vorgaben definiert werden. Sollten diese beim deployment nicht erfüllt werden, schlägt die Erstellung des Stacks fehl. | immutable: Definiert ob der Parameter aktualisiert werden kann. Für den Fall, dass der Parameter auf true gesetzt ist und sich der Wert bei einem stack update ändert, schlägt das Update fehl. | . ",
    "url": "/optimist/guided_tour/step14/#parameter",
    
    "relUrl": "/optimist/guided_tour/step14/#parameter"
  },"157": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Resources",
    "content": "Dieser Punkt definiert die aktuell genutzten Resourcen, die bei der Erstellung des Stacks genutzt werden. Dabei wird jede Ressource in einem eigenen Block definiert: . resources: &lt;ID der Ressource&gt;: type: &lt;Ressourcen Typ&gt; properties: &lt;Name der Eigenschaft&gt;: &lt;Wert der Eigenschaft&gt; metadata: &lt;Ressourcen spezifische Metadaten&gt; depends_on: &lt;Ressourcen ID oder eine Liste der IDs&gt; update_policy: &lt;Update Regel&gt; deletion_policy: &lt;Regel für das Löschen&gt; external_id: &lt;Externe Ressourcen ID&gt; condition: &lt;Name der Kondition oder Ausdruck oder boolean&gt; . | ID der Ressource: Diese muss einzigartig im Bereich der Ressourcen sein. Es darf durchaus ein sprechender Name gewählt werden z. B. (web_network). | type: Der Typ der Ressource, wie zum Beispiel OS::NEUTRON::SecurityGroup (für eine Security Group). (benötigt) | properties: Eine Liste der Ressourcen spezifischen Eigenschaften. (optional) | metadata: Hier können für die jeweilige Ressource spezifische Metadaten hinterlegt werden. (optional) | depends_on: Hier können verschiedene Abhängigkeiten zu anderen Ressourcen hinterlegt werden. (optional) | update_policy: Hier können Update Regeln festgelegt werden. Die Voraussetzung dafür ist, dass die entsprechende Resource dies auch unterstützt. (optional) | deletion_policy: Hier werden die Regeln für das Löschen festgelegt. Erlaubt sind Delete, Retain und Snapshot. Mit der heat_template_version 2016-10-14 ist nun auch die Kleinschreibung der Werte erlaubt . | external_id: Falls notwendig, können externe Ressourcen IDs verwendet werden | condition: Anhand der Kondition wird entschieden, ob die Ressource erstellt wird oder nicht. (optional) | . ",
    "url": "/optimist/guided_tour/step14/#resources",
    
    "relUrl": "/optimist/guided_tour/step14/#resources"
  },"158": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Output",
    "content": "Die Output-Sektion definiert die ausgegeben Parameter die für den User oder auch für andere Templates nach dem Erstellen des Stacks verfügbar sind. Das können zum Beispiel Parameter wie die IP-Adresse der erstellten Instanz oder auch die URL der erstellten Web-App sein. Auch wird jeder Parameter in einem eigenen Block definiert: . outputs: &lt;Name des Parameters&gt;: description: &lt;Beschreibung&gt; value: &lt;Wert des Parameters&gt; condition: &lt;Name der Kondition oder Ausdruck oder boolean&gt; . | Name des Parameters: Dieser muss wieder einzigartig sein. | description: Es kann eine Beschreibung für den Parameter hinterlegt werden. (optional) | value: Hier wird der Wert des Parameters vermerkt. (benötigt) | condition: Hier kann eine Kondition für den Parameter festlegt werden. (optional) | . ",
    "url": "/optimist/guided_tour/step14/#output",
    
    "relUrl": "/optimist/guided_tour/step14/#output"
  },"159": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Condition",
    "content": "Dieser Bereich legt die verschiedenen Konditionen fest und das auf Basis der eingegebenen Parameter des Users, beim Erstellen oder updaten des Stacks. Die Konditionen können mit Resources, Resource properties und Outputs verbunden werden. Auch dieser Bereich folgt wieder einem Muster: . conditions: &lt;Name der Condition1&gt;: {Bezeichnung1} &lt;Name der Condition2&gt;: {Bezeichnung2} . | Name der Condition: Dieser muss wieder einzigartig im Bereich der Condition sein. | Bezeichnung: Bei der Bezeichnung wird erwartet, dass sie ein true oder false zurückgibt. | . ",
    "url": "/optimist/guided_tour/step14/#condition",
    
    "relUrl": "/optimist/guided_tour/step14/#condition"
  },"160": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "Abschluss",
    "content": "In diesem Schritt wurden wichtige Bestandteile eines Heat-Templates vorgestellt. Mit diesem Wissen wird im nächsten Schritt das erste eigene Heat-Template erstellt. ",
    "url": "/optimist/guided_tour/step14/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step14/#abschluss"
  },"161": {
    "doc": "14: Unsere ersten Schritte mit Heat",
    "title": "14: Unsere ersten Schritte mit Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step14/",
    
    "relUrl": "/optimist/guided_tour/step14/"
  },"162": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Schritt 15: Das erste eigene Heat Orchestration Template (HOT)",
    "content": " ",
    "url": "/optimist/guided_tour/step15/#schritt-15-das-erste-eigene-heat-orchestration-template-hot",
    
    "relUrl": "/optimist/guided_tour/step15/#schritt-15-das-erste-eigene-heat-orchestration-template-hot"
  },"163": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Vorwort",
    "content": "Im folgenden Schritt sind die wichtigsten Elemente eines Templates erläutert worden und auf dieses Wissen, wird in diesem Schritt aufgebaut. ",
    "url": "/optimist/guided_tour/step15/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step15/#vorwort"
  },"164": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Der Anfang",
    "content": "Dieser ist bei jedem Template gleich und ist immer heat_template_version . Für das Beispiel wird Version 2016-10-14 genutzt und somit sieht das Template erst einmal so aus: . heat_template_version: 2016-10-14 . Nachdem die heat_template_version festgelegt ist, wird dem Template nun eine Beschreibung hinzugefügt: . heat_template_version: 2016-10-14   description: Ein einfaches Template, um eine Instanz zu erstellen . Nachdem die Beschreibung in das Template integriert wurde, wird nun eine Ressource, also die Instanz hinzugefügt. Dabei sind einige Punkte zu beachten, starten wir zunächst mit der Ressource. Wichtig ist dabei, dass eine Strukturierung mit Leerzeichen genutzt wird. Dies dient der Übersichtlichkeit, außerdem würden Tabstops zu Fehlern führen und nur so kann das Template korrekt ausgeführt werden: . heat_template_version: 2016-10-14 description: Ein einfaches Template, um eine Instanz zu erstellen resources: Instanz: . Der nächste Schritt ist dann den Typ der Ressource zu benennen. Eine ausführliche Liste aller verfügbaren Typen befindet sich unter anderem in der offiziellen OpenStack Dokumentation . Da im Beispiel eine Instanz erstellt werden soll, ist der Typ dann folgender: . heat_template_version: 2016-10-14 description: Ein einfaches Template, um eine Instanz zu erstellen resources: Instanz: type: OS::Nova::Server . Nach dem Typ sind dann die Eigenschaften der nächste Punkt. Im Beispiel soll dies ein SSH-Key, ein Flavor und ein Image sein: . heat_template_version: 2016-10-14 description: Ein einfaches Template, um eine Instanz zu erstellen resources: Instanz: type: OS::Nova::Server properties: key_name: BeispielKey image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step15/#der-anfang",
    
    "relUrl": "/optimist/guided_tour/step15/#der-anfang"
  },"165": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "Abschluss",
    "content": "Damit ist das Erste eigenes Template fertiggestellt und kann, wenn es gespeichert wird, einfach mit dem OpenStackClienten wie in Schritt 13: “Der strukturierte Weg zu einer Instanz (mit Stacks)” beschrieben, gestartet werden. ",
    "url": "/optimist/guided_tour/step15/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step15/#abschluss"
  },"166": {
    "doc": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "title": "15: Das erste eigene Heat Orchestration Template (HOT)",
    "content": " ",
    "url": "/optimist/guided_tour/step15/",
    
    "relUrl": "/optimist/guided_tour/step15/"
  },"167": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Schritt 16: Wir lernen Heat besser kennen",
    "content": " ",
    "url": "/optimist/guided_tour/step16/#schritt-16-wir-lernen-heat-besser-kennen",
    
    "relUrl": "/optimist/guided_tour/step16/#schritt-16-wir-lernen-heat-besser-kennen"
  },"168": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Vorwort",
    "content": "Bisher konnte der Eindruck entstehen, das Heat und das manuelle Erstellen per Kommandozeile genau so viel Zeit in Anspruch nimmt, was beim einmaligen Erstellen auch stimmt. Dadurch das nun ein Template existiert, können wir diese Grundlage immer wieder nutzen und im Zweifel weiter entwickeln. Damit dies auch möglich ist, wird in diesem Schritt weiter auf Heat eingegangen. ",
    "url": "/optimist/guided_tour/step16/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step16/#vorwort"
  },"169": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Parameter",
    "content": "Da das Ganze aufgebaut werden soll, ist es zunächst sinnvoll, bekannte oder individuelle Parameter zu definieren. In diesem Kontext wird der vorgegebene SSH-Key ersetzt und statt einem festen Wert, wird er als individueller Parameter definiert, der beim Start angegeben werden kann: . heat_template_version: 2014-10-16   parameters: key_name: type: string . Wie bisher gelernt, beginnt das Template mit der Version und wird dann mit parameters fortgeführt. Nach dem Parameter wird der Name, welcher individuell benannt werden kann, vergeben. Auch ist es notwendig den Typ anzugeben, in diesem Fall ist es string. Nachdem der Parameter festgelegt ist, nutzen wir als Vorlage das vorige Template und ergänzen es. Damit sieht das Template dann so aus: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: BeispielKey image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . Um das Template zu komplettieren, wird key_name durch den vorher definierten Parameter ersetzt. Der Befehl dafür lautet get_param. Dieser sagt aus, dass er einen definierten Parameter nutzen soll und damit das Template weiß, welchen Parameter er nutzen soll, ergänzen wir den Befehl get_param um key_name: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step16/#parameter",
    
    "relUrl": "/optimist/guided_tour/step16/#parameter"
  },"170": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "Abschluss",
    "content": "Das Template wurde jetzt bereits über einen frei definierbaren Parameter erweitert und im nächsten Schritt wird das Netzwerk hinzugefügt. ",
    "url": "/optimist/guided_tour/step16/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step16/#abschluss"
  },"171": {
    "doc": "16: Wir lernen Heat besser kennen",
    "title": "16: Wir lernen Heat besser kennen",
    "content": " ",
    "url": "/optimist/guided_tour/step16/",
    
    "relUrl": "/optimist/guided_tour/step16/"
  },"172": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Schritt 17: Das Netzwerk im Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/#schritt-17-das-netzwerk-im-heat",
    
    "relUrl": "/optimist/guided_tour/step17/#schritt-17-das-netzwerk-im-heat"
  },"173": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Vorwort",
    "content": "Im letzten Schritt war ein individueller Parameter das Ziel und in diesem das komplette Netzwerk. ",
    "url": "/optimist/guided_tour/step17/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step17/#vorwort"
  },"174": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Das Template",
    "content": "Um nicht bei Null zu starten, dient das Template aus dem vorigen Schritt als Vorlage. Wichtig ist dabei, dass direkt ein neuer Parameter hinzufügt wird, genauer die ID des öffentlichen Netzwerks: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step17/#das-template",
    
    "relUrl": "/optimist/guided_tour/step17/#das-template"
  },"175": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Netzwerk",
    "content": "Nachdem dieser Parameter eingefügt wurde, ist es Zeit das Netzwerk in das Template einzufügen. Hierbei handelt es sich um eine weitere Ressource und wird unter dem Punkt resources eingefügt. Der zugehörige Typ lautet OS::Neutron::Net: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk . ",
    "url": "/optimist/guided_tour/step17/#netzwerk",
    
    "relUrl": "/optimist/guided_tour/step17/#netzwerk"
  },"176": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Der Port",
    "content": "Der nächste Schritt ist dann der Port, der Typ lautet dafür OS::Neutron::Port. Wichtig ist, dass der Port in das bestehende Netzwerk eingegliedert wird und die Instanz dem Port zuzuordnen ist. Um dies zu erreichen, wird erneut ein get Befehl genutzt und statt dem Parameter eine Ressource eingebunden: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } . ",
    "url": "/optimist/guided_tour/step17/#der-port",
    
    "relUrl": "/optimist/guided_tour/step17/#der-port"
  },"177": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Der Router",
    "content": "Nachdem Netzwerk und Port, wird nun ein Router (Typ = OS::Neutron::Router) in das Template eingebunden. Bei diesem Typ ist es wichtig, das öffentliche Netzwerk einzubinden: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter . ",
    "url": "/optimist/guided_tour/step17/#der-router",
    
    "relUrl": "/optimist/guided_tour/step17/#der-router"
  },"178": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Das Subnet",
    "content": "Der vorletzte Schritt ist das Subnet (Typ = OS::Neutron::Subnet ) . In selbigem werden eigene Nameserver eintragen, die Informationen des Netzwerks eingebunden, die IP-Version sowie der IP-Adressraum festgelegt und die verfügbaren IPs definiert: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } . ",
    "url": "/optimist/guided_tour/step17/#das-subnet",
    
    "relUrl": "/optimist/guided_tour/step17/#das-subnet"
  },"179": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Subnet Bridge",
    "content": "Im letzten Schritt wird eine Subnet Bridge (Typ = OS::Neutron::RouterInterface) angelegt, also eine Brücke zwischen Router und Subnet. Auch gibt es hier eine weitere neue Komponente, genauer depends_on. Damit können wir Resource erstellen lassen, die nur dann gebaut werden, wenn es die referenzierte Resource auch gibt.   . In unserem Beispiel wird die Bridge zwischen Subnet und Router nur gebaut, wenn es auch ein Subnet gibt. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } . ",
    "url": "/optimist/guided_tour/step17/#subnet-bridge",
    
    "relUrl": "/optimist/guided_tour/step17/#subnet-bridge"
  },"180": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "Abschluss",
    "content": "Nachdem nun das komplette Netzwerk eingerichtet wurde, wird im nächsten Schritt eine eigene Security Group erstellt und zusätzlich der Instanz eine öffentliche IP-Adresse zugewiesen. ",
    "url": "/optimist/guided_tour/step17/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step17/#abschluss"
  },"181": {
    "doc": "17: Das Netzwerk im Heat",
    "title": "17: Das Netzwerk im Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/",
    
    "relUrl": "/optimist/guided_tour/step17/"
  },"182": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Schritt 18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "content": " ",
    "url": "/optimist/guided_tour/step18/#schritt-18-unsere-instanz-wird-von-au%C3%9Fen-per-ipv4-erreichbar",
    
    "relUrl": "/optimist/guided_tour/step18/#schritt-18-unsere-instanz-wird-von-außen-per-ipv4-erreichbar"
  },"183": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Vorwort",
    "content": "Im letzten Schritt wurde das komplette Netzwerk eingerichtet und es ist nun an der Zeit, die Instanz von außen zu erreichen (u.a. per ICMP und SSH Zugriff). ",
    "url": "/optimist/guided_tour/step18/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step18/#vorwort"
  },"184": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Floating-IP",
    "content": "Den Anfang macht die öffentliche IP-Adresse, welche auch als Ressource hinzugefügt wird. (Der zugehörige Typ lautet OS::Neutron::FloatingIP). Wichtig ist, dass der Floating IP der entsprechende Port und welches das öffentliche Netz genutzt wird, mitgeteilt wird: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } . ",
    "url": "/optimist/guided_tour/step18/#floating-ip",
    
    "relUrl": "/optimist/guided_tour/step18/#floating-ip"
  },"185": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Security Groups",
    "content": "Wird das oben geschriebene Template gestartet, würde die Instanz erstellt werden, nur kann diese aufgrund der voreingestellten Security Group nicht erreicht werden. Um dies zu ändern, wird eine Security Group (Typ = OS::Neutron::SecurityGroup). Auch gibt es einige Besonderheiten zu beachten, es wird zum einen mit Regeln (rules) gearbeitet und zum anderen müssen selbige noch dem Port zugewiesen werden. So kann die direction (Richtung des Traffics) in ingress (eingehend) oder egress (ausgehend) eingeteilt, der entsprechende Port oder auch die Range der Ports definiert und auch das Protokoll festgelegt werden. Außerdem kann mit remote_ip_prefix noch festgelegt werden, wer die Instanz erreicht (falls nötig). heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol: tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . ",
    "url": "/optimist/guided_tour/step18/#security-groups",
    
    "relUrl": "/optimist/guided_tour/step18/#security-groups"
  },"186": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "Abschluss",
    "content": "Die erstellte Instanz ist von außen erreichbar inklusive einer öffentliche IP. Im nächsten Schritt wird die Instanz per CloudConfig angepasst. ",
    "url": "/optimist/guided_tour/step18/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step18/#abschluss"
  },"187": {
    "doc": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "title": "18: Unsere Instanz wird von außen per IPv4 erreichbar",
    "content": " ",
    "url": "/optimist/guided_tour/step18/",
    
    "relUrl": "/optimist/guided_tour/step18/"
  },"188": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "Schritt 19: Unsere Instanz lernt IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step19/#schritt-19-unsere-instanz-lernt-ipv6",
    
    "relUrl": "/optimist/guided_tour/step19/#schritt-19-unsere-instanz-lernt-ipv6"
  },"189": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "Vorwort",
    "content": "Nachdem im letzten Schritt die Instanz mit einer öffentlichen IPv4 Adresse versehen wurde und diese auch per SSH erreichbar ist, wird es nun Zeit die Instanz selber anzupassen. Dafür nutzen wir in diesem Schritt CloudConfig und passen auch die Security Group an. ",
    "url": "/optimist/guided_tour/step19/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step19/#vorwort"
  },"190": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "CloudConfig",
    "content": "Ist eine Ressource und wird daher auch unter resources geführt. (Typ = OS::HEAT::CloudConfig) . Es gibt sehr viele Möglichkeiten, was alles in einer Instanz mit CloudConfig bearbeiten werden kann. Im diesem Schritt beschäftigen wir uns damit, alles notwendige für IPv6 vorzubereiten. Der Start macht hierbei das erstellen der entsprechenden Dateien mit dem notwendigen Inhalt, den wir bereits aus Schritt 11: Zugriff aus dem Internet vorbereiten: Wir ergänzen IPv6 kennen und nutzen CloudConfig in der cloud_config den Befehl write_files: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port }   Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - #MussNochEingetragenWerden - #MussNochEingetragenWerden network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . Wir haben die Dateien erstellt und den entsprechenden Inhalt eingefügt. Wie in Schritt 11: Zugriff aus dem Internet vorbereiten: Wir ergänzen IPv6 beschrieben, ist es noch notwendig das Interface mit dem Befehl runcmd neu zustarten. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . Im letzten Schritt passen wir die Security Group an, damit auch ein Zugriff über IPv6 möglich ist. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } - { direction: ingress, remote_ip_prefix: \"::/0\", port_range_min: 22, port_range_max: 22, protocol: tcp, ethertype: IPv6 } - { direction: ingress, remote_ip_prefix: \"::/0\", protocol: ipv6-icmp, ethertype: IPv6 } . ",
    "url": "/optimist/guided_tour/step19/#cloudconfig",
    
    "relUrl": "/optimist/guided_tour/step19/#cloudconfig"
  },"191": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "Abschluss",
    "content": "Wir haben nun die Möglichkeit Instanzen per Cloud-Init anzupassen und IPv6 nutzbar gemacht. Im nächsten und letzten Schritt werden wir mehrere Instanzen per Heat starten. ",
    "url": "/optimist/guided_tour/step19/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step19/#abschluss"
  },"192": {
    "doc": "19: Unsere Instanz lernt IPv6",
    "title": "19: Unsere Instanz lernt IPv6",
    "content": " ",
    "url": "/optimist/guided_tour/step19/",
    
    "relUrl": "/optimist/guided_tour/step19/"
  },"193": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Schritt 20: Mehrere Instanzen gleichzeitig erstellen",
    "content": " ",
    "url": "/optimist/guided_tour/step20/#schritt-20-mehrere-instanzen-gleichzeitig-erstellen",
    
    "relUrl": "/optimist/guided_tour/step20/#schritt-20-mehrere-instanzen-gleichzeitig-erstellen"
  },"194": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Vorwort",
    "content": "Nachdem im Schritt 15 eine Instanz inklusive aller wichtigen Einstellungen angelegt wurde, ist der nächste Schritt, mehr als eine Instanz per Template zu starten. In diesem Schritt, werden zwei Instanzen erstellt, die ein gemeinsames Netzwerk nutzen. ",
    "url": "/optimist/guided_tour/step20/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step20/#vorwort"
  },"195": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Start",
    "content": "Neben den beiden Instanzen wird auch das Template aufgeteilt und in zwei Dateien erstellt. Dies hat verschiedene Teile und den Start macht ein simples Template, welches nur ein Net und ein Subnet enthält: . heat_template_version: 2014-10-16 description: Ein simples Template welches 2 Instanzen erstellt resources: BeispielNet: type: OS::Neutron::Net properties: name: BeispielNet BeispielSubnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: BeispielNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . Dies stellt das Grundgerüst des Stacks dar und wird vorerst als Gruppen.yaml gespeichert. Die Instanzen selber werden in einer zweiten Datei BeispielServer.yaml beschrieben, welche dem gleichen Aufbau wie in den vorigen Schritten folgt. Um image: zu füllen kann wahlweise der Image Name oder die Image-ID benutzt werden. Eine korrekte Image-ID bzw. einen korrekten Namen erhält man mit openstack image list. Es ist wichtig, dass kein Server-Namen definiert wird und network_id auch keinen Eintrag erfährt: . heat_template_version: 2014-10-16 description: Ein einzelner Server der durch eine Ressourcen Gruppe verwendet wird parameters: network_id: type: string server_name: type: string resources: Instanz: type: OS::Nova::Server properties: user_data_format: RAW image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small name: { get_param: server_name } networks: - port: { get_resource: BeispielPort } BeispielPort: type: OS::Neutron::Port properties: network: { get_param: network_id } . Nachdem die Datei fertiggestellt wurde, wird diese wie oben beschrieben als BeispielServer.yaml gespeichert. Um weiter fortzufahren, wird die Arbeit am ursprünglichen Template (Gruppen.yaml) fortgesetzt. Hier gilt es nun, das zweite erstellte Template als RessourceGroup einzubinden. Auch ist so direkt die Möglichkeit gegeben, die Anzahl der Instanzen, die Namen etc. anzugeben: . heat_template_version: 2014-10-16 description: Ein simples Template welches 2 Instanzen erstellt resources:   BeispielInstanzen: type: OS::Heat::ResourceGroup depends_on: BeispielSubnet properties: count: 2 resource_def: type: BeispielServer.yaml properties: network_id: { get_resource: BeispielNet } server_name: BeispielInstanz_%index% BeispielNet: type: OS::Neutron::Net properties: name: BeispielNet BeispielSubnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: BeispielNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . Nachdem die Arbeit an Gruppen.yaml abgeschlossen wurde, kann der so erstellte Stack direkt mit dem OpenStackClient gestartet werden: . openstack stack create -t Gruppen.yaml &lt;Name des Stacks&gt; . ",
    "url": "/optimist/guided_tour/step20/#start",
    
    "relUrl": "/optimist/guided_tour/step20/#start"
  },"196": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "Abschluss",
    "content": "Nachdem am Anfang der Guided Tour noch Instanzen per Hand erstellt wurden, können nun bereits mehrere Instanzen gleichzeitig per Template ausgerollt werden und stellen einen guten Startpunkt für die Administration von OpenStack dar. ",
    "url": "/optimist/guided_tour/step20/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step20/#abschluss"
  },"197": {
    "doc": "20: Mehrere Instanzen gleichzeitig erstellen",
    "title": "20: Mehrere Instanzen gleichzeitig erstellen",
    "content": " ",
    "url": "/optimist/guided_tour/step20/",
    
    "relUrl": "/optimist/guided_tour/step20/"
  },"198": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Schritt 21: Eine Instanz von einem SSD-Volume starten",
    "content": " ",
    "url": "/optimist/guided_tour/step21/#schritt-21-eine-instanz-von-einem-ssd-volume-starten",
    
    "relUrl": "/optimist/guided_tour/step21/#schritt-21-eine-instanz-von-einem-ssd-volume-starten"
  },"199": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Vorwort",
    "content": "In den vorigen Schritten haben wir uns bereits eine eigene Instanz erstellt und auch die ersten Grundlagen in HEAT sind gelegt. Wir werden in diesem Schritt eine Instanz von einem Volume starten und dafür den SSD-Speicher nutzen. Auch hier gibt es mehrere Wege unser Ziel zu erreichen, daher werden wir in diesem Schritt sowohl das Horizon(Dashboard) nutzen, als auch unser HEAT Template aus Schritt 18 weiter modifizieren. ",
    "url": "/optimist/guided_tour/step21/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step21/#vorwort"
  },"200": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Der Weg über das Horizon(Dashboard)",
    "content": "Um zu starten, loggen wir uns zunächst wie in “Schritt 1: Das Dashboard (Horizon)” erklärt ins Horizon(Dashboard) ein. Hier wechseln wir links in der Seitenleiste auf Project → Volumes → Volumes und klicken dann rechts auf “+ Create Volume” . In dem sich öffnenden Fenster geben wir folgende Optionen an und klicken dann auf “Create Volume”: . | Volume Name: Hier wird der Name des Volumes vergeben, dieser kann frei gewählt werden. Im Beispiel wird nach der Auswahl des Images automatisch “Ubuntu 16.04 Xenial Xerus - Latest” eingetragen. | Description: In diesem Feld kann eine Beschreibung hinzugefügt werden, je nach Bedarf. Im Beispiel wird keine Beschreibung verwendet. | Volume Source: Hier kann zwischen “Image” und “No source, empty image” gewählt werden. Für unser Beispiel nutzen wir “Image”. | Use image as a source: Es kann ein beliebiges Image genutzt werden. Im Beispiel wird “Ubuntu 16.04 Xenial Xerus - Latest (276.2 MB)” verwendet. | Type: Hier besteht die Wahl zwischen “high-iops”, “low-iops” und “default”. Da wir SSD-Speicher nutzen wollen, wählen wir “high-iops” aus. | Size: In diesem Feld bestimmen wir die Größe des Volumes, bei unseren Flavors sind es 20 GiB, daher nutzen wir dies auch für unser Beispiel | Availability Zone: Hier kann man zwischen 3 Optionen “Any Availability Zone”, “es1” oder “ix1” wählen und die entsprechende Zone festlegen. Im Beispiel nutzen wir ix1. | . Nachdem das Horizon das Volume korrekt erstellt hat, sollte es in etwa so aussehen: . Um eine neue Instanz von diesem Volume zu starten, können wir entweder rechts auf den Pfeil nach unten, neben “Edit Volume”, klicken und dann auf “Launch as Instance” oder alternativ dazu kann man auch links in der Seitenleiste auf Compute → Instances wechseln und dort auf “Launch Instane” klicken. Im sich öffnenden Fenster geben wir der Instanz einen Namen (Instance Name), wählen dieselbe Availability Zone wie weiter oben, also ix1 und wechseln dann links auf Source. Unter Source wählen wir Volume als Select Boot Source aus und klicken dann neben unserem erstellten Volume auf den Pfeil nach oben. Nun klicken wir links auf Flavor und wählen einen der möglichen Flavors aus, indem wir auf den Pfeil nach oben neben dem gewünschten Flavor klicken. Im nächsten Schritt wählen wir links, über den Reiter Networks das Netzwerk für die VM aus. Auch hier klicken wir neben dem gewünschten Netzwerk auf den Pfeil nach oben. Damit sind alle wichtigen Einstellungen getroffen und die Instanz kann mit “Launch Instance” gestartet werden. Falls benötigt, können noch eigene Security Groups und/oder Key Pairs der Instanz hinzugefügt werden. ",
    "url": "/optimist/guided_tour/step21/#der-weg-%C3%BCber-das-horizondashboard",
    
    "relUrl": "/optimist/guided_tour/step21/#der-weg-über-das-horizondashboard"
  },"201": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Der Weg über HEAT",
    "content": "Wie bereits im Vorwort erwähnt, nutzen wir unser HEAT Template aus Schritt 18. Dieses Template startet bereits eine Instanz. Damit diese nun aber ein SSD-Volume nutzt, bedarf es einiger Änderungen. Zunächst fügen wir unseren Parametern noch die “availability_zone” hinzu: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 . Der nächste Schritt ist am Ende des Templates einen eigenen Punkt “boot_ssd” für das Volume hinzuzufügen: . boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . Nun haben wir bereits einen Parameter hinzugefügt und nutzten diesen auch direkt in unserem neu erstellten Boot-Volume. Damit die Instanz auch vom Volume startet, überarbeiten wir den Punkt “Instanz” in unserem HEAT-Template . Dort können wir den Punkt “image” entfernen (im Beispiel ist er per # auskommentiert), da dieser ja über das Volume bereitgestellt wird. Wir fügen nun noch die “availability_zone”, einen Namen “name”, das Netzwerk “networks” und das Volume “block_device_mapping” hinzu: . Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] . Damit ist unser HEAT-Template für diesen Schritt fertig und sollte so aussehen: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 resources: Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron:SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol: tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . ",
    "url": "/optimist/guided_tour/step21/#der-weg-%C3%BCber-heat",
    
    "relUrl": "/optimist/guided_tour/step21/#der-weg-über-heat"
  },"202": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "Abschluss",
    "content": "In diesem Schritt haben wir gelernt, dass es ohne Weiteres möglich ist, eine Instanz auch von einem Volume zu starten und auch gleichzeitig schnellen SSD Speicher zu nutzen. Außerdem haben wir unsere HEAT-Kenntnisse aufgefrischt und ein Volume mit eingebunden. ",
    "url": "/optimist/guided_tour/step21/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step21/#abschluss"
  },"203": {
    "doc": "21: Eine Instanz von einem SSD-Volume starten",
    "title": "21: Eine Instanz von einem SSD-Volume starten",
    "content": " ",
    "url": "/optimist/guided_tour/step21/",
    
    "relUrl": "/optimist/guided_tour/step21/"
  },"204": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "Schritt 22: Anlegen eines DNS-Record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/#schritt-22-anlegen-eines-dns-record-in-designate",
    
    "relUrl": "/optimist/guided_tour/step22/#schritt-22-anlegen-eines-dns-record-in-designate"
  },"205": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "Vorwort",
    "content": "Die Openstackplattform Optimist enthält eine Technologie namens DNS-as-a-Service (DNSaaS), auch bekannt als Designate. DNSaaS enthält eine REST-API für die Domänen- und Datensatzverwaltung, ist multi-tenant und integriert den OpenStack Identity Service (Keystone) für die Authentifizierung. Wir werden in diesem Schritt eine fiktive Zone (Domain) mit MX und A-Records erstellen und die entsprechende IP/CNAME hinterlegen. Um zu starten, lesen wir uns zunächst wie in “Schritt 4: Der Weg vom Horizon auf die Kommandozeile” erklärt die Zugangsdaten ein und sorgen dafür das der python-designateclient installiert ist (pip install python-openstackclient python-designateclient) Anschliessend bedienen wir den Openstack-Client und erstellen zuerst eine Zone für unser Projekt. $ openstack zone create --email webmaster@foobar.cloud foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | CREATE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | PENDING | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | None | version | 1 | +----------------+--------------------------------------+ . Man beachte den abschliessenden “.” an der zu erstellenden Zone/Domain. Das Resultat bisher: . $ openstack zone list +--------------------------------------+-----------------------+---------+------------+--------+--------+ | id | name | type | serial | status | action | +--------------------------------------+-----------------------+---------+------------+--------+--------+ | 036ae6e6-6318-47e1-920f-be518d845fb5 | foobar.cloud. | PRIMARY | 1534315524 | ACTIVE | NONE | +--------------------------------------+-----------------------+---------+------------+--------+--------+ $ openstack zone show foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | NONE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | ACTIVE | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | 2018-08-15T06:45:30.000000 | version | 2 | +----------------+--------------------------------------+ . Nun ist die Domain “foobar.cloud” für unser Projekt registriert und einsatzbereit (status: ACTIVE). Wir wollen im nächsten Schritt MX-Records (Datensätze für Mailserver in dieser Zone) für diese Domain erstellen. Doch zuerst schauen wir, welche Inhalte (Recordsets) unsere neue Zone bereits jetzt besitzt. $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 | ACTIVE | NONE | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ . Hier sehen wir eine “leere Hülle” einer Domain mit automatisch erstellen NS und SOA-Einträgen, die sofort zur Abfrage bereit stehen. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud SOA dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 . Anlegen eines MX-Records: Nun können wir Datensätze innerhalb dieser Zone hinzufügen, verändern oder löschen (openstack recordset –help). Als nächstes fügen wir einen MX und einen A-Record hinzu. Bei den MX-Records richten wir auch gleich die typischen Mailserver-Prioritäten (10,20) mit ein. Wobei immer der niedrigere Wert als erstes angesteuert wird und der zweite Eintrag als “Backup” dient. $ openstack recordset create --record '10 mx1.foobar.cloud.' --record '20 mx2.foobar.cloud.' --type MX foobar.cloud. foobar.cloud. +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:15:32.000000 | description | None | id | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | name | foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 10 mx1.foobar.cloud. | | 20 mx2.foobar.cloud. | status | PENDING | ttl | None | type | MX | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534317332 3507 600 86400 3600 | PENDING | UPDATE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | PENDING | CREATE | | | 10 mx1.foobar.cloud. | | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ . $ openstack recordset create --type A --record 1.2.3.4 foobar.cloud. www +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:28:15.000000 | description | None | id | d932688f-21d5-44b1-aa27-030c342788e7 | name | www.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 1.2.3.4 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Resultat: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318095 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . Wenn die Recordsets aktiv sind können wir die dafür vorgesehenen DNS-Server . | dns1.ddns.innovo.cloud | dns2.ddns.innovo.cloud nach diesen Records abfragen. | . $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud MX 10 mx1.foobar.cloud. 20 mx2.foobar.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud www.foobar.cloud 1.2.3.4 . ACHTUNG! Zu diesem Zeitpunkt ist diese Domaine (foobar.cloud) noch nicht weltweit auflösbar. Damit dieses Konstrukt weltweit benutzt werden kann, muss jede im Designate verwaltete Domain bei dem jeweiligen Registrar die Delegation zu den Nameservern dns1.ddns.innovo.cloud und dns2.ddns.innovo.cloud eingerichtet haben. Details zu unseren authoritativen DNS-Server: . | dns1.ddns.innovo.cloud: ‘185.116.244.45’ / ‘2a00:c320:0:1::d’ | dns2.ddns.innovo.cloud: ‘185.116.244.46’ / ‘2a00:c320:0:1::e’ | . Um die die Mail-Records abzuschliessen, bietet sich noch an für die Mailserver entsprechende A-Records zu hinterlegen . $ openstack recordset create --type A --record 2.3.4.5 foobar.cloud. mx1 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:31.000000 | description | None | id | 630d5103-7c02-4a58-83a5-97f802cf141c | name | mx1.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 2.3.4.5 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ und $ openstack recordset create --type A --record 3.4.5.6 foobar.cloud. mx2 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:56.000000 | description | None | id | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | name | mx2.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 3.4.5.6 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Das Resultat nach einigen Sekunden: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318976 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | 630d5103-7c02-4a58-83a5-97f802cf141c | mx1.foobar.cloud. | A | 2.3.4.5 | ACTIVE | NONE | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | mx2.foobar.cloud. | A | 3.4.5.6 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . ",
    "url": "/optimist/guided_tour/step22/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step22/#vorwort"
  },"206": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "Abschluss",
    "content": "In diesem Schritt haben wir gelernt, wie man eine Zone anlegt, einen Recordset konfiguriert und diesen abfragen kann. ",
    "url": "/optimist/guided_tour/step22/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step22/#abschluss"
  },"207": {
    "doc": "22: Anlegen eines DNS-Record in Designate",
    "title": "22: Anlegen eines DNS-Record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/",
    
    "relUrl": "/optimist/guided_tour/step22/"
  },"208": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Schritt 23: Der Object Storage (S3 kompatibel)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/#schritt-23-der-object-storage-s3-kompatibel",
    
    "relUrl": "/optimist/guided_tour/step23/#schritt-23-der-object-storage-s3-kompatibel"
  },"209": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Vorwort",
    "content": "In den vorigen Schritten haben wir bereits einige interessante Bausteine kennengelernt. Als nächstes widmen wir uns dem Object Storage, der uns interessante Möglichkeiten bietet, Dateien zu speichern. ",
    "url": "/optimist/guided_tour/step23/#vorwort",
    
    "relUrl": "/optimist/guided_tour/step23/#vorwort"
  },"210": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Der Start (Benutzerdaten)",
    "content": "Damit wir auf den Object Storage zugreifen können, benötigen wir zunächst Login Daten(Credentials), um auf diesen zugreifen zu können. Dafür benötigen wir den OpenStackClienten(siehe Schritt 4), damit wir per OpenstackAPI die entsprechenden Daten erstellen. Der Befehl in der Kommandozeile dafür lautet: . openstack ec2 credentials create . Wenn die Daten korrekt erstellt worden sind, sieht die Ausgabe in etwa so aus: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | links | {u'self': u'https://identity.optimist.gec.io/v3/users/bbb | | bbbbbbbbbbbbbbbbbbbbbbbbbbbbb/credentials/OS- | | EC2/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'} | project_id | cccccccccccccccccccccccccccccccc | secret | dddddddddddddddddddddddddddddddd | trust_id | None | user_id | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | +------------+-----------------------------------------------------------------+ . Nachdem die Zugangsdaten (Credentials) vorliegen, brauchen wir eine Möglichkeit auf den S3 kompatiblen ObjectStorage zuzugreifen. ",
    "url": "/optimist/guided_tour/step23/#der-start-benutzerdaten",
    
    "relUrl": "/optimist/guided_tour/step23/#der-start-benutzerdaten"
  },"211": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Zugriff auf den S3 kompatiblen ObjectStorage",
    "content": "Es gibt natürlich verschiedene Optionen auf den ObjectStorage zuzugreifen. Wir empfehlen dafür die Nutzung von s3cmd Dieses kleine Tool ist einfach zu bedienen und zu nutzen. Da wir bereits in Schritt 4 “pip” als Paketmanager installiert haben und nutzen, können wir S3cmd auch über “pip” installieren: . pip install s3cmd . Da jetzt S3cmd installiert ist, müssen die vorher erstellten Zugangsdaten (Credentials) eingetragen werden, nur so lässt sich S3cmd auch korrekt nutzen. Alle wichtigen Informationen finden wir in der .s3cfg , sollte diese noch nicht existieren, erstellen wir diese vorher. Folgende Daten tragen wir dann in der .s3cfg ein: . access_key = aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = dddddddddddddddddddddddddddddddd use_https = True . ",
    "url": "/optimist/guided_tour/step23/#zugriff-auf-den-s3-kompatiblen-objectstorage",
    
    "relUrl": "/optimist/guided_tour/step23/#zugriff-auf-den-s3-kompatiblen-objectstorage"
  },"212": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Der Bucket",
    "content": "Da wir Zugriff auf den S3 kompatiblen Object Storage haben, ist es an der Zeit damit auch zu arbeiten. Alle verfügbaren Befehle von s3cmd können mit folgendem Befehl angezeigt werden: . s3cmd --help . Als Nächstes erstellen wir einen Bucket. Buckets entsprechen dabei im weitesten Sinne Ordnern, die wir für eine Struktur benötigen. Eine Datei kann also nur in einem existierenden Bucket gespeichert werden und der Name vom Bucket selber ist einzigartig (über den gesamten Optimist). Wenn also bereits ein Bucket mit dem Namen “Test” besteht, kann dieser nicht erneut angelegt werden. Daher ist es aus unserer Sicht eine gute Option, eine UUID zu nutzen und diese dann in der entsprechenden Applikation aufzulösen. Auch gibt es die Möglichkeit, bei Buckets und auch bei Dateien, zwischen public und private zu unterscheiden. Alle Buckets die erstellt und Dateien die hochgeladen werden, sind per default private, d.h. wenn keine weiteren Einstellungen vorgenommen werden, kann nur der Ersteller auf den Bucket und den Inhalt zugreifen. Dies lässt sich zum Beispiel per Access Control List (ACL) ändern. WICHTIG: Sollte man einen kompletten Bucket auf public stellen, können auch Informationen über Dateien, in diesem Bucket, die auf private gesetzt sind, abgerufen werden. Da wir die wichtigsten Details kennen, ist es Zeit, einen Bucket mit einer UUID zu erstellen: . $ s3cmd mb s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 Bucket 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/' created . ",
    "url": "/optimist/guided_tour/step23/#der-bucket",
    
    "relUrl": "/optimist/guided_tour/step23/#der-bucket"
  },"213": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Eine Datei hochladen",
    "content": "Da wir jetzt einen Bucket erstellt haben, ist der nächste Schritt, eine oder auch mehrere Dateien hochzuladen. Dafür nehmen wir den Befehl s3cmd put Dateiname s3://Name_des_Buckets und eine Ausgabe kann dann so aussehen: . $ s3cmd put test.yaml s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 upload: 'test.yaml' -&gt; 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml' [1 of 1] 4218 of 4218 100% in 0s 4.61 kB/s done . ",
    "url": "/optimist/guided_tour/step23/#eine-datei-hochladen",
    
    "relUrl": "/optimist/guided_tour/step23/#eine-datei-hochladen"
  },"214": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Zugriff auf die Datei erhalten",
    "content": "Die generelle URL für den Zugriff auf Dateien lautet im Optimisten https://s3.es1.fra.optimist.gec.io/Name_des_Buckets/Dateiname. Damit auf die Datei aus unserem Beispiel zugegriffen werden kann, ist es notwendig, die Einstellung von private auf public zu ändern. Dafür können wir, wie bereits unter dem Punkt “Der Bucket” erwähnt, die Access Control List (ACL) nutzen: . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-public s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Public [1 of 1] . Die Datei kann jetzt über folgenden Link aufgerufen werden: https://s3.es1.fra.optimist.gec.io/e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml . Um sie wieder auf private zu stellen, nutzen wir folgenden Befehl: . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-private s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Private [1 of 1] . ",
    "url": "/optimist/guided_tour/step23/#zugriff-auf-die-datei-erhalten",
    
    "relUrl": "/optimist/guided_tour/step23/#zugriff-auf-die-datei-erhalten"
  },"215": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "Abschluss",
    "content": "In diesem Schritt haben wir den S3 kompatiblen Storage kennengelernt und die ersten Schritte im Umgang damit geübt. ",
    "url": "/optimist/guided_tour/step23/#abschluss",
    
    "relUrl": "/optimist/guided_tour/step23/#abschluss"
  },"216": {
    "doc": "23: Der Object Storage (S3 kompatibel)",
    "title": "23: Der Object Storage (S3 kompatibel)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/",
    
    "relUrl": "/optimist/guided_tour/step23/"
  },"217": {
    "doc": "Netzwerke",
    "title": "Netzwerke",
    "content": " ",
    "url": "/optimist/networking/",
    
    "relUrl": "/optimist/networking/"
  },"218": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Port Forwarding auf Floating IPs",
    "content": "Floating IP Port Forwarding erlaubt die Weiterleitung eines beliebigen TCP/UDP/anderen Protokoll-Ports einer Floating IP-Adresse an einen TCP/UDP/anderen Protokoll-Port, der mit einer festen IP-Adresse eines Neutron-Ports verbunden ist. ",
    "url": "/optimist/networking/port_forwarding/",
    
    "relUrl": "/optimist/networking/port_forwarding/"
  },"219": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Port Forwarding auf einer Floating IP erstellen",
    "content": "Um ein Port forwarding auf eine Floating IP anzuwenden, sind die folgenden Informationen erforderlich: . | Die zu verwendende interne IP-Adresse | Die UUID des Ports, der mit der Floating IP assoziiert werden soll | Die Portnummer des Netzwerkports der festen IPv4-Adresse | Die externe Portnummer der Floating IP-Adresse | Das spezifische Protokoll, das bei der Port-Weiterleitung zu verwenden ist (in diesem Beispiel TCP) | Die Floating IP, auf der dieser Port freigeschalten werden soll. (in diesem Beispiel 185.116.244.141) | . Das folgende Beispiel zeigt die Erstellung eines Port Forwarding auf einer Floating IP unter Verwendung der erforderlichen Optionen: . $ openstack floating ip port forwarding create \\ --internal-ip-address 10.0.0.14 \\ --port 12c29300-0f8a-4c54-a9dc-bee4c12c6ad2 \\ --internal-protocol-port 80 \\ --external-protocol-port 8080 \\ --protocol tcp 185.116.244.141 . ",
    "url": "/optimist/networking/port_forwarding/#port-forwarding-auf-einer-floating-ip-erstellen",
    
    "relUrl": "/optimist/networking/port_forwarding/#port-forwarding-auf-einer-floating-ip-erstellen"
  },"220": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Anzeigen der Port Forwarding Einstellungen bestimmter Floating IPs",
    "content": "Innerhalb eines Projekts kann eine Liste der Port Forwarding-Regeln, die für eine bestimmte Floating IP gelten, mit dem folgenden Befehl abgerufen werden: . $ openstack floating ip port forwarding list 185.116.244.141 . Der obige Befehl kann weiter gefiltert werden, indem vor der Floating IP die Flags --sort-column, --port, --external-protcol-port und/oder --protocol verwendet werden. ",
    "url": "/optimist/networking/port_forwarding/#anzeigen-der-port-forwarding-einstellungen-bestimmter-floating-ips",
    
    "relUrl": "/optimist/networking/port_forwarding/#anzeigen-der-port-forwarding-einstellungen-bestimmter-floating-ips"
  },"221": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Anzeigen der Details einer port forwarding-Regel",
    "content": "Um Details zu einer bestimmten Port Forwarding-Regel anzuzeigen, kann der folgende Befehl verwendet werden: . $ openstack floating ip port forwarding show &lt;floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#anzeigen-der-details-einer-port-forwarding-regel",
    
    "relUrl": "/optimist/networking/port_forwarding/#anzeigen-der-details-einer-port-forwarding-regel"
  },"222": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Ändern von Floating IP Port Forwarding-Eigenschaften",
    "content": "Wenn eine Port Forwarding-Konfiguration auf einer Floating IP bereits mit $ openstack floating ip port forwarding create erstellt wurde, können Änderungen an der bestehenden Konfiguration mit $ openstack floating ip port forwarding set ... vorgenommen werden. Die folgenden Parameter eines Port Forwardings können geändert werden: . | --port: Die UUID des Ports | --internal-ip-address: Die zum Zielport der Forwarding-Regel gehoerende feste interne IPv4-Adresse | --internal-protocol-port: Die interne TCP/UDP/etc. Portnummer auf die die Floating IPs Port Forwarding-Regel weiterleitet | --external-protocol-port: Die TCP/UDP/etc. Portnummer der Floating-IP-Adresse des Port Forwardings | --protocol: Das IP-Protokoll, das in der Floating IP Port Forwarding-Regel verwendet wird (TCP/UDP/andere) | --description: Text zur Beschreibung der Verwendung der Port Forwarding-Konfiguration | . Die Konfiguration jeder der oben genannten Parameter kann mit einer Variation des folgenden Befehls geändert werden: . $ openstack floating ip port forwarding set \\ --port &lt;port&gt; \\ --internal-ip-address &lt;internal-ip-address&gt; \\ --internal-protocol-port &lt;port-number&gt; \\ --extern-protocol-port &lt;port-number&gt; \\ --protocol &lt;protocol&gt; \\ --description &lt;description&gt; \\ &lt;Floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#%C3%A4ndern-von-floating-ip-port-forwarding-eigenschaften",
    
    "relUrl": "/optimist/networking/port_forwarding/#ändern-von-floating-ip-port-forwarding-eigenschaften"
  },"223": {
    "doc": "Port Forwarding auf Floating IPs",
    "title": "Löschen der Port Forwarding-Konfiguration zu einer Floating IP",
    "content": "Um eine Port Forwarding-Regel von einer Floating IP zu entfernen, benötigen wir die folgenden Informationen: . | Die Floating IP dessen Port Forwarding-Regel entfernt werden soll | Die Port Forwarding ID (Diese ID wird bei der Erstellung erzeugt und kann mit dem Befehl $ openstack Floating ip port forwarding list ... angezeigt werden) | . Mit dem folgenden Befehl lässt sich die Konfiguration für ein Floating IP Port Forwarding löschen: . $ openstack floating ip port forwarding delete &lt;Floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#l%C3%B6schen-der-port-forwarding-konfiguration-zu-einer-floating-ip",
    
    "relUrl": "/optimist/networking/port_forwarding/#löschen-der-port-forwarding-konfiguration-zu-einer-floating-ip"
  },"224": {
    "doc": "Geteilte Netzwerke",
    "title": "Geteilte Netzwerke",
    "content": " ",
    "url": "/optimist/networking/shared_networks/",
    
    "relUrl": "/optimist/networking/shared_networks/"
  },"225": {
    "doc": "Geteilte Netzwerke",
    "title": "Motivation",
    "content": "Es kommt oft die Frage auf, ob es möglich ist ein Netzwerk zwischen zwei Projekten im OpenStack zu teilen. ",
    "url": "/optimist/networking/shared_networks/#motivation",
    
    "relUrl": "/optimist/networking/shared_networks/#motivation"
  },"226": {
    "doc": "Geteilte Netzwerke",
    "title": "Netzwerk teilen",
    "content": "Zugriff auf beide Projekte ist vorhanden: . Damit das Netzwerk geteilt werden kann, brauchen wir zum einen den OpenStackClient, sowie die Projekt-ID in welches das Netzwerk geteilt werden soll und die Netzwerk-ID des zu teilenden Netzwerks. Die Projekt-ID findet sich in der Ausgabe unter “id” wenn wir folgenden Befehl benutzen: . openstack project show &lt;Name des Projekts&gt; -f value -c id . Als nächstes benötigen wir noch die Netzwerk-ID des zu teilenden Netzwerks, diese finden wir in der Ausgabe unter “id” wenn folgender Befehl genutzt wird: . openstack network show &lt;Name des Netzwerks&gt; -f value -c id . Mit den erhaltenen IDs kann nun das Netzwerk in das entsprechende Projekt geteilt werden, dafür benutzen wir die rollenbasierte Zugriffskontrolle (RBAC): . openstack network rbac create --type network --action access_as_shared --target-project &lt;ID des Projekts&gt; &lt;ID des zu teilenden Netzwerks&gt; . Zugriff auf beide Projekte ist nicht vorhanden: . In diesem Fall kann das Netzwerk nur vom Support, nach voriger Freigabe des anderen Projekt Inhabers, geteilt werden. Um ein Netzwerk mit einem Projekt zu teilen, schreiben Sie uns bitte eine E-Mail an support@gec.io mit folgenden Angaben: . | Name und ID des Netzwerks, welches geteilt werden soll | Name und ID des Projekts, in welchem das Netzwerk sichtbar sein soll | . ",
    "url": "/optimist/networking/shared_networks/#netzwerk-teilen",
    
    "relUrl": "/optimist/networking/shared_networks/#netzwerk-teilen"
  },"227": {
    "doc": "Geteilte Netzwerke",
    "title": "Wichtige Informationen zum geteilten Netzwerk",
    "content": "Wenn man auf ein geteiltes Netzwerk zugreift, gibt es Einschränkungen, die beachtet werden müssen. Eine Einschränkung ist, dass keine Remote Security-Groups benutzt werden können. Auch gibt es keinen Einblick in Ports und IP Adressen vom anderen Projekt gibt. Daher kann man auch keine konkrete IP Adressen für neue Ports in einem Subnetz (im geteilten Netzwerk) angeben, da es so möglich wäre, IPs zu finden die bereits genutzt werden. Damit man das geteilte Netzwerk sinnvoll nutzen kann, gibt es die Möglichkeit einen neuen Port zu erstellen. Dieser erhält dann eine beliebige IP-Adresse und kann weiter genutzt werden, um zum Beispiel einen Router über diesen Port hinzuzufügen. Im Dashboard (Horizon) ist dies nicht möglich und der OpenStackClient wird benötigt. Bitte achten Sie darauf bei den Bezeichnungen keine Leer- und/oder Sonderzeichen zu nutzen, da die Nutzung selbiger zu Problem führen kann. Zuerst erstellen wir den Port und geben dort das geteilte Netzwerk an: . openstack port create --network &lt;ID des geteilten Netzwerks&gt; &lt;Name des Ports&gt; . Jetzt kann zum Beispiel ein Router erstellt und dann dem neu erstellten Port zugeordnet werden: . ##Erstellung des Routers $ openstack router create &lt;Name des Routers&gt; ##Port dem Router zuordnen $ openstack router add port &lt;Name des Routers&gt; &lt;Name des Ports&gt; . ",
    "url": "/optimist/networking/shared_networks/#wichtige-informationen-zum-geteilten-netzwerk",
    
    "relUrl": "/optimist/networking/shared_networks/#wichtige-informationen-zum-geteilten-netzwerk"
  },"228": {
    "doc": "Geteilte Netzwerke",
    "title": "Netzwerk Topology Projekt 1",
    "content": ". Das Netzwerk “shared” aus dem Projekt 1 wird mit Projekt 2 geteilt. In diesem Netzwerk steht der Service “Example” zur Verfügung der dort auf einer Instanz läuft. ",
    "url": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-1",
    
    "relUrl": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-1"
  },"229": {
    "doc": "Geteilte Netzwerke",
    "title": "Netzwerk Topology Projekt 2",
    "content": ". Das Netzwerk “shared” ist auch in Projekt 2 sichtbar und wurde dort an den Router “router2” angehangen. Zusätzlich existiert dort das Netzwerk “network”, aus dem auf die Services in dem Netzwerk “shared” zugegriffen wird. Dabei muss berücksichtigt werden, das im Subnet des “shared” Networks in Projekt 1 die entsprechende Route unter dem Eintrag “Host Routes” gesetzt wird, um einen korrekten Rücktransport der Pakete zu ermöglichen. Im unserem Beispiel ist die folgende Route notwendig: 10.0.1.0/24,10.0.0.1 . ",
    "url": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-2",
    
    "relUrl": "/optimist/networking/shared_networks/#netzwerk-topology-projekt-2"
  },"230": {
    "doc": "Octavia Loadbalancers",
    "title": "Der Octavia Loadbalancer",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/#der-octavia-loadbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#der-octavia-loadbalancer"
  },"231": {
    "doc": "Octavia Loadbalancers",
    "title": "Vorwort",
    "content": "Octavia ist eine hochverfügbare und skalierbare Open-Source Load-Balancing-Lösung, die für die Arbeit mit OpenStack entwickelt wurde. Octavia erledigt das Load-Balancing nach Bedarf, indem es virtuelle Maschinen – auch Amphoren genannt – in seinem Projekt verwaltet und konfiguriert. In diesen Amphoren wirkt schlussendlich ein HAproxy. ",
    "url": "/optimist/networking/octavia_loadbalancer/#vorwort",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#vorwort"
  },"232": {
    "doc": "Octavia Loadbalancers",
    "title": "Der Start",
    "content": "Für die Nutzung von Octavia ist es notwendig, dass der Client auf dem eigenen System installiert ist. Eine Anleitung für sein System findet man unter Schritt 4) . ",
    "url": "/optimist/networking/octavia_loadbalancer/#der-start",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#der-start"
  },"233": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellung eines Octavia-Ladbalancer",
    "content": "Für unser Beispiel nutzen wir das aus Schritt 10 schon bestehende BeispielSubnet. $ openstack loadbalancer create --name Beispiel-LB --vip-subnet-id 32259126-dd37-44d5-922c-99d68ee870cd +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | flavor_id | | id | e94827f0-f94d-40c7-a7fd-b91bf2676177 | listeners | | name | Beispiel-LB | operating_status | OFFLINE | pools | | project_id | b15cde70d85749689e08106f973bb002 | provider | amphora | provisioning_status | PENDING_CREATE | updated_at | None | vip_address | 10.0.0.10 | vip_network_id | f2a8f00e-204b-4c37-9d19-1d5c8e4efbf6 | vip_port_id | 37fc5b34-ee07-49c8-b054-a8d591a9679f | vip_qos_policy_id | None | vip_subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | +---------------------+--------------------------------------+ . Nun geht Octavia her und spawned seine Amphoreninstanzen im Hintergrund. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | PENDING_CREATE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . Mit dem provisioning_status ACTIVE ist dieser Vorgang erfolgreich abgeschlossen und der erste Octavia-Loadbalancer kann weiter konfiguiert werden. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | ACTIVE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellung-eines-octavia-ladbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellung-eines-octavia-ladbalancer"
  },"234": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen eines LB-Listener",
    "content": "In unserem Beispiel wollen wir einen Listener für den HTTP-Port 80 erstellen. Als Listener ist hier - vergleichbar mit anderen LB-Lösungen - der Port des Frontends gemeint. $ openstack loadbalancer listener create --name Beispiel-listener --protocol HTTP --protocol-port 80 Beispiel-LB +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2019-05-01T09:00:00 | default_pool_id | None | default_tls_container_ref | None | description | | id | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | insert_headers | None | l7policies | | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | name | Beispiel-listener | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | protocol_port | 80 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | | client_authentication | | client_crl_container_ref | +-----------------------------+--------------------------------------+ . Der Befehl war erfolgreich, wenn der admin_state_up True ist. $ openstack loadbalancer listener list +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | id | default_pool_id | name | project_id | protocol | protocol_port | admin_state_up | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | None | Beispiel-listener | b15cde70d85749689e08106f973bb002 | HTTP | 80 | True | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-listener",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-listener"
  },"235": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen eines LB-Pools",
    "content": "Als LB-Pool ist hier eine Ansammlung aller Objekte (Listeners, Member, etc.) für zum Beispiel eine Region gemeint - vergleichbar mit einem Pool an öffentlichen IP-Adressen aus derer man sich eine belegen kann. Einen Pool für unser Beispiel erstellt man wie folgt: . $ openstack loadbalancer pool create --name Beispiel-pool --lb-algorithm ROUND_ROBIN --listener Beispiel-listener --protocol HTTP +----------------------+--------------------------------------+ | Field | Value | +----------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | healthmonitor_id | | id | 4053e88e-c2b5-47c6-987e-4387d837c88d | lb_algorithm | ROUND_ROBIN | listeners | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | members | | name | Beispiel-pool | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | provisioning_status | PENDING_CREATE | session_persistence | None | updated_at | None | tls_container_ref | | ca_tls_container_ref | | crl_container_ref | | tls_enabled | +----------------------+--------------------------------------+ . Hier sei erwähnt, dass man mit openstack loadbalancer pool create --help sich alle möglichen Einstellungen anzeigen lassen kann. Die häufigsten Einstellungen und deren Auswahlmöglichkeiten: . --protocol: {TCP,HTTP,HTTPS,TERMINATED_HTTPS,PROXY,UDP} --lb-algorithm {SOURCE_IP,ROUND_ROBIN,LEAST_CONNECTIONS} . Der Pool ist erfolgreich erstellt, wenn der provisioning_status den Status ACTIVE erreicht hat. $ openstack loadbalancer pool list +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | id | name | project_id | provisioning_status | protocol | lb_algorithm | admin_state_up | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | 4053e88e-c2b5-47c6-987e-4387d837c88d | Beispiel-pool | b15cde70d85749689e08106f973bb002 | ACTIVE | HTTP | ROUND_ROBIN | True | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-pools",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-lb-pools"
  },"236": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen der LB-member",
    "content": "Damit unser Loadbalancer weiß, an welche Backends er weiterleiten darf, fehlen uns noch sogenannte member, die wir wie folgt definieren: . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.11 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.11 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . und . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.12 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.12 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 2add1e17-73a6-4002-82af-538a3374e5dc | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . Hier sei erwähnt, dass die beiden IP’s aus 10.0.0.* bereits vorhandene, auf Port 80 lauschende Webserver sind, die eine einfache Webseite mit Info’s über ihren Servicenamen ausliefern. Unter der Vorraussetzung es handelt sich bei diesen Webservern im folgenden Beispiel um ein Ubuntu/Debian und man hat root-Berechtigungen, könnte man die einfache Webseite schnell erstellen mit: . root@BeispielInstanz1:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver1\" &gt; /var/www/html/index.html . root@BeispielInstanz2:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver2\" &gt; /var/www/html/index.html . Das Resultat vom Anlegen der Member können wir wie folgt überprüfen: . $ openstack loadbalancer member list Beispiel-pool +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | id | name | project_id | provisioning_status | address | protocol_port | operating_status | weight | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.11 | 80 | NO_MONITOR | 1 | 2add1e17-73a6-4002-82af-538a3374e5dc | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.12 | 80 | NO_MONITOR | 1 | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ . Nun ist das “interne” Konstrukt des Loadbalancers konfiguriert. Wir haben nun: . | 2 member die über Port 80 den tatsächlichen Service bereitstellen und zwischen denen das Loadbalancing stattfindet, | einen pool für diese member, | einen listener, der auf Port TCP/80 lauscht und ein ROUND_ROBIN zu den beiden Endpunkten macht und | einen Loadbalancer, über den wir alle Komponenten vereint haben. | . Der operating_status NO_MONITOR wird unter healthmonitor korrigiert. ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-der-lb-member",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-der-lb-member"
  },"237": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen und konfigurieren der Floating-IP",
    "content": "Damit wir auch den Loadbalancer außerhalb unseres Beispiel-Netzwerk einsetzen können, müssen wir eine FloatingIP reservieren und diese dann mit dem vip_port_id des Beispiel-LB verknüpfen. Mit folgendem Befehl erstellen wir uns eine Floating IP aus dem provider-Netz: . $ openstack floating ip create provider +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2019-05-01T09:00:00Z | description | | dns_domain | None | dns_name | None | fixed_ip_address | None | floating_ip_address | 185.116.247.133 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 46c0e8cf-783d-44a0-8256-79f8ae0be7fe | location | Munch({'project': Munch({'domain_id': 'default', 'id': u'b15cde70d85749689e08106f973bb002', 'name': 'beispiel-tenant', 'domain_name': None}), 'cloud': '', 'region_name': 'fra', 'zone': None}) | name | 185.116.247.133 | port_details | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 0 | router_id | None | status | DOWN | subnet_id | None | tags | [] | updated_at | 2019-05-01T09:00:00Z | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ . Im nächsten Schritt benötigen wir die vip_port_id des Loadbalancers. Diese bekommt man mit folgendem Befehl heraus: . $ openstack loadbalancer show Beispiel-LB -f value -c vip_port_id 37fc5b34-ee07-49c8-b054-a8d591a9679f . Mit dem folgendem Befehl weisen wir dem Loadbalancer nun die öffentliche IP Adresse zu. Damit ist der LB (und somit auch die Endpunkte dahinter) aus dem Internet erreichbar. openstack floating ip set --port 37fc5b34-ee07-49c8-b054-a8d591a9679f 185.116.247.133 . Wir sind soweit, dass wir unser Loadbalancer-Deployment testen können. Mit folgendem Befehl fragen wir unseren Loadbalancer über Port TCP/80 an und bekommen anschließend eine entsprechende Antwort von den einzelnen member zurück: . $ for ((i=1;i&lt;=10;i++)); do curl http://185.116.247.133; sleep 1; done you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 ... (usw.) . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-und-konfigurieren-der-floating-ip",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-und-konfigurieren-der-floating-ip"
  },"238": {
    "doc": "Octavia Loadbalancers",
    "title": "Erstellen eines healthmonitor",
    "content": "Mit dem folgenden Befehl erstellen wir einen Monitor, der bei einem Ausfall eines der Backends genau dieses fehlerhafte Backend aus der Lastverteilung nimmt und somit die Webseite oder Applikation weiterhin sauber ausgeliefert wird. $ openstack loadbalancer healthmonitor create --delay 5 --max-retries 2 --timeout 10 --type HTTP --name Beispielmonitor --url-path / Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | PENDING_CREATE | updated_at | None | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | OFFLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . In diesem Beispiel entfernt der Monitor das fehlerhafte Backend aus dem Pool, wenn die Integritätsprüfung (–type HTTP, –url-path / ) alle zwei Fünf-Sekunden-Intervalle fehlschlägt(–delay 5, –max-retries 2, –timeout 10). Wenn der Server wiederhergestellt wird und erneut auf TCP/80 reagiert, wird er erneut zum Pool hinzugefügt. Ein manueller Failover kann erzwungen werden, indem der Statuscodes des Webservers ungleich “200” ist, oder gar keine Antwort des Webservers erfolgt. $ openstack loadbalancer healthmonitor show Beispielmonitor +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | ACTIVE | updated_at | 2019-05-01T09:00:00 | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | ONLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . Aus unserem Deployment-Beispiel würde das Resultat ungefähr so aussehen: . you hit: webserver1 Mi 22 Mai 2019 17:09:39 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:40 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:41 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:42 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:43 CEST - bis hier waren beide Webserver online, doch nun ist der webserver2 offline gegangen. you hit: webserver1 Mi 22 Mai 2019 17:09:44 CEST - noch erwarteter hit durch ROUND_ROBIN you hit: webserver1 Mi 22 Mai 2019 17:09:50 CEST - 1. retry zu webserver2 schlägt fehl you hit: webserver1 Mi 22 Mai 2019 17:09:56 CEST - 2. retry zu webserver2 schlägt fehl you hit: webserver1 Mi 22 Mai 2019 17:10:01 CEST - das Backend (webserver2) wurde aus dem Pool genommen. you hit: webserver1 Mi 22 Mai 2019 17:10:02 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:03 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:04 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:05 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:06 CEST . ",
    "url": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-healthmonitor",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#erstellen-eines-healthmonitor"
  },"239": {
    "doc": "Octavia Loadbalancers",
    "title": "Monitoring mit Prometheus",
    "content": "Der Octavia Amphora-Driver bietet einen Prometheus-Endpunkt. Auf diese Weise können Sie Metriken von Octavia-Loadbalancern sammeln. Um einen Prometheus-Endpunkt zu einem vorhandenen Octavia-Load-Balancer hinzuzufügen, erstellen Sie einen Listener mit dem Protokoll PROMETHEUS. Dadurch wird der Endpunkt als /metrics auf dem Listener aktiviert. Der Listener unterstützt alle Funktionen eines Octavia-Load-Balancers, z. B. allowed_cidrs, unterstützt jedoch nicht das Anhängen von Pools oder L7-Richtlinien. Alle Metriken werden durch die Octavia-Objekt-ID (UUID) der Ressourcen identifiziert. Hinweis: Derzeit werden UDP- und SCTP-Metriken nicht über Prometheus-Endpunkte gemeldet, wenn der Amphora-Provider verwendet wird. Um beispielsweise einen Prometheus-Endpunkt auf Port 8088 für den Load Balancer lb1 zu erstellen, führen Sie den folgenden Befehl aus: . $ openstack loadbalancer listener create --name stats-listener --protocol PROMETHEUS --protocol-port 8088 lb1 +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2021-10-03T01:44:25 | default_pool_id | None | default_tls_container_ref | None | description | | id | fb57d764-470a-4b6b-8820-627452f55b96 | insert_headers | None | l7policies | | loadbalancers | b081ed89-f6f8-48cb-a498-5e12705e2cf9 | name | stats-listener | operating_status | OFFLINE | project_id | 4c1caeee063747f8878f007d1a323b2f | protocol | PROMETHEUS | protocol_port | 8088 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | None | client_authentication | NONE | client_crl_container_ref | None | allowed_cidrs | None | tls_ciphers | None | tls_versions | None | alpn_protocols | None | tags | +-----------------------------+--------------------------------------+ . Sobald der PROMETHEUS-Listener AKTIV ist, können Sie Prometheus so konfigurieren, dass es Metriken vom Load Balancer sammelt, indem Sie die Datei prometheus.yml aktualisieren. [scrape_configs] - job_name: 'Octavia LB1' static_configs: - targets: ['192.0.2.10:8088'] . ",
    "url": "/optimist/networking/octavia_loadbalancer/#monitoring-mit-prometheus",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#monitoring-mit-prometheus"
  },"240": {
    "doc": "Octavia Loadbalancers",
    "title": "Bekannte Probleme",
    "content": "Wenn sie bei der Zuweisung der öffentliche IP Adresse zum Loadbalancer folgenden Fehler bekommen: . ResourceNotFound: 404: Client Error for url: https://network.fra.optimist.gec.io/v2.0/floatingips/46c0e8cf-783d-44a0-8256-79f8ae0be7fe, External network 54258498-a513-47da-9369-1a644e4be692 is not reachable from subnet 32259126-dd37-44d5-922c-99d68ee870cd. Therefore, cannot associate Port 37fc5b34-ee07-49c8-b054-a8d591a9679f with a Floating IP. dann fehlt eine Verbindung zwischen ihrem Beispiel-Netz (Router) und dem Provider-Netz (Schritt 10) . Die default-connect-Einstellung der haproxy-Prozesse innerhalb einer Amphore liegen bei 50 Sekunden, d.h. wenn eine Verbindung länger als 50 Sekunden anhalten soll, müssen sie am Listener diese Werte entsprechend konfigurieren. Beispiel für einen Connect mit Timeout: . $ time kubectl -n kube-system exec -ti machine-controller-5f649c5ff4-pksps /bin/sh ~ $ 50.69 real 0.08 user 0.05 sys . Um in diesem Beispiel den Timeout auf 4h zu erweitern: . openstack loadbalancer listener set --timeout_client_data 14400000 &lt;Listener ID&gt; openstack loadbalancer listener set --timeout_member_data 14400000 &lt;Listener ID&gt; . Wenn Octavia versucht, einen LB mit port_security_enabled = False in einem Netzwerk zu starten, wird der LB in den Status ERROR versetzt. ",
    "url": "/optimist/networking/octavia_loadbalancer/#bekannte-probleme",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#bekannte-probleme"
  },"241": {
    "doc": "Octavia Loadbalancers",
    "title": "Abschluss",
    "content": "Es macht durchaus Sinn immer einen Monitor für seinen Pool zu etablieren. ",
    "url": "/optimist/networking/octavia_loadbalancer/#abschluss",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#abschluss"
  },"242": {
    "doc": "Octavia Loadbalancers",
    "title": "Octavia Loadbalancers",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/"
  },"243": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service (VPNaaS)",
    "content": "OpenStack unterstützt bei Bedarf Site-to-Site VPNs as a Service. Damit können Benutzer zwei private Netzwerke miteinander verbinden. Dazu werden von OpenStack voll funktionale IPsec VPNs innerhalb eines Projekts konfiguriert, ohne dass weitere netzwerkfähige VMs benötigt werden. ",
    "url": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas",
    
    "relUrl": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas"
  },"244": {
    "doc": "VPN as a Service",
    "title": "Anlegen eines Site-to-Site IPSec VPN",
    "content": "Erzeugen von “linken” und “rechten” Netzwerken und Subnetzen . Bevor wir ein VPN anlegen können, benötigen wir zwei getrennte Netzwerke, die miteinander verbunden werden sollen. In dieser Anleitung erzeugen wir diese Netzwerke in zwei verschiedenen OpenStack Projekten und nennen sie “links” (left) und “rechts” (right). Die folgenden Schritte müssen für beide Netzwerke (“links” und “rechts”) durchgeführt werden, damit die zwei verschiedenen OpenStack Cluster miteinander verbunden werden können. Der Einfachheit halber zeigen wir in dieser Anleitung nur, wie wir das linke Netzwerk erzeugen. Die Schritte für das rechte Netzwerk sind bis auf den Namen und das Subnetz-Präfix für OpenStack identisch. In diesem Beispiel verwenden wir das Subnetz-Präfix 2001:db8:1:33bc::/64 für das linke Netzwerk und 2001:db8:1:33bd::/64 für das rechte Netzwerk. Falls Sie bereits über zwei Netzwerke verfügen, die Sie über einen Site-to-Site VPN verbinden möchten, können Sie den Schritt Erzeugen von IKE und IPSec Policies auf beiden Seiten überspringen. Verwenden von Horizon (GUI) . | Erzeugen Sie das linke Netzwerk mit einem neuen Subnetz. | . Navigieren Sie innerhalb Ihres Projekts zu Network → Networks und klicken Sie auf Create Network. Geben Sie dem neuen Netzwerk einen Namen und wählen Sie Enable Admin State , um das Netzwerk zu aktivieren. Wählen Sie anschließend Create Subnet aus, um das Netzwerk und Subnetz in einem Schritt zu erzeugen. Klicken Sie danach auf Next. Geben Sie Ihrem neuen Netzwerk-Subnetz einen Namen und wählen Sie Enter Network Address manually aus. Falls Sie Ihr eigenes Subnetz verwenden möchten, geben Sie Ihr gewünschtes Subnetz in Network Address ein. Falls Sie ein Subnetz von einem vordefinierten Pool verwenden möchten, wählen Sie Allocate Network Address from a pool und wählen Sie einen Pool aus. Klicken Sie anschließend auf Next. Für Dokumentationszwecke verwenden wir die vorher genannten Präfixe. Wählen Sie Enable DHCP und unter IPv6 Address Configuration Mode DHCPV6 STATEFUL aus. Die Allokationspools werden automatisch erzeugt. Klicken Sie auf Create. Verwenden von CLI . | Erzeugen Sie das linke Netzwerk mit dem Befehl openstack network create. | . $ openstack network create vpnaas-left-network +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:45:42Z | description | | dns_domain | | id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | is_vlan_transparent | None | mtu | 1500 | name | vpnaas-left-network | port_security_enabled | True | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 1 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | tags | | updated_at | 2022-09-12T12:45:42Z | +---------------------------+--------------------------------------+ . | Erzeugen Sie ein neues Subnetz und weisen Sie es mit dem Befehl openstack subnet create dem neuen Netzwerk zu. | . $ openstack subnet create \\ vpnaas-left-network-subnet \\ --subnet-range 2001:db8:1:33bc::/64 --ip-version 6 \\ --network vpnaas-left-network +----------------------+--------------------------------------------------------+ | Field | Value | +----------------------+--------------------------------------------------------+ | allocation_pools | 2001:db8:1:33bc::1-2001:db8:1:33bc:ffff:ffff:ffff:ffff | cidr | 2001:db8:1:33bc::/64 | created_at | 2022-09-12T12:47:51Z | description | | dns_nameservers | | dns_publish_fixed_ip | None | enable_dhcp | True | gateway_ip | 2001:db8:1:33bc:: | host_routes | | id | e217a377-48c7-4c18-93b5-cfd805bde40a | ip_version | 6 | ipv6_address_mode | None | ipv6_ra_mode | None | name | vpnaas-left-network-subnet | network_id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | tags | | updated_at | 2022-09-12T12:47:51Z | +----------------------+--------------------------------------------------------+ . Erzeugen des linken und rechten Routers . Verwenden von Horizon (GUI) . | Erzeugen Sie einen Router mit dem Provider-Netzwerk als externes Gateway. | . Navigieren Sie innerhalb Ihres Projekts zu Network → Routers und klicken Sie auf Create Router. Geben Sie dem neuen Router einen Namen. Wählen Sie Enable Admin State, um den Router zu aktivieren. Wählen Sie “PROVIDER” als External Network und klicken Sie auf Create Router. Verwenden von CLI . | Erzeugen Sie einen Router mit dem Befehl openstack router create. | . $ openstack router create vpnaas-left-router +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:48:15Z | description | | enable_ndp_proxy | None | external_gateway_info | null | flavor_id | None | id | 052e968a-a63b-4824-b904-eb70c42c53e5 | name | vpnaas-left-router | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 2 | routes | | status | ACTIVE | tags | | tenant_id | 281fa14f782e4d4cbfd4e34a121c2680 | updated_at | 2022-09-12T12:48:15Z | +-------------------------+--------------------------------------+ . | Verwenden Sie den Befehl openstack router set, um das Provider-Netzwerk als externes Gateway für den Router anzulegen. | . $ openstack router set vpnaas-left-router --external-gateway provider . Zuordnen des Subnetzes an den Router . Verwenden von Horizon (GUI) . Navigieren Sie innerhalb Ihres Projekts zu Network → Routers und wählen Sie den zuvor erzeugten Router aus. Wählen Sie Interfaces und klicken Sie auf Add Interface. Wählen Sie Ihr Subnetz aus und klicken Sie auf Submit. Verwenden von CLI . Verwenden Sie den Befehl openstack router add subnet, um das Subnetz mit dem Router zu verbinden. $ openstack router add subnet vpnaas-left-router vpnaas-left-network-subnet . Erzeugen von IKE und IPSec Policies auf beiden Seiten . Die IKE und IPSec Policies müsssen auf beiden Seiten identisch konfiguriert werden. In dieser Anleitung verwenden wir die folgenden Parameter: . | Parameter | IKE Policy | IPSec Policy | . | Authorization algorithm | SHA256 | SHA256 | . | Encryption algorithm | AES-256 | AES-256 | . | Encapsulation mode | N/A | TUNNEL | . | IKE Version | V2 | N/A | . | Perfect Forward Secrecy | GROUP14 | GROUP14 | . | Transform Protocol | N/A | ESP | . Verwenden von Horizon (GUI) . | Erzeugen Sie die IKE Policy. | . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN, wählen Sie IKE Policies aus und klicken Sie auf Add IKE Policy. Geben Sie Ihrer IKE Policy einen Namen und geben Sie die IKE Policy Parameter ein. Klicken Sie anschließend auf Add. | Erzeugen Sie die IPSec Policy. | . Sie befinden sich noch immer innerhalb von Network → VPN. Wählen Sie IPSec Policies aus und klicken Sie auf Add IPsec Policy. Geben Sie Ihrer IPSec Policy einen Namen und geben Sie die IPSec Policy Parameter ein. Klicken Sie anschließend auf Add. Verwenden von CLI . | Erzeugen Sie die IKE Policy mit dem Befehl openstack vpn ike policy create. | . $ openstack vpn ike policy create \\ vpnaas-left-ike-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --ike-version v2 \\ --pfs group14 +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encryption Algorithm | aes-256 | ID | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IKE Version | v2 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ike-policy | Perfect Forward Secrecy (PFS) | group14 | Phase1 Negotiation Mode | main | Project | 281fa14f782e4d4cbfd4e34a121c2680 | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . | Erzeugen Sie die IPSec Policy mit dem Befehl openstack vpn ipsec policy create. | . $ openstack vpn ipsec policy create \\ vpnaas-left-ipsec-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --pfs group14 \\ --transform-protocol esp +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encapsulation Mode | tunnel | Encryption Algorithm | aes-256 | ID | 553a600e-f39d-47a0-9550-97f2b4033685 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ipsec-policy | Perfect Forward Secrecy (PFS) | group14 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Transform Protocol | esp | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . Erzeugen der VPN Services auf beiden Seiten . Verwenden von Horizon (GUI) . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN. Wählen Sie VPN Services aus und klicken Sie auf Add VPN Service. Geben Sie Ihrem VPN Service einen Namen. Wählen Sie Ihren Router aus und selektieren Sie Enable Admin State. Ein Subnetz wird nicht benötigt, da wir die Endpoint Gruppen verwenden. Klicken Sie anschließend auf Add. Verwenden von CLI . Verwenden Sie den Befehl openstack vpn service create, um den VPN Service zu erzeugen. $ openstack vpn service create vpnaas-left-vpn --router vpnaas-left-router +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | Description | | Flavor | None | ID | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | Name | vpnaas-left-vpn | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Router | 052e968a-a63b-4824-b904-eb70c42c53e5 | State | True | Status | PENDING_CREATE | Subnet | None | external_v4_ip | 185.116.244.85 | external_v6_ip | 2a00:c320:1003::23a | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +----------------+--------------------------------------+ . Erzeugen der Endpoint Gruppen . Bei Verwendung mehrerer Subnetze muss sichergestellt werden, dass der VPN-Endpunkt das Routing mehrerer Subnetze über dieselbe Verbindung unterstützt. Während OpenStack dies tut, müssen für Implementierungen, welche dies nicht unterstützen, mehrere Endpunktgruppen erstellt werden, eine für jedes Subnetz. Verwenden von Horizon (GUI) . | Erzeugen Sie die lokale Endpoint Gruppe für die linke Seite. | . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN. Wählen Sie Endpoint Groups aus und klicken Sie auf Add Endpoint Group. Geben Sie Ihrer Endpoint Gruppe einen Namen. Wählen Sie als Typ Subnet und wählen Sie unter Local System Subnets Ihr Subnetz aus. Klicken Sie anschließend auf Add. | Erzeugen Sie die Peer Endpoint Gruppe für die linke Seite. | . Sie befinden sich noch immer innerhalb von Network → VPN und Endpoint Groups. Klicken Sie erneut auf Add Endpoint Group. Geben Sie Ihrer Endpoint Gruppe einen Namen. Wählen Sie den Typ CIDR und geben Sie das Subnetz für die rechte Seite ein. Klicken Sie anschließend auf Add. Verwenden von CLI . | Verwenden Sie den Befehl openstack vpn endpoint group create, um die lokale Endpoint Gruppe für die linke Seite zu erzeugen. | . $ openstack vpn endpoint group create \\ vpnaas-left-local \\ --type subnet \\ --value vpnaas-left-network-subnet +-------------+------------------------------------------+ | Field | Value | +-------------+------------------------------------------+ | Description | | Endpoints | ['e217a377-48c7-4c18-93b5-cfd805bde40a'] | ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Name | vpnaas-left-local | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | subnet | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+------------------------------------------+ . | Verwenden Sie erneut den Befehl openstack vpn endpoint group create, um die Peer Endpoint Gruppe für die linke Seite zu erzeugen. | . $ openstack vpn endpoint group create \\ vpnaas-left-remote \\ --type cidr \\ --value 2001:db8:1:33bd::/64 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | Description | | Endpoints | ['2001:db8:1:33bd::/64'] | ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Name | vpnaas-left-remote | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | cidr | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+--------------------------------------+ . Erzeugen der Site-Verbindungen . Wie auch bei den Endpunktgruppen müssen Sie, wenn Ihr VPN-Endpunkt das Routing mehrerer Subnetze über dieselbe Verbindung nicht unterstützt, mehrere Site-Verbindungen erstellen, eine für jedes Subnetz/Endpunktgruppe. Verwenden von Horizon (GUI) . Navigieren Sie innerhalb Ihres Projekts zu Network → VPN. Wählen Sie IPSec Site Connections aus und klicken Sie auf Add IPSec Site Connection. Geben Sie Ihrer Verbindung einen Namen. Wählen Sie den vorher erzeugten VPN Service, die lokale Endpoint Guppe und die IKE und IPSec Policy, den Pre-Shared Key, die Peer IP und die Router Identity. In dieser Anleitung nutzen wir 2001:db8::4:703 als IP Addresse des rechten Routers. Verwenden von CLI . Verwenden Sie den Befehl openstack vpn ipsec site connection create, um den VPN Service zu erzeugen. $ openstack vpn ipsec site connection create \\ vpnaas-left-connection \\ --vpnservice vpnaas-left-vpn \\ --ikepolicy vpnaas-left-ike-policy \\ --ipsecpolicy vpnaas-left-ipsec-policy \\ --local-endpoint-group vpnaas-left-local \\ --peer-address 2001:db8::4:703 \\ --peer-id 2001:db8::4:703 \\ --peer-endpoint-group vpnaas-left-remote \\ --psk 1gHAsAeR8lFEDDu7 +--------------------------+----------------------------------------------------+ | Field | Value | +--------------------------+----------------------------------------------------+ | Authentication Algorithm | psk | Description | | ID | d81dbe28-ccda-4ee3-ba96-145fadc74e0f | IKE Policy | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IPSec Policy | 553a600e-f39d-47a0-9550-97f2b4033685 | Initiator | bi-directional | Local Endpoint Group ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Local ID | | MTU | 1500 | Name | vpnaas-left-connection | Peer Address | 2001:db8::4:703 | Peer CIDRs | | Peer Endpoint Group ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Peer ID | 2001:db8::4:703 | Pre-shared Key | 1gHAsAeR8lFEDDu7 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Route Mode | static | State | True | Status | PENDING_CREATE | VPN Service | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | dpd | {'action': 'hold', 'interval': 30, 'timeout': 120} | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +--------------------------+----------------------------------------------------+ . ",
    "url": "/optimist/networking/vpnaas/#anlegen-eines-site-to-site-ipsec-vpn",
    
    "relUrl": "/optimist/networking/vpnaas/#anlegen-eines-site-to-site-ipsec-vpn"
  },"245": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/optimist/networking/vpnaas/",
    
    "relUrl": "/optimist/networking/vpnaas/"
  },"246": {
    "doc": "Storage",
    "title": "Storage",
    "content": " ",
    "url": "/optimist/storage/",
    
    "relUrl": "/optimist/storage/"
  },"247": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Einführung in den S3 Kompatiblen Objekt Storage",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/#einf%C3%BChrung-in-den-s3-kompatiblen-objekt-storage",
    
    "relUrl": "/optimist/storage/s3_documentation/#einführung-in-den-s3-kompatiblen-objekt-storage"
  },"248": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Was genau ist Object Storage?",
    "content": "Object Storage ist eine alternative zum klassischen Block Storage. Dabei werden die einzelnen Daten nicht einzelnen Blöcken auf einem Gerät zugeordnet, sondern als binäre Objekte innerhalb eines Storage Clusters gespeichert. Die notwendigen Metadaten werden in einer separaten DB gespeichert. Der Storage Cluster besteht dabei aus mehreren einzelnen Servern (Nodes), die wiederum mehrere Storage Devices verbaut haben. Bei den Storage Devices haben wir einen mix aus klassischen HDDs, SSDs und modernen NVMe Lösungen. Auf welchem Gerät letztendlich ein Object landet, wird durch den CRUSH Algorithmus entschieden, der beim Object Storage serverseitig implementiert ist. ",
    "url": "/optimist/storage/s3_documentation/#was-genau-ist-object-storage",
    
    "relUrl": "/optimist/storage/s3_documentation/#was-genau-ist-object-storage"
  },"249": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Wie kann ich darauf zugreifen?",
    "content": "Der Zugriff auf diese Art von Storage erfolgt ausschließlich über HTTPS Zugriffe. Dafür stellen wir einen hochverfügbaren Endpunkt zur Verfügung, über den die einzelnen Operationen ausgeführt werden können. Dabei unterstützen wir zwei unabhängige Protokolle: . | S3 | Swift | . S3 ist ein von Amazon ins leben gerufene Protokoll, um mit dieser Art von Daten arbeiten zu können. Swift ist das Protokoll, welches vom gleichnamigen OpenStack Service bereitgestellt wird. Unabhängig davon, welches Protokoll man nutzt, hat man immer Zugriff auf seine gesamten Daten. Man kann also beide Protokolle in Kombination nutzen. Es gibt für alle gängigen Plattformen Tools, um mit den Daten im Object Storage arbeiten zu können: . | Windows: s3Browser, Cyberduck | MacOS: s3cmd, Cyberduck | Linux: s3cmd | . Darüberhinaus gibt es Integrationen in allen gängigen Programmiersprachen. ",
    "url": "/optimist/storage/s3_documentation/#wie-kann-ich-darauf-zugreifen",
    
    "relUrl": "/optimist/storage/s3_documentation/#wie-kann-ich-darauf-zugreifen"
  },"250": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Wie sicher sind meine Daten?",
    "content": "Basis für den Object Storage ist unsere zugrunde liegende Openstack Cloud Plattform mit dem verteilten Ceph Storage Cluster. Die Objekte werden serverseitig über mehrere Storage Devices hinweg verteilt und repliziert. Dabei sorgt Ceph für die Sicherstellung von Replikation und Integrität der Datensätze. Fällt ein Server oder eine Festplatte aus, werden die betroffenen Datensätze auf verfügbare Server repliziert und das gewünschte Replikationslevel automatisch wiederhergestellt. Darüberhinaus werden die Daten in ein weiteres RZ auf einen weiteren dedizierten Storage Cluster gespiegelt und können von dort im K-Fall weitergenutzt werden. ",
    "url": "/optimist/storage/s3_documentation/#wie-sicher-sind-meine-daten",
    
    "relUrl": "/optimist/storage/s3_documentation/#wie-sicher-sind-meine-daten"
  },"251": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "Vorteile im Überblick",
    "content": ". | Bereitstellung per API: Die HTTPS-Schnittstelle ist sowohl kompatibel zur Amazon S3 API, also auch zur OpenStack Swift API. | unterstützt alle gängigen Betriebssysteme und Programmiersprachen. | Volle Skalierbarkeit - Der Storage kann dynamisch genutzt werden. | Höchste Ausfallsicherheit dank integrierter Replikation und Spiegelung über 2 unabhängige Rechenzentren. | Zugriff ist nahezu von jedem internetfähigen Gerät aus möglich - Somit eine super alternative zu NFS und Co. | PAYG Abrechnung nach genutztem Monatsdurchschnitt. | Transparente Abrechnung und somit gute Planbarkeit - Keine extra Traffic Kosten oder Kosten für Zugriffe auf die Daten. | Automatisierte Verwaltung der Objekte in Buckets mit s3 Lifecycle Policies möglich. | . ",
    "url": "/optimist/storage/s3_documentation/#vorteile-im-%C3%BCberblick",
    
    "relUrl": "/optimist/storage/s3_documentation/#vorteile-im-überblick"
  },"252": {
    "doc": "S3 Kompatiblen Objekt Storage",
    "title": "S3 Kompatiblen Objekt Storage",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/",
    
    "relUrl": "/optimist/storage/s3_documentation/"
  },"253": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "S3 Kennung erstellen und einlesen",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/"
  },"254": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Inhalt:",
    "content": ". | Credentials erstellen . | S3cmd | S3Browser | Cyberduck | Boto3 | . | Credentials anzeigen | Credentials löschen | . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#inhalt"
  },"255": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Credentials erstellen",
    "content": "Damit wir auf den Object Storage zugreifen können, benötigen wir zunächst Login Daten(Credentials). Um diese Daten per OpenStackAPI erzeugen zu können, benötigen wir den OpenStackClient und führen dort folgenden Befehl aus: . $ openstack ec2 credentials create . Wenn die Daten korrekt erstellt worden sind, sieht die Ausgabe in etwa so aus: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | links | {u'self': u'https://identity.optimist.gec.io/v3/users/bbb | | bbbbbbbbbbbbbbbbbbbbbbbbbbbbb/credentials/OS- | | EC2/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'} | project_id | cccccccccccccccccccccccccccccccc | secret | dddddddddddddddddddddddddddddddd | trust_id | None | user_id | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | +------------+-----------------------------------------------------------------+ . Nachdem die Zugangsdaten (Credentials) vorliegen, brauchen wir eine Möglichkeit auf den S3 kompatiblen ObjectStorage zuzugreifen. Hierfür gibt es die unterschiedliche Möglichkeiten, in der Dokumentation stellen wir hierfür vier Möglichkeiten vor, genauer: S3cmd für Linux/Mac, S3Browser für Windows, Cyberduck und Boto3. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-erstellen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-erstellen"
  },"256": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Benutzerdaten in die Konfigurationsdatei eintragen",
    "content": "S3cmd . Um s3cmd zu installieren, brauchen wir einen Paketmanager wie zum Beispiel “pip”. Die Installation und Nutzung erklären wir im Schritt 4: “Der Weg vom Horizon auf die Kommandozeile” unserer Guided Tour. Der Befehl für die Installation lautet dann: . pip install s3cmd . Nach der erfolgreichen Installation von s3cmd, müssen die vorher erstellten Zugangsdaten (Credentials) in die Konfigurationsdatei von s3cmd eingetragen werden. Die dafür zuständige Datei ist die “.s3cfg”, welche sich standardgemäß im Homeverzeichnis befindet. Sollte diese noch nicht existieren, muss diese vorher erstellt werden. Folgende Daten tragen wir dann in der .s3cfg ein und speichern diese: . access_key = aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = dddddddddddddddddddddddddddddddd use_https = True . S3Browser . Für den S3Browser genügt es, diese heruterzuladen und zu installieren. Nach der erfolgreichen Installation, gilt es nun die entsprechenden Daten zu hinterlegen. Hierfür öffnen wir den S3Browser und es öffnet sich beim ersten Starten automatisch folgendes Fenster: . Dort tragen wir nun folgende Werte ein und klicken auf “Add new account” . * Account Name: Frei wählbarer Name für den Account * Account Type: S3 Compatible Storage * REST Endpoint: s3.es1.fra.optimist.gec.io * Signature Version: Signature V2 * Access Key ID: Den entsprechenden Access Key (Im Beispiel: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa) * Secret Access Key: Das entsprechende Secret (Im Beispiel: dddddddddddddddddddddddddddddddd) . Cyberduck . Um Cyberduck zu nutzen, ist es notwendig diese herunterzuladen. Nach der Installation und dem ersten öffnen, ist es notwendig auf “Neue Verbindung” zu klicken. (1) Danach öffnet sich ein neues Fenster in dem im Dropdown Menü(2) “Amazon S3” ausgewählt wird und danach werden folgende Daten benötigt: . | Server(3): s3.es1.fra.optimist.gec.io | Access Key ID(4): Den entsprechenden Access Key (Im Beispiel: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa) | Secret Access Key(5): Das entsprechende Secret (Im Beispiel: dddddddddddddddddddddddddddddddd) | . Um nun eine Verbindung herzustellen, wird als letzter Schritt auf “Verbinden” geklickt. Boto3 . Um Boto3 nutzen zu können, wird ein Paketmanager wie zum Beispiel “pip” benötigt. Die Installation und Nutzung wird im Schritt 4: “Der Weg vom Horizon auf die Kommandozeile” unserer Guided Tour erklärt. Der Befehl für die Installation lautet dann: . pip install boto3 . Nach der erfolgreichen Installation von boto3 ist es nun nutzbar. Wichtig ist, dass bei boto3 ein Script erstellt wird, welches am Ende ausgeführt wird. Daher ist der Konfigurationsteil der im Anschluss gezeigt wird, später immer Teil der weiterführenden Scripte. Hierfür erstellen wir eine Python-Datei wie z.B. “Beispiel.py” und fügen dort folgenden Inhalt ein: . | endpoint_url: s3.es1.fra.optimist.gec.io | aws_access_key_id: Den entsprechenden Access Key (Im Beispiel: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa) | aws_secret_access_key: Das entsprechende Secret (Im Beispiel: dddddddddddddddddddddddddddddddd) | . #!/usr/bin/env/python import boto3 from botocore.client import Config s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='dddddddddddddddddddddddddddddddd', ) . Dies dient als Startpunkt und wird in den folgenden Skripten referenziert und verwendet. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#benutzerdaten-in-die-konfigurationsdatei-eintragen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#benutzerdaten-in-die-konfigurationsdatei-eintragen"
  },"257": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Credentials anzeigen",
    "content": "Um erstellte Object Storage EC2-Credentials anzuzeigen benötigen wir den OpenstackClient und führen dort folgenden Befehl aus: . $ openstack ec2 credentials list . Der Befehl erstellt uns eine Liste mit allen EC2 Credentials, die für den aktuellen Nutzer sichtbar sind. $ openstack ec2 credentials list +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | Access | Secret | Project ID | User ID | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx | 12341234123412341234123412341234 | 32132132132132132132132132132132 | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy | 56756756756756756756756756756756 | 65465465465465465465465465465465 | cccccccccccccccccccccccccccccccc | zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz | 89089089089089089089089089089089 | 09809809809809809809809809809809 | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-anzeigen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-anzeigen"
  },"258": {
    "doc": "S3 Kennung erstellen und einlesen",
    "title": "Credentials löschen",
    "content": "Um vorhandene Object Storage EC2-Credentials zu löschen benötigen wir den OpenstackClient und führen dort folgenden Befehl aus: . $ openstack ec2 credentials delete &lt;access-key&gt; . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-l%C3%B6schen",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#credentials-löschen"
  },"259": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Einen Bucket erstellen und wieder löschen",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/"
  },"260": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Inhalt:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Zum Hochladen Ihrer Daten wie zum Beispiel (Dokumente, Fotos, Videos, usw.) ist es zunächst notwendig einen Bucket zu erstellen. Dieser dient als eine Art Ordner. Aufgrund der Funktionsweise unseres Objects-Storages ist es notwendig einen global eindeutigen Namen für Ihren Bucket zu verwenden. Sollte bereits ein Bucket mit dem gewählten Namen existieren, kann der Name erst verwendet werden, wenn der bereits existierende Bucket gelöscht wurde. Sollte der gewünschte Name bereits von einem weiteren Kunden in Benutzung sein müssen sie einen anderen Namen wählen. Es empfiehlt sich, Namen der Form “inhaltsbeschreibung.bucket.meine-domain.tld” oder vergleichbares zu verwenden. ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#inhalt"
  },"261": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "S3cmd",
    "content": "Bucket erstellen . Um einen Bucket zu erstellen, nutzt man folgenden Befehl: . s3cmd mb s3://NameDesBuckets . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd mb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' created . Bucket löschen . Um einen Bucket zu löschen, nutzt man folgenden Befehl: . s3cmd rb s3://NameDesBuckets . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd rb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' removed . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd"
  },"262": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "S3Browser",
    "content": "Bucket erstellen . Nach dem Öffnen von S3Browser, klicken wir oben links auf “New bucket”(1), in dem sich öffnenden Fenster vergeben wir unter “Bucket name”(2) den Namen des Buckets und klicken dann auf “Create new bucket”(3). Bucket löschen . Zuerst wird der zu löschende Bucket markiert(1) und danch oben links auf “Delete bucket” geklickt. Im sich nun öffnenden Fenster, bestätigen mit dem markieren der Checkbox(1), dass die Datei gelöscht werden soll und klicken dann auf “Delete Bucket”(2). ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser"
  },"263": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Cyberduck",
    "content": "Bucket erstellen . Nach dem Öffnen von Cyberduck, klicken wir oben in der Mitte auf “Aktion”(1) und auf “Neuer Ordner”(2) . Danach öffnet sich das folgende Fenster, hier können wir den Namen(1) festlegen und bestätigen dies im Anschluß mit “Anlegen”(2): . Bucket löschen . Um einen Bucket zu löschen, wird dieser mit einem linken Mausklick makiert. Gelöscht wird der Bucket dann über “Aktion”(1) und “Löschen”(2). Die Bestätigung erfolgt dann über das erneute klicken auf “Löschen”(1) . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck"
  },"264": {
    "doc": "Einen Bucket erstellen und wieder löschen",
    "title": "Boto3",
    "content": "Bei boto3 brauchen wir zunächst die S3 Kennung, damit ein Script nutzbar ist. Für Details: S3 Kennung erstellen und einlesen #boto3 . Bucket erstellen . Um nun einen Bucket zu erstellen, müssen wir einen Clienten nutzen und den Bucket dann erstellen. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Erstellung des Buckets s3.create_bucket(Bucket='iNNOVO-Test') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Erstellung des Buckets s3.create_bucket(Bucket='iNNOVO-Test') . Bucket löschen . Wie bei der Erstellung des Buckets, wird zunächst ein Client benötigt um dann den Bucket zu löschen. Um nun einen Bucket zu erstellen, müssen wir einen Clienten nutzen und den Bucket dann erstellen. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Löschen eines Buckets s3.delete_bucket(Bucket='iNNOVO-Test') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='6229490344a445f2aa59cdc0e53add88', ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Erstellung des Buckets s3.delete_bucket(Bucket='iNNOVO-Test') . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3"
  },"265": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Ein Objekt hochladen und löschen",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/"
  },"266": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Inhalt:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Zum Hochladen Ihrer Daten wie zum Beispiel (Dokumente, Fotos, Videos, usw.) ist es zunächst notwendig einen Bucket zu erstellen. Eine Datei kann dabei nur in einem Bucket gespeichert werden. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#inhalt"
  },"267": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "S3cmd",
    "content": "Objekt hochladen . Um eine Datei hochzuladen, nutzt man folgenden Befehl: . s3cmd put NameDerDatei s3://NameDesBuckets/NameDerDatei . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd put innovo.txt s3://innovo-test/innovo.txt&lt;font&gt;&lt;/font&gt; upload: 'innovo.txt' -&gt; 's3://innovo-test/innovo.txt' [1 of 1]&lt;font&gt;&lt;/font&gt; 95 of 95 100% in 0s 176.63 B/s done . Objekt löschen . Um eine Datei zu löschen, nutzt man folgenden Befehl: . s3cmd del s3://NameDesBuckets/NameDerDatei . Die Ausgabe in der Kommandozeile kann dann so aussehen: . $ s3cmd del s3://innovo-test/innovo.txt&lt;font&gt;&lt;/font&gt; delete: 's3://innovo-test/innovo.txt' . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd"
  },"268": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "S3Browser",
    "content": "Objekt hochladen . Nach dem öffnen von S3Browser, klicken wir auf den gewünschten “Bucket”(1), wähle dann “Upload”(2) und zu letzt “Upload file(s)”(3) . Hier wählen wir nun die entsprechende Datei(1) aus und klicken auf Öffnen(2). Objekt löschen . Um eine Datei zu löschen, wird dieser mit einem linken Mausklick markiert(1). Danach wird auf “Delete”(2) geklickt. Die darauf folgende Abfrage wird mit “Ja” bestätigt. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser"
  },"269": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Cyberduck",
    "content": "Objekt hochladen . Nach dem Öffnen von Cyberduck, klicken wir auf den gewünschten Bucket(1), klicken dann auf Aktion(2) und dort auf Upload(3). Hier wählen wir nun unsere Wunsch-Datei und klicken auf Upload. Objekt löschen . Um eine Datei zu löschen, wird dieser mit einem linken Mausklick markiert(1). Gelöscht wird sie dann über “Aktion”(2) und “Löschen”(3). Die Bestätigung erfolgt dann über das erneute klicken auf “Löschen”. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck"
  },"270": {
    "doc": "Ein Objekt hochladen und löschen",
    "title": "Boto3",
    "content": "Bei boto3 brauchen wir zunächst die S3 Kennung, damit ein Script nutzbar ist. Für Details: S3 Kennung erstellen und einlesen #boto3 . Objekt hochladen . Um nun eine Datei hochzuladen, müssen wir einen Clienten nutzen und den Bucket angeben in welchen die Datei hochgeladen werden soll. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Hochladen einer Datei s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io',&lt;font&gt;&lt;/font&gt; aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',&lt;font&gt;&lt;/font&gt; aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb',&lt;font&gt;&lt;/font&gt; ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Hochladen einer Datei s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . Objekt löschen . Wie beim hochladen einer Datei, wird zunächst ein Client benötigt um dann die Datei zu löschen. Dafür geben wir neben der Datei selber, auch noch den Bucket an, in dem die Datei gespeichert ist. Eine Option sieht so aus: . ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Löschen eines Objekts s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Erstellung eines S3 Clienten s3 = boto3.client('s3') ## Löschen eines Objekts s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3"
  },"271": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/versioning/",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/"
  },"272": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Inhalt:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Versionierung ermöglicht es, mehrere Versionen eines Objekts in einem Bucket aufzubewahren. So können Beispielsweise innovo.txt (Version 1) und innovo.txt (Version 2) in einem einzigen Bucket speichern. Die Versionierung kann Sie vor den Folgen von unbeabsichtigtem Überschreiben oder Löschen bewahren. ",
    "url": "/optimist/storage/s3_documentation/versioning/#inhalt",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#inhalt"
  },"273": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "S3cmd",
    "content": "Mit S3cmd ist es nicht möglich die Versionierung einzuschalten und/oder versionierte Dateien zu löschen. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3cmd"
  },"274": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "S3Browser",
    "content": "Versionierung einschalten . Um die Versionierung zu aktivieren, markieren wir den gewünschten Bucket(1). Machen auf den Bucket einen rechten Mausklick und klicken dann auf “Edit Versioning Settings”(2). Im sich öffnenden Fenster klicken wir in die Checkbox von “Enable versioning for bucket”(1) und bestätigen dies mit “OK”(2). Versionierung deaktivieren . Um die Versionierung zu deaktivieren, markieren wir den gewünschten Bucket(1). Klicken dann mit einem rechten Mausklick auf den Bucket und dann auf “Edit Versioning Settings”(2). Im sich öffnenden Fenster entfernen wir die Checkbox bei “Enable versioning for bucket”(1) und bestätigen dies mit “OK”(2). Versionierte Datei löschen . Dies ist in der Free-Version von S3Browser nicht möglich. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3browser"
  },"275": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Cyberduck",
    "content": "Um die verschiedenen Version einer Datei zu sehen, müssen versteckte Dateien angezeigt werden. Diese Option findet man unter Darstellung(1) → Versteckte Dateien anzeigen(2) . Versionierung einschalten . Nach dem Öffnen von Cyberduck, klicken wir auf eine Datei, wo wir die Versionierung(1) für aktivieren wollen. Danach auf Aktion(2) und auf Info(3). Danach öffnet sich das folgende Fenster, hier setzen wir den Haken bei “Bucket Versionierung”(1): . Versionierung deaktivieren . Um die Versionierung zu deaktivieren, markieren wir wieder eine Datei(1), gehen auf Aktion(2) und auf Info(3). In dem sich öffnenden Fenster wird der Haken bei “Bucket Versionierung” entfernt. Versionierte Datei löschen . Hier wird einfach die zu löschende Datei markiert(1) und über Aktion(2) → Löschen(3) entfernt. ",
    "url": "/optimist/storage/s3_documentation/versioning/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#cyberduck"
  },"276": {
    "doc": "Versionierung aktivieren/deaktivieren und wie eine versioniertes Objekt gelöscht wird",
    "title": "Boto3",
    "content": "Bei boto3 brauchen wir zunächst die S3 Kennung, damit ein Script nutzbar ist. Für Details: S3 Kennung erstellen und einlesen #boto3. Versionierung einschalten . Um nun einen Bucket zu erstellen, müssen wir einen Clienten nutzen und den Bucket dann erstellen. Eine Option sieht so aus: . ## Angabe des Buckets in dem die Versionierung aktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung aktivieren bucket.configure_versioning(True) . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Angabe des Buckets in dem die Versionierung aktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung aktivieren bucket.configure_versioning(True) . Versionierung deaktivieren . Wie bei der Aktivierung der Versionierung wird zunächst der Bucket benötigt um dann die Versionierung zu deaktivieren. Eine Option sieht so aus: . ## Angabe des Buckets in dem die Versionierung deaktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung deaktivieren bucket.configure_versioning(False) . Ein komplettes Script für boto 3 inkl. Authentifizierung kann so aussehen: . #!/usr/bin/env/python ## Definieren das boto3 genutzt werden soll import boto3 from botocore.client import Config ## Authentifizierung s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='6229490344a445f2aa59cdc0e53add88', ) ## Angabe des Buckets in dem die Versionierung deaktiviert werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versionierung deaktivieren bucket.configure_versioning(False) . Versioniertes Objekt löschen . Um ein versioniertes Objekt komplett zu löschen, ist folgender Befehl hilfreich: . ## Angabe des Buckets in dem das versionierte Objekt gelöscht werden soll bucket = s3.Bucket('iNNOVO-Test') ## Versioniertes Objekt löschen bucket.object_versions.all().delete('innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/versioning/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#boto3"
  },"277": {
    "doc": "S3 Security",
    "title": "S3 Security",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/security/",
    
    "relUrl": "/optimist/storage/s3_documentation/security/"
  },"278": {
    "doc": "S3 Security",
    "title": "Einführung",
    "content": "Diese Seite bietet einen Überblick über die folgenden Themen im Zusammenhang mit S3-Buckets / Swift: . | Container Access Control Lists (ACLs) | Bucket-Policies | . Operationen auf Container-ACLs müssen auf OpenStack-Ebene mit Swift-Befehlen ausgeführt werden, während Bucket-Policies für jeden Bucket innerhalb eines Projekts mit Hilfe der s3cmd-Befehlszeile festgelegt werden müssen. In diesem Dokument werden einige Beispiele für jede Art von Operation beschrieben. ",
    "url": "/optimist/storage/s3_documentation/security/#einf%C3%BChrung",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#einführung"
  },"279": {
    "doc": "S3 Security",
    "title": "Container Access Control Lists (ACLs)",
    "content": "Standardmäßig haben nur Projektbesitzer die Berechtigung, Container und Objekte zu erstellen, zu lesen und zu ändern. Ein Eigentümer kann jedoch anderen Benutzern mit Hilfe einer Access Control List (ACL) Zugriff gewähren. Die ACL kann für jeden Container festgelegt werden und gilt nur für diesen Container und die Objekte in diesem Container. Einige der Hauptelemente, mit denen eine ACL für einen Container festgelegt werden kann, sind nachfolgend aufgeführt: . | Element | Beschreibung | . | .r: *. | Jeder Benutzer hat Zugriff auf Objekte. In der Anforderung ist kein Token erforderlich. | . | .r: &lt;Referrer&gt; | Der Referrer erhält Zugriff auf Objekte. Der Referrer wird durch den Referer-Anforderungsheader in der Anforderung identifiziert. Es ist kein Token erforderlich. | . | .r: - &lt;Referrer&gt; | Diese Syntax (mit “-“ vor dem Referrer) wird unterstützt. Der Zugriff wird jedoch nicht verweigert, wenn ein anderes Element (z. B.r:*) den Zugriff gewährt. | . | .rlistings | Jeder Benutzer kann HEAD- oder GET-Operationen für den Container ausführen, wenn der Benutzer auch Lesezugriff auf Objekte hat (z. B. auch .r: * oder .r: &lt;Referrer&gt;. Es ist kein Token erforderlich. | . Als Beispiel setzen wir die Policy .r:*. für einen Container mit dem Namen \"&lt;example-container&gt;\". Diese Policy ermöglicht jedem externen Benutzer den Zugriff auf die Objekte im Container. swift post example-container --read-acl \".r:*\" . Umgekehrt können wir Benutzern auch erlauben, die Liste der Objekte in einem Container aufzulisten, aber nicht darauf zuzugreifen, indem wir die Policy “.rlistings” für unseren “example-container” festlegen: . swift post example-container --read-acl \".rlistings\" . Der folgende Befehl kann verwendet werden, um die Lese-Policy zu entfernen und den Container auf den Standardstatus “privat” zu setzen: . swift post -r \"\" example-container . Verwenden Sie den folgenden Befehl, um zu überprüfen, welche ACL für einen Container festgelegt ist. swift stat example-container . Dies gibt einen Überblick über die Statistiken für den Container und zeigt die aktuelle ACL-Regel für einen Container an. Verhindern Sie das Auflisten auf Containern, wenn Sie die Policy .r: * . verwenden: . In der aktuellen Version von OpenStack empfehlen wir, ein leeres index.html-Objekt im Container zu erstellen, um zu verhindern, dass der Inhalt aufgelistet wird, während die Policy .r: * . für einen Container verwendet wird. Auf diese Weise können Benutzer Objekte herunterladen, ohne den Inhalt der Buckets aufzulisten zu koennen. Dies kann mit den folgenden Schritten erreicht werden: . Fügen Sie zunächst die leere Datei index.html zu unserem example-container hinzu: . swift post -m 'Webindex: index.html' example-container . Erstellen Sie dann die Datei index.html als Objekt im Container: . touch index.html &amp;&amp; openstack object create example-container index.html . Dadurch können externe Benutzer auf bestimmte Dateien zugreifen, ohne den Inhalt des Containers aufzulisten. ",
    "url": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls"
  },"280": {
    "doc": "S3 Security",
    "title": "Bucket-Policy",
    "content": "Bucket-Policies werden verwendet, um den Zugriff auf jeden Bucket in einem Projekt zu steuern. Es wird empfohlen, bei der Erstellung eine Policy für alle Buckets festzulegen. Der erste Schritt besteht darin, eine Policy wie folgt zu erstellen. Für die folgende Vorlage muss nur der Bucket-Name für nachfolgende Policy geändert werden. Im folgenden Beispiel wird eine Policy für den Bucket example-bucket erstellt: . cat &gt; examplepolicy { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Sid\": \"AddPerm\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::example-bucket/*\" } ] } . Aufschlüsselung jedes Elements innerhalb des obigen Policy-Beispiels: . | Version: Gibt die Sprachsyntaxregeln an, die zum Verarbeiten der Policy verwendet werden sollen. Es wird empfohlen, immer Folgendes zu verwenden: “2012-10-17”, da dies die aktuelle Version der Policy-Sprache ist. | Statement: Das Hauptelement der Policy, die anderen Elemente befinden sich in dieser Anweisung. | SID: Die Statement-ID. Dies ist eine optionale Kennung, mit der die Richtlinienanweisung beschrieben werden kann. Empfohlen, damit der Zweck jeder Policy klar ist. | Effect: Stellen Sie entweder “Allow” oder “Deny” ein. | Principal: Gibt den Principal an, dem der Zugriff auf eine Ressource gestattet oder verweigert wird. Hier wird der Platzhalter “*” verwendet, um die Regel auf alle anzuwenden. | Action: Beschreibt die spezifischen Aktionen, die zugelassen oder abgelehnt werden. | . (Weitere Informationen zu den verfügbaren Policy-Optionen und zur Anpassung an Ihre spezifischen Anforderungen finden Sie in der offiziellen AWS-Dokumentation). Wenden Sie als Nächstes die neu erstellte Policy auf example-bucket an: . s3cmd setpolicy examplepolicy s3://example-bucket . Anschließend können Sie den folgenden Befehl ausführen, um sicherzustellen, dass die Policy vorhanden ist: . s3cmd info s3://example-bucket . Sobald die Policy angewendet wurde, können Sie im Dashboard erneut festlegen: Public Access: Disabled. Sobald die oben genannten Schritte ausgeführt wurden, erhalten wir die folgenden Ergebnisse: . | Der Container ist privat und Dateien werden nicht über XML aufgelistet oder angezeigt. | Die Policy ermöglicht jetzt den Zugriff auf bestimmte Dateien mit einem direkten Link. | . ",
    "url": "/optimist/storage/s3_documentation/security/#bucket-policy",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#bucket-policy"
  },"281": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving Static Websites",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites"
  },"282": {
    "doc": "Swift - Serving a Static Website",
    "title": "Einführung",
    "content": "Mit Hilfe der Swift-CLI ist es möglich, die Daten in Containern als statische Website auszuliefern. Die folgende Anleitung beschreibt die wichtigsten Schritte, um damit zu beginnen, und enthält auch ein Beispiel. ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#einf%C3%BChrung",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#einführung"
  },"283": {
    "doc": "Swift - Serving a Static Website",
    "title": "Erste Schritte",
    "content": "Erstellen eines Containers . Zunächst erstellen wir einen Container mit dem Namen example-webpage, den wir als Basis für diese Anleitung verwenden werden: . swift post example-webpage . Den Container öffentlich lesbar machen . Als nächstes müssen wir sicherstellen, dass der Container öffentlich lesbar ist. Weitere Informationen zum Sichern von Containern und zum Festlegen von Bucket-Richtlinien finden Sie hier: . swift post -r '.r:*' example-webpage . Indexdatei der Seite setzen . Setzen Sie die Indexdatei. In diesem Fall wird index.html die Standarddatei sein, die angezeigt wird, wenn die Seite erscheint: . swift post -m 'web-index:index.html' example-webpage . Dateiliste aktivieren . Optional können wir auch die Dateiliste aktivieren. Wenn Sie mehrere Downloads bereitstellen müssen, ist es sinnvoll, die Verzeichnisliste zu aktivieren: . swift post -m 'web-listings: true' example-webpage . CSS für Dateilisten aktivieren . Aktivieren Sie ein benutzerdefiniertes Listing-Stylesheet: . swift post -m 'web-listings-css:style.css' example-webpage . Fehlerseiten einrichten . Schließlich sollten wir eine benutzerdefinierte Fehlerseite einbinden: . swift post -m 'web-error:404error.html' example-webpage . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#erste-schritte",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#erste-schritte"
  },"284": {
    "doc": "Swift - Serving a Static Website",
    "title": "Beispiel-Webseite",
    "content": "Lassen Sie uns die Schritte rekapitulieren, die wir bis jetzt unternommen haben, um statische Webseiten zu aktivieren: . swift post example-webpage swift post -r '.r:*' example-webpage swift post -m 'web-index:index.html' example-webpage swift post -m 'web-listings: true' example-webpage swift post -m 'web-listings-css:style.css' example-webpage swift post -m 'web-error:404error.html' example-webpage . Wenn die obigen Schritte abgeschlossen sind, können wir damit beginnen, unsere statische Webseite anzupassen. Das Folgende demonstriert eine schnelle Einrichtung unter Verwendung unseres Containers example-webpage . Anpassen der Seiten index.html, page.html und 404error.html . Dies wird als Startseite dienen, die einen Link zu einer sekundären Seite erstellt. &lt;!-- index.html --&gt; &lt;html&gt; &lt;h1&gt; See the web page &lt;a href=\"mywebsite/page.html\"&gt;here&lt;/a&gt;. &lt;/h1&gt; &lt;/html&gt; . Die nächste Seite (page.html) zeigt ein Bild namens sample.png an: . &lt;!-- page.html --&gt; &lt;html&gt; &lt;img src=\"sample.png\"&gt; &lt;/html&gt; . Wir können auch benutzerdefinierte Fehlerseiten hinzufügen. Beachten Sie, dass derzeit nur die Fehler 401 (Nicht autorisiert) und 404 (Nicht gefunden) unterstützt werden. Das folgende Beispiel demonstriert die Erstellung einer 404-Fehlerseite: . &lt;!-- 404error.html --&gt; &lt;html&gt; &lt;h1&gt; 404 Not Found - We cannot find the page you are looking for! &lt;/h1&gt; &lt;/html&gt; . Hochladen der Dateien index.html und page.html . Nachdem die Inhalte der Dateien erstellt wurden, laden Sie die Dateien mit den folgenden Befehlen hoch: . swift upload beispiel-webseite index.html swift upload beispiel-webseite meinewebseite/seite.html swift upload beispiel-webseite-meine-website/beispiel.png swift upload beispiel-webseite 404error.html . Betrachten der Website . Wenn alle oben genannten Schritte abgeschlossen sind, können wir nun unsere neu erstellte Website betrachten. Den Link zur Website finden Sie im Optimist Dashboard &gt; Object Store &gt; Containers über den abgebildeten Link. Wenn Sie auf den Link klicken, wird unsere neu erstellte Website angezeigt: . Klicken Sie auf “here”, um zu der Seite zu navigieren, auf der wir unser Beispielbild hochgeladen haben: . Für den Fall, dass wir versuchen, zu einer Seite zu navigieren, die nicht existiert, wird unsere benutzerdefinierte 404-Seite angezeigt: . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#beispiel-webseite",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#beispiel-webseite"
  },"285": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving a Static Website",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/"
  },"286": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Problemstellung",
    "content": "Beim Durchführen von Sicherungen auf Dateiebene eines Nodes in S3 benötigt die Sicherungssoftware Schreibberechtigungen für den S3-Bucket. Verschafft sich ein Angreifer jedoch Zugriff auf die Maschine, kann er auch die Backups im Bucket zerstören, da die S3-Anmeldeinformationen auf dem kompromittierten System vorhanden sind. Die Lösung kann so einfach sein, wie die Zugriffsebene der Backup-Software auf den Bucket einzuschränken. Leider ist das bei S3 nicht trivial. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#problemstellung",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#problemstellung"
  },"287": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Hintergrund",
    "content": "S3-Zugriffskontrolllisten (ACLs) ermöglichen Ihnen die Verwaltung des Zugriffs auf Buckets und Objekte, sind jedoch sehr eingeschränkt. Sie unterscheiden im Wesentlichen READ- und WRITE-Berechtigungen: . | READ – Ermöglicht dem Benutzer, die Objekte im Bucket aufzulisten | WRITE - Ermöglicht dem Benutzer, jedes Objekt im Bucket zu erstellen, zu überschreiben und zu löschen | . Die Einschränkungen von ACLs wurden durch die Zugriffsrichtlinienberechtigungen (ACP) behoben. Wir könnten dem Bucket eine No-Delete-Richtlinie anhängen, z.B. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"nodelete1\", \"Effect\": \"Deny\", \"Action\": [ \"s3:DeleteBucket\", \"s3:DeleteBucketPolicy\", \"s3:DeleteBucketWebsite\", \"s3:DeleteObject\", \"s3:DeleteObjectVersion\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] } ] } . Leider wurde das S3-Protokoll selbst nicht mit dem Konzept von WORM-Backups (Write Once Read Many) im Hinterkopf entwickelt. Zugriffsrichtlinienberechtigungen unterscheiden nicht zwischen dem Ändern eines vorhandenen Objekts (was ein effektives Löschen ermöglichen würde) und dem Erstellen eines neuen Objekts. Das Anhängen der obigen Richtlinie an einen Bucket verhindert nicht das Überschreiben der darin enthaltenen Objekte. $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.46 KB/s done # die Richtlinie erlaubt Schreiben $ s3cmd rm s3://appendonly-bucket/testfile ERROR: S3 error: 403 (AccessDenied) # die Richtlinie verweigert das Löschen $ $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.50 KB/s done # :( . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#hintergrund",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#hintergrund"
  },"288": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Vorgeschlagene Lösung",
    "content": "Da ein Angreifer auf einem kompromittierten System immer Zugriff auf die S3-Anmeldeinformationen und jeden auf dem System laufenden Dienst hat – einschließlich Restic selbst, Proxys usw. – benötigen wir eine zweite, abgesicherte VM, die Löschvorgänge einschränken kann. Restic lässt sich perfekt in rclone integrieren, daher verwenden wir es in diesem Beispiel. Our environment . | appsrv: das zu sichernde System, mit Zugriff auf die Sicherungen | rclonesrv: das System, auf dem der rclone-Proxy ausgeführt wird (und nichts anderes, um die Angriffsfläche zu minimieren) | . Den rclone-Proxy einrichten . | rclone auf dem rclonesrv installieren: . sudo apt install rclone . | User für rclone anlegen . sudo useradd -m rcloneproxy sudo su - rcloneproxy . | Die Backend-Konfiguration für rclone backend erstellen . mkdir -p .config/rclone cat &lt;&lt; EOF &gt; .config/rclone/rclone.conf [s3-resticrepo] type = s3 provider = Other env_auth = false access_key_id = 111122223333444455556666 secret_access_key = aaaabbbbccccddddeeeeffffgggghhhh region = eu-central-1 endpoint = s3.es1.fra.optimist.gec.io acl = private bucket_acl = private upload_concurrency = 8 EOF . | Den Zugriff auf das Repository testen: . rclone lsd s3-resticrepo:databucket 0 2021-11-21 20:02:10 -1 data 0 2021-11-21 20:02:10 -1 index 0 2021-11-21 20:02:10 -1 keys 0 2021-11-21 20:02:10 -1 snapshots . | . Den Appserver konfigurieren . | Ein SSH-Schlüsselpaar auf appsrv mit dem Benutzer generieren, mit dem Sie das Backup durchführen: . ssh-keygen -o -a 256 -t ed25519 -C \"$(hostname)-$(date +'%d-%m-%Y')\" . | Die Umgebungsvariablen für restic setzen: . export RESTIC_PASSWORD=\"MyV3ryS3cUr3r3571cP4ssW0rd\" export RESTIC_REPOSITORY=rclone:s3-resticrepo:databucket . | . Den SSH-Schlüssel auf restic-only Befehle beschränken . Der letzte Schritt ist nun die SSH-Datei authorized_keys auf dem rclonesrv zu bearbeiten, um den neu generierten SSH-Schlüssel auf einen einzigen Befehl zu beschränken. Auf diese Weise kann ein Angreifer das SSH-Schlüsselpaar nicht verwenden, um beliebige Befehle auf dem rclone-Proxy auszuführen und die Backups zu kompromittieren. vi ~/.ssh/authorized_keys # Fügen Sie inen Eintrag mit dem öffentlichen Schlüssel des restic-Benutzers hinzu, der in dem obigen Schritt generiert wurde: command=\"rclone serve restic --stdio --append-only s3-resticrepo:databucket\" ssh-ed25519 AAAAC3fdsC1lZddsDNTE5ADsaDgfTwNtWmwiocdT9q4hxcss6tGDfgGTdiNN0z7zN appsrv-18-11-2021 . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#vorgeschlagene-l%C3%B6sung",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#vorgeschlagene-lösung"
  },"289": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "restic mit dem rclone Proxy verwenden",
    "content": "Wenn die Umgebungsvariablen gesetzt sind, sollte restic jetzt von dem appsrv aus funktionieren. Beispiel: Sicherung von /srv/myapp: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" backup /srv/myapp . Snapshots auflisten: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" snapshots . Snapshots löschen: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" forget 2738e969 repository b71c391e opened successfully, password is correct Remove(&lt;snapshot/2738e9693b&gt;) returned error, retrying after 446.577749ms: blob not removed, server response: 403 Forbidden (403) . Ah, das geht nicht. Das war ja unser Ziel! . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#restic-mit-dem-rclone-proxy-verwenden",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#restic-mit-dem-rclone-proxy-verwenden"
  },"290": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Zusammenfassung",
    "content": "Auf dieser Art und Weise, . | der rclone-Proxy auf dem rclonerv läuft nicht einmal als Dienst. Es wird nur bei Bedarf für die Dauer der Restic-Operation gestartet. Die Kommunikation erfolgt über HTTP2 über stdin/stdout in einem verschlüsselten SSH-Tunnel. | Da rclone mit --append-only läuft, ist es nicht möglich, Snapshots im S3-Bucket zu löschen (oder zu überschreiben). | Alle Daten (außer Zugangsdaten) werden lokal verschlüsselt/entschlüsselt und dann über rclonesrv an/von S3 gesendet/empfangen. | Alle Zugangsdaten werden nur auf rclonesrv gespeichert, um mit S3 zu kommunizieren. | . Da der Befehl in der SSH-Konfiguration für den SSH-Schlüssel des Benutzers fest eingetragen ist, gibt es keine Möglichkeit, den Schlüssel zu verwenden, um Zugriff auf den rclone-Proxy zu erhalten. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#zusammenfassung",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#zusammenfassung"
  },"291": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Noch ein paar Gedanken",
    "content": "Die Vorteile der Lösung sind wahrscheinlich schon klar. Im Weiteren, . | die Verwaltung von Snapshots (sowohl manuell als auch mit einer Aufbewahrungsrichtlinie) ist nur noch auf dem rclone-Proxy möglich. | Eine einzelne rclone-Proxy-VM (oder sogar ein Docker-Container auf einer isolierten VM) kann mehrere Backup-Clients bedienen. | Es ist sehr empfohlen, für jeden Server, der Daten sichert, einen eigenen Schlüssel zu verwenden. | Wenn Sie mehr als ein Repository aus einem Node verwenden möchten, benötigen Sie dafür neue SSH-Schlüssel. Sie können dann mit -i ~/.ssh/id_ed25519_another_repo in den rclone.program-Argumenten genau wie bei SSH angeben, welcher Schlüssel verwendet werden soll. | . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#noch-ein-paar-gedanken",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#noch-ein-paar-gedanken"
  },"292": {
    "doc": "Sichere Datensicherung mit Restic und Rclone",
    "title": "Sichere Datensicherung mit Restic und Rclone",
    "content": "Restic ist eine sehr einfache und leistungsstarke file-basierte Backup-Lösung, die schnell an Popularität gewinnt. Es kann in Kombination mit S3 verwendet werden, was es zu einem großartigen Werkzeug für Optimist macht. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/"
  },"293": {
    "doc": "Localstorage",
    "title": "Compute Localstorage für Ihre Instanzen",
    "content": "L1 / Localstorage-Flavors können bei support@gec.io angefordert werden. ",
    "url": "/optimist/storage/localstorage/#compute-localstorage-f%C3%BCr-ihre-instanzen",
    
    "relUrl": "/optimist/storage/localstorage/#compute-localstorage-für-ihre-instanzen"
  },"294": {
    "doc": "Localstorage",
    "title": "Was genau ist Compute Localstorage?",
    "content": "Beim Localstorage befindet sich der Storage Ihrer Instanzen direkt auf dem Hypervisor (Server). Die Localstorage Funktion ist über unsere l1 Flavors verfügbar und für Anwendungen vorgesehen, die eine geringe Latenz erfordern. ",
    "url": "/optimist/storage/localstorage/#was-genau-ist-compute-localstorage",
    
    "relUrl": "/optimist/storage/localstorage/#was-genau-ist-compute-localstorage"
  },"295": {
    "doc": "Localstorage",
    "title": "Datensicherheit und Verfügbarkeit",
    "content": "Da Ihre Daten direkt durch Ihre Instanz auf dem Storage des lokalen Hypervisors gebunden sind, empfiehlt es sich, diese Daten mithilfe eines HA Konzepts über die gegebenen Availability Zonen zu verteilen. Das Storage Backend der Localstorage Instanzen ist gegen einen Ausfall einzelner Speichermedien des Arrays geschützt, die dadurch hergestellte Redundanz besteht jedoch gegenüber der Ceph basierten Instanzen nur innerhalb des Hypervisor Nodes welcher die Instanz bereitstellt. Beim Ersetzen von Einzelkomponenten auf Grund eines Hardwaredefekts, kann es bis zur Wiederherstellung kurzfristig zu einer eingeschränkten Verfügbarkeit und Performance kommen. Die Hypervisor unterliegen einem definierten Patch Zyklus, bei dem die Hypervisoren nacheinander gebootet werden müssen. Durch den Localstorage der Instanzen können die Wartungsarbeiten nicht wie bei den auf Ceph Storage basierten Flavors unterbrechungsfrei durchgeführt werden. Aus diesem Grund existiert für l1 Flavors ein regelmäßiges Wartungsfenster. Dabei wird innerhalb einer Availability Zone und innerhalb des festgelegten Wartungsfensters ein Server nach dem anderen aktualisiert und rebootet. Innerhalb des Wartungsfensters werden laufende Instanzen von unserem System heruntergefahren und nach 10 Minuten gestoppt. ",
    "url": "/optimist/storage/localstorage/#datensicherheit-und-verf%C3%BCgbarkeit",
    
    "relUrl": "/optimist/storage/localstorage/#datensicherheit-und-verfügbarkeit"
  },"296": {
    "doc": "Localstorage",
    "title": "Standard Wartungsfenster",
    "content": "| Intervall | Tag | Zeit (in UTC) | . | wöchentlich | Mittwoch | 9:00 Uhr - 16:00 Uhr | . ",
    "url": "/optimist/storage/localstorage/#standard-wartungsfenster",
    
    "relUrl": "/optimist/storage/localstorage/#standard-wartungsfenster"
  },"297": {
    "doc": "Localstorage",
    "title": "Openstack Features",
    "content": "OpenStack bietet Ihnen viele Funktionen für Ihre Instanzen, wie z.B. resize, shelving oder snapshot. Wenn Sie für Ihre Instanzen l1 Flavors verwenden möchten, beachten Sie bitte folgendes: . Resize: Die Option Resize wird Ihnen angezeigt, aber technisch ist es nicht möglich, eine auf einem l1 Flavor basierende Instanz zu resizen. Sie können das umgehen, in dem Sie einen Cluster (Applikationsbezogen) mit l1 Flavors aufsetzen, größere l1 Flavors parallel starten und Ihre Daten von den alten l1 Flavors auf die neuen l1 Flavors rollen. Dies gilt auch bei einem wechsel von einem andren Flavor zu l1 Flavors. Shelving/Snapshotting: Beide Features sind möglich, aber aufgrund der Disk Size innerhalb der l1 Flavors raten wir wegen der langen Uploadzeiten davon ab. Hier empfiehlt es sich, die für die Applikation vorgesehene externe Backup-Lösung zu nutzen. ",
    "url": "/optimist/storage/localstorage/#openstack-features",
    
    "relUrl": "/optimist/storage/localstorage/#openstack-features"
  },"298": {
    "doc": "Localstorage",
    "title": "Localstorage",
    "content": " ",
    "url": "/optimist/storage/localstorage/",
    
    "relUrl": "/optimist/storage/localstorage/"
  },"299": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/optimist/faq/",
    
    "relUrl": "/optimist/faq/"
  },"300": {
    "doc": "FAQ",
    "title": "Der Befehl openstack --help zeigt bei vielen Punkten “Could not load EntryPoint.parse” an.",
    "content": "In diesem Fall ist eine der mit dem OpenStack Client installierten Komponenten nicht aktuell. Um zu sehen, welche der Komponenten aktualisiert werden muss, rufen Sie den folgenden Befehl auf: . openstack --debug --help . Hier wird nun vor jedem Punkt angezeigt, was genau der Fehler ist und man kann einfach die jeweilige Komponente mit dem folgenden Befehl aktualisieren (&lt;PROJECT&gt; muss durch die richtige Komponente ersetzt werden): . pip install python-&lt;PROJECT&gt;client -U . ",
    "url": "/optimist/faq/#der-befehl-openstack---help-zeigt-bei-vielen-punkten-could-not-load-entrypointparse-an",
    
    "relUrl": "/optimist/faq/#der-befehl-openstack---help-zeigt-bei-vielen-punkten-could-not-load-entrypointparse-an"
  },"301": {
    "doc": "FAQ",
    "title": "Wie kann ich VRRP nutzen?",
    "content": "Um VRRP nutzen zu können, muss dies in einer Security-Group aktiviert und dann den jeweiligen Instanzen zugeordnet werden. Aktuell ist dies nur mit dem OpenStack Client möglich, zum Beispiel: . openstack security group rule create --remote-ip 10.0.0.0/24 --protocol vrrp --ethertype IPv4 --ingress default . ",
    "url": "/optimist/faq/#wie-kann-ich-vrrp-nutzen",
    
    "relUrl": "/optimist/faq/#wie-kann-ich-vrrp-nutzen"
  },"302": {
    "doc": "FAQ",
    "title": "Warum werden mir Floating IPs berechnet, die ich gar nicht benutze?",
    "content": "Der Grund dafür ist mit hoher Wahrscheinlichkeit, dass Floating IPs erstellt wurden, aber nach der Benutzung nicht korrekt gelöscht wurden. Um eine Übersicht über die aktuell verwendeten Floating IPs zu erhalten, können Sie das Horizon Dashboard nutzen. Dort befindet sich der entsprechende Punkt unter Project → Network → Floating-IPs. Alternativ können Sie den Weg über den OpenStack Client nutzen: . $ openstack floating ip list +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | Floating Network | Project | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | 84eca713-9ac1-42c3-baf6-860ba920a23c | 185.116.245.222 | 192.0.2.7 | a3097883-21cc-49fa-a060-bccc1678ece7 | 54258498-a513-47da-9369-1a644e4be692 | b15cde70d85749689e6568f973bb002 | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ . ",
    "url": "/optimist/faq/#warum-werden-mir-floating-ips-berechnet-die-ich-gar-nicht-benutze",
    
    "relUrl": "/optimist/faq/#warum-werden-mir-floating-ips-berechnet-die-ich-gar-nicht-benutze"
  },"303": {
    "doc": "FAQ",
    "title": "Wie kann ich den Flavor einer Instanz ändern (instance resize)?",
    "content": "Resizing über die Command Line . Geben Sie den Namen oder die UUID des Servers an, dessen Größe Sie ändern möchten, und ändern Sie die Größe mit dem Befehl openstack server resize. Geben Sie das gewünschte neue Flavor und dann den Instanznamen oder die UUID an: . openstack server resize --flavor FLAVOR SERVER . Die Größenänderung kann einige Zeit in Anspruch nehmen. Während dieser Zeit wird der Instanzstatus als RESIZE angezeigt. Wenn die Resizing abgeschlossen ist, wird der Instanzstatus VERIFY_RESIZE angezeigt. Sie können nun die Größenänderung bestätigen, um den Status auf ACTIVE zu ändern: . openstack server resize --confirm SERVER . Warnung . Die Größenänderung wird nach einer Stunde automatisch bestätigt, falls sie vorher nicht manuell bestätigt oder rückgängig gemacht wurde. Resizing über das Optimist-Dashboard . Gehen Sie zu Optimist Dashboard → Instances und navigieren Sie zu der Instanz, deren Größe geändert werden soll. Wählen Sie anschließend Actions → Resize Flavor. Der aktuelle Flavor wird angezeigt. Verwenden Sie die Dropdown-Liste “Select a new flavor”, wählen Sie den neuen Flavor aus und bestätigen Sie mit “Resize”. Warnung . Dies gilt nicht für l1 (localstorage) Flavors. Weitere Informationen finden Sie unter Storage → Localstorage. ",
    "url": "/optimist/faq/#wie-kann-ich-den-flavor-einer-instanz-%C3%A4ndern-instance-resize",
    
    "relUrl": "/optimist/faq/#wie-kann-ich-den-flavor-einer-instanz-ändern-instance-resize"
  },"304": {
    "doc": "FAQ",
    "title": "Warum ist das Logfile der Compute Instanz im Optimist Dashboard leer?",
    "content": "Bedingt durch Wartungsarbeiten oder einem Umverteilen der Last im OpenStack wurde die Instanz verschoben. In diesem Fall wird das Logfile neu angelegt und neue Meldungen werden wie gewohnt protokolliert. ",
    "url": "/optimist/faq/#warum-ist-das-logfile-der-compute-instanz-im-optimist-dashboard-leer",
    
    "relUrl": "/optimist/faq/#warum-ist-das-logfile-der-compute-instanz-im-optimist-dashboard-leer"
  },"305": {
    "doc": "FAQ",
    "title": "Warum erhalte ich den Fehler “Conflict (HTTP 409)” beim Erstellen eines Swift Containers?",
    "content": "Swift verwendet einzigartige Namen über die gesamte OpenStack Umgebung hinweg. Die Fehlermeldung besagt, dass der gewählte Name bereits in Verwendung ist. ",
    "url": "/optimist/faq/#warum-erhalte-ich-den-fehler-conflict-http-409-beim-erstellen-eines-swift-containers",
    
    "relUrl": "/optimist/faq/#warum-erhalte-ich-den-fehler-conflict-http-409-beim-erstellen-eines-swift-containers"
  },"306": {
    "doc": "FAQ",
    "title": "Anbringen von Cinder-Volumes an Instanzen per UUID",
    "content": "Wenn Sie mehrere Cinder-Volumes an eine Instanz anhängen, werden die Mount-Punkte möglicherweise bei jedem Neustart neu gemischt. Durch das Mounten der Volumes mittels der UUID wird sichergestellt, dass die richtigen Volumes wieder an die richtigen Mount-Punkte angehängt werden, falls für die Instanz ein Aus- und Wiedereinschalten erforderlich ist. Nachdem Sie die UUID des Volumes mit blkid in der Instanz abgerufen haben, ändern Sie den Mountpunkt in /etc/fstab wie folgt, um die UUID zu verwenden. Beispiel: . # /boot war auf /dev/sda2 während der Installation /dev/disk/by-uuid/f6a0d6f3-b66c-bbe3-47ba-d264464cb5a2 /boot ext4 defaults 0 2 . ",
    "url": "/optimist/faq/#anbringen-von-cinder-volumes-an-instanzen-per-uuid",
    
    "relUrl": "/optimist/faq/#anbringen-von-cinder-volumes-an-instanzen-per-uuid"
  },"307": {
    "doc": "FAQ",
    "title": "Ist die Nutzung von Cinder multi-attached Volumes möglich?",
    "content": "Wir unterstützen keine multi-attached Volumes in der Optimist Platform, da für die Nutzung von multi-attached Volumes clusterfähige Dateisysteme erforderlich sind, um den gleichzeitigen Zugriff auf das Dateisystem zu koordinieren. Versuche, multi-attached Volumes ohne clusterfähige Dateisysteme zu verwenden, bergen ein hohes Risiko für Datenkorruption, daher ist diese Funktion auf der Optimist Plattform nicht aktiviert. ",
    "url": "/optimist/faq/#ist-die-nutzung-von-cinder-multi-attached-volumes-m%C3%B6glich",
    
    "relUrl": "/optimist/faq/#ist-die-nutzung-von-cinder-multi-attached-volumes-möglich"
  },"308": {
    "doc": "FAQ",
    "title": "Warum kann ich keinen Snapshot einer laufenden Instance erstellen?",
    "content": "Um konsistente Snapshots zu erstellen, verwendet die Optimist-Plattform das Property os_require_quiesce=yes. Diese Eigenschaft ermöglicht die Nutzung von fsfreeze, um den Zugriff auf laufende Instanzen auszusetzen, um einen konsistenten Snapshot der Instanz zu erstellen. Zur Erstellung von Snapshots von Instanzen stehen in der Optimist Platform die folgenden Optionen zur Verfügung: . Die erste Option besteht darin, einen Snapshot der laufenden Instanz zu erstellen, indem der qemu-guest-agent installiert und ausgeführt wird. Dieser kann wie folgt installiert und ausgeführt werden: . apt install qemu-guest-agent systemctl start qemu-guest-agent systemctl enable qemu-guest-agent . Außerdem empfehlen wir beim Hochladen Ihrer eigenen Images, dass Sie --property hw_qemu_guest_agent=True als Property an Ihren Images hinzufügen. Sobald der qemu-guest-agent läuft, kann der Snapshot erstellt werden. Die zweite Möglichkeit besteht darin, die laufende Instanz zu stoppen, den Snapshot zu erstellen und die Instanz schließlich erneut zu starten. Dies kann über das Horizon Dashboard oder auf der CLI wie folgt erfolgen: . openstack server stop ExampleInstance openstack server image create --name ExampleInstanceSnapshot ExampleInstance openstack server start ExampleInstance . ",
    "url": "/optimist/faq/#warum-kann-ich-keinen-snapshot-einer-laufenden-instance-erstellen",
    
    "relUrl": "/optimist/faq/#warum-kann-ich-keinen-snapshot-einer-laufenden-instance-erstellen"
  },"309": {
    "doc": "Spezifikationen",
    "title": "Spezifikationen",
    "content": " ",
    "url": "/optimist/specs/",
    
    "relUrl": "/optimist/specs/"
  },"310": {
    "doc": "Flavor Spezifikationen",
    "title": "Flavor Spezifikationen",
    "content": "“Flavor” bezeichnet im OpenStack-Kontext ein Hardware-Profil, das eine virtuelle Maschine nutzt bzw. nutzen kann. Im Optimist sind diverse Standard-Hardwareprofile (Flavors) eingerichtet. Diese haben unterschiedliche Limits und Begrenzungen, welche hier für alle verfügbaren Flavors aufgelistet sind. L1 / Localstorage-Flavors können bei support@gec.io angefordert werden. ",
    "url": "/optimist/specs/flavor_specification/",
    
    "relUrl": "/optimist/specs/flavor_specification/"
  },"311": {
    "doc": "Flavor Spezifikationen",
    "title": "Migration zwischen Flavor-Typen",
    "content": "Um die Flavors bestehender Instanzen zu ändern, kann die OpenStack-Option „Resize Instance“ entweder über das Dashboard oder die CLI verwendet werden. Dies führt zu einem Neustart der Instanz, aber der Inhalt der Instanz bleibt erhalten. Bitte beachten Sie, dass ein Wechsel der Flavors von den großen Root-Disk-Typen zu einem Flavor mit einer kleineren Root-Disk nicht möglich ist. Warnung . Dies gilt nicht für l1 (localstorage) Flavors. Weitere Informationen finden Sie unter Storage → Localstorage. ",
    "url": "/optimist/specs/flavor_specification/#migration-zwischen-flavor-typen",
    
    "relUrl": "/optimist/specs/flavor_specification/#migration-zwischen-flavor-typen"
  },"312": {
    "doc": "Flavor Spezifikationen",
    "title": "Flavor-Typen",
    "content": "Standard-Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | s1.micro | 1 | 2 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | s1.small | 2 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | s1.medium | 4 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | s1.large | 8 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.xlarge | 16 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.2xlarge | 30 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Standard CPU Large Disk Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | s1.micro.d | 1 | 2 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | s1.small.d | 2 | 4 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | s1.medium.d | 4 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | s1.large.d | 8 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.xlarge.d | 16 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.2xlarge.d | 30 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Dedicated CPU Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | d1.micro | 1 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | d1.small | 2 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | d1.medium | 4 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | d1.large | 8 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.xlarge | 16 | 128 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.2xlarge | 30 | 256 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Dedicated CPU Large Disk Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | d1.micro.d | 1 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | d1.small.d | 2 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | d1.medium.d | 4 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | d1.large.d | 8 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.xlarge.d | 16 | 128 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.2xlarge.d | 30 | 256 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Localstorage Flavors . | Bezeichnung | Kerne | RAM | Disk | IOPS Limits (read/write) | IO throughput rate (read/write) | Network Bandwidth | . | l1.micro | 1 | 8 GB | 300 GB | 25000 / 10000 | 125 MB/s / 60 MB/s | 1 Gbit/s | . | l1.small | 2 | 16 GB | 600 GB | 50000 / 25000 | 250 MB/s / 125 MB/s | 2 Gbit/s | . | l1.medium | 4 | 32 GB | 1200 GB | 100000 / 50000 | 500 MB/s / 250 MB/s | 3 Gbit/s | . | l1.large | 8 | 64 GB | 2500 GB | 100000 / 100000 | 1000 MB/s / 500 MB/s | 4 Gbit/s | . | l1.xlarge | 16 | 128 GB | 5000 GB | 100000 / 100000 | 2000 MB/s / 1125 MB/s | 4 Gbit/s | . | l1.2xlarge | 30 | 256 GB | 10000 GB | 100000 / 100000 | 2000 MB/s / 2000 MB/s | 4 Gbit/s | . ",
    "url": "/optimist/specs/flavor_specification/#flavor-typen",
    
    "relUrl": "/optimist/specs/flavor_specification/#flavor-typen"
  },"313": {
    "doc": "Default Quotas",
    "title": "OpenStack Default Quotas",
    "content": "Im Optimist haben wir Standardwerte für den OpenStack Compute-Dienst, den OpenStack Block Storage-Dienst und den OpenStack Networking-Dienst definiert. Wir haben auch separate Quotas für den Octavia Loadbalancer-Dienst und die zugehörigen Komponenten. Diese Standardwerte sind unten aufgeführt. ",
    "url": "/optimist/specs/default_quota/#openstack-default-quotas",
    
    "relUrl": "/optimist/specs/default_quota/#openstack-default-quotas"
  },"314": {
    "doc": "Default Quotas",
    "title": "Compute Standardwerte",
    "content": "| Ressource | Wert | . | Cores | 256 | . | Fixed IPs | Unlimited | . | Floating IPs | 15 | . | Injected File Size | 10240 (MB) | . | Injected Files | 100 | . | Instances | 100 | . | Key Pairs | 100 | . | Properties | 128 | . | Ram | 524288 (MB) | . | Server Groups | 10 | . | Server Group Members | 10 | . ",
    "url": "/optimist/specs/default_quota/#compute-standardwerte",
    
    "relUrl": "/optimist/specs/default_quota/#compute-standardwerte"
  },"315": {
    "doc": "Default Quotas",
    "title": "Block Storage Standardwerte",
    "content": "| Ressource | Wert | . | Backups | 100 | . | Backup Gigabytes | 10000 (MB) | . | Gigabytes | 10000 (MB) | . | Per-volume-gigabytes | Unlimited | . | Snapshots | 100 | . | Volumes | 100 | . ",
    "url": "/optimist/specs/default_quota/#block-storage-standardwerte",
    
    "relUrl": "/optimist/specs/default_quota/#block-storage-standardwerte"
  },"316": {
    "doc": "Default Quotas",
    "title": "Network Standardwerte",
    "content": "| Ressource | Wert | . | Floating IPs | 15 | . | Secgroup Rules | 1000 | . | Secgroups | 100 | . | Networks | 100 | . | Subnets | 200 | . | Ports | 500 | . | Routers | 50 | . | RBAC Policies | 100 | . | Subnetpools | Unlimited | . ",
    "url": "/optimist/specs/default_quota/#network-standardwerte",
    
    "relUrl": "/optimist/specs/default_quota/#network-standardwerte"
  },"317": {
    "doc": "Default Quotas",
    "title": "Octavia Loadbalancers",
    "content": "| Ressource | Wert | . | Load Balancers | 100 | . | Listeners | 100 | . | Pools | 100 | . | Health Monitors | 100 | . | Members | 100 | . ",
    "url": "/optimist/specs/default_quota/#octavia-loadbalancers",
    
    "relUrl": "/optimist/specs/default_quota/#octavia-loadbalancers"
  },"318": {
    "doc": "Default Quotas",
    "title": "Default Quotas",
    "content": " ",
    "url": "/optimist/specs/default_quota/",
    
    "relUrl": "/optimist/specs/default_quota/"
  },"319": {
    "doc": "Application Credentials",
    "title": "Einführung",
    "content": "Benutzer können Application Credentials erstellen, damit sich ihre Anwendungen bei der OpenStack-Komponente Keystone authentifizieren können, ohne ihre eigenen Anmeldeinformationen des Benutzers verwenden zu müssen. Mit Application Credentials können sich Anwendungen mit der Application Credential-ID und einer geheimen Zeichenfolge authentifizieren, die nicht das Kennwort des Benutzers ist. Auf diese Weise wird das Passwort des Benutzers nicht in die Konfiguration der Anwendung eingebettet. Benutzer können eine Teilmenge ihrer Rollenzuweisungen für ein Projekt an Application Credentials delegieren und der Anwendung dieselben oder eingeschränkte Berechtigungen innerhalb eines Projekts erteilen. ",
    "url": "/optimist/specs/application_credentials/#einf%C3%BChrung",
    
    "relUrl": "/optimist/specs/application_credentials/#einführung"
  },"320": {
    "doc": "Application Credentials",
    "title": "Anforderungen für Application Credentials",
    "content": "Name / Secrets . Application Credentials für Ihr Projekt können über die Befehlszeile oder über das Dashboard generiert werden. Diese werden dem Projekt zugeordnet, in dem sie erstellt werden. Der einzige erforderliche Parameter zum Erstellen der Anmeldeinformationen ist ein Name, jedoch kann mit dem Parameter —-secret ein bestimmtes Secret festgelegt werden. Ohne Parameter wird stattdessen automatisch ein Secret in der Ausgabe generiert. Es ist in jedem Fall wichtig das Secret zu notieren, da dieses vor dem Speichern gehasht wird und nach dem Festlegen nicht wiederhergestellt werden kann. Wenn das Secret verloren geht, müssen neue Application Credential für die Anwendung erstellt werden. Roles . Wir empfehlen außerdem, die Roles festzulegen, die die Application Credentials der Anwendung im Projekt haben sollen, da standardmäßig ein neu erstellter Satz von Anmeldeinformationen alle verfügbaren Roles erbt. Im Folgenden sind die verfügbaren Roles aufgeführt, die einem Satz von Application Credentials zugewiesen werden können. Wenn Sie diese Roles mithilfe des Parameters --role auf einen Satz von Application Credentials anwenden, beachten Sie bitte, dass bei allen Role-Namen die Groß-/Kleinschreibung beachtet wird: . | Member: Die Rolle “Member” hat nur administrativen Zugriff auf das zugewiesene Projekt. | heat_stack_owner: Als “heat_stack_owner” können Sie vorhandene HEAT-Templates verwenden und ausführen. | load-balancer_member: Als „load-balancer_member“ können Sie die Octavia LoadBalancer-Ressourcen nutzen. | . Expiration . Standardmäßig laufen erstellte Application Credentials nicht ab, jedoch können feste Ablaufdaten/-zeiten für Application Credentials bei der Erstellung festgelegt werden, indem der Parameter --expires im Befehl verwendet wird (zum Beispiel: --expires '2021-07-15T21: 00:00'). ",
    "url": "/optimist/specs/application_credentials/#anforderungen-f%C3%BCr-application-credentials",
    
    "relUrl": "/optimist/specs/application_credentials/#anforderungen-für-application-credentials"
  },"321": {
    "doc": "Application Credentials",
    "title": "Erstellen von Application Credentials über die CLI",
    "content": "Ein Set von Application Credentials kann im gewünschten Projekt über die CLI erstellt werden. Das folgende Beispiel zeigt, wie ein Set von Credentials mit den folgenden Parametern erstellt wird: . | Name: test-credentials | Secret: ZYQZm2k6pk | Roles: Member, heat_stack_owner, load-balancer_member | Expiration Date/Time: 2021-07-12 at 21:00:00 | . Die neuen Zugangsdaten sollten wie folgt aussehen: . $ openstack application credential create test-credentials --secret ZYQZm2k6pk --role Member --role heat_stack_owner --role load-balancer_member --expires '2021-07-15T21:00:00' +--------------+----------------------------------------------+ | Field | Value | +--------------+----------------------------------------------+ | description | None | expires_at | 2021-07-15T21:00:00.000000 | id | 707d14e835124b4f957938bb5a57d1be | name | test-credentials | project_id | c704ac5a32b84b54a0407d28ad448399 | roles | Member heat_stack_owner load-balancer_member | secret | ZYQZm2k6pk | system | None | unrestricted | False | user_id | 1d9f1ecb5de3607e8982695f72036fa5 | +--------------+----------------------------------------------+ . Hinweis: Das Secret (ob vom Benutzer festgelegt oder automatisch generiert) wird nur beim Erstellen der Application Credentials angezeigt. Bitte notieren Sie sich das Secret zu diesem Zeitpunkt. ",
    "url": "/optimist/specs/application_credentials/#erstellen-von-application-credentials-%C3%BCber-die-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#erstellen-von-application-credentials-über-die-cli"
  },"322": {
    "doc": "Application Credentials",
    "title": "Anzeigen von Application Credentials über die CLI",
    "content": "Die Liste der zu einem Projekt gehörenden Application-Credentials kann mit dem folgenden Befehl aufgelistet werden. $ openstack application credential list +----------------------------------+-------------------+----------------------------------+-------------+------------+ | ID | Name | Project ID | Description | Expires At | +----------------------------------+-------------------+----------------------------------+-------------+------------+ | 707d14e835124b4f957938bb5a57d1be | test-credentials | c704ac5a32b84b54a0407d28ad448399 | None | None | +----------------------------------+-------------------+----------------------------------+-------------+------------+ . Einzelne Credentials können mit dem $ openstack application credential show &lt;name&gt; Befehl angezeigt werden. ",
    "url": "/optimist/specs/application_credentials/#anzeigen-von-application-credentials-%C3%BCber-die-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#anzeigen-von-application-credentials-über-die-cli"
  },"323": {
    "doc": "Application Credentials",
    "title": "Löschen von Application Credentials über die CLI",
    "content": "Application Credentials können über die CLI mit dem folgenden Befehl mit dem Namen oder der ID des spezifischen Satzes von Anmeldeinformationen gelöscht werden: . openstack application credential delete test-credentials . ",
    "url": "/optimist/specs/application_credentials/#l%C3%B6schen-von-application-credentials-%C3%BCber-die-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#löschen-von-application-credentials-über-die-cli"
  },"324": {
    "doc": "Application Credentials",
    "title": "Erstellen und Löschen von Application Credentials für Anwendungen über das Optimist Dashboard",
    "content": "Alternativ können Application Credentials auch über das Optimist Dashboard unter Identität &gt; Application Credentials generiert werden: . Hinweis: Hier können mehrere Rollen ausgewählt werden, indem Sie die Umschalttaste gedrückt halten und durch die Optionen navigieren. Nach der Erstellung wird ein Dialogfeld angezeigt, in dem Sie aufgefordert werden, die ID und das Secret zu notieren. Wenn Sie fertig sind, klicken Sie auf “Close”. Die Zugangsdaten hier können jederzeit gelöscht werden, indem Sie den zu löschenden Zugangsdatensatz zu markieren und dann auf “DELETE APPLICATION CREDENTIAL” klicken. ",
    "url": "/optimist/specs/application_credentials/#erstellen-und-l%C3%B6schen-von-application-credentials-f%C3%BCr-anwendungen-%C3%BCber-das-optimist-dashboard",
    
    "relUrl": "/optimist/specs/application_credentials/#erstellen-und-löschen-von-application-credentials-für-anwendungen-über-das-optimist-dashboard"
  },"325": {
    "doc": "Application Credentials",
    "title": "Application Credentials testen",
    "content": "Sobald wir über die CLI oder das Dashboard einen Satz von Application Credentials erstellt haben, können wir sie mit dem folgenden curl-Befehl testen, um zu überprüfen, ob sie funktionieren. Wir müssen unsere &lt;name&gt; und &lt;secret&gt; im curl-Befehl verwenden: . curl -i -H \"Content-Type: application/json\" -d ' { \"auth\": { \"identity\": { \"methods\": [\"application_credential\"], \"application_credential\": { \"id\": “&lt;id&gt;\", \"secret\": “&lt;secret&gt;\"}}}}' https://identity.optimist.gec.io/v3/auth/tokens . Ein erfolgreicher curl-Versuch gibt ein x-subject-token aus, erfolglose Versuche mit falschen Anmeldeinformationen führen zu einem 401-Fehler. ",
    "url": "/optimist/specs/application_credentials/#application-credentials-testen",
    
    "relUrl": "/optimist/specs/application_credentials/#application-credentials-testen"
  },"326": {
    "doc": "Application Credentials",
    "title": "Application Credentials",
    "content": " ",
    "url": "/optimist/specs/application_credentials/",
    
    "relUrl": "/optimist/specs/application_credentials/"
  },"327": {
    "doc": "Volumenspezifikationen",
    "title": "Volumenspezifikationen",
    "content": "In OpenStack sind “Volumes” persistente Speicher, die Sie an Ihre laufenden OpenStack Compute-Instanzen anhängen können. In Optimist haben wir drei Serviceklassen für Volumes eingerichtet. Diese haben unterschiedliche Limits, die unten aufgeführt sind. ",
    "url": "/optimist/specs/volume_specification/",
    
    "relUrl": "/optimist/specs/volume_specification/"
  },"328": {
    "doc": "Volumenspezifikationen",
    "title": "Volume-Typen",
    "content": "Wir haben drei Hauptvolumentypen: . | high-iops | default | low-iops | . ",
    "url": "/optimist/specs/volume_specification/#volume-typen",
    
    "relUrl": "/optimist/specs/volume_specification/#volume-typen"
  },"329": {
    "doc": "Volumenspezifikationen",
    "title": "Volumen-Typen-Liste",
    "content": "Nachfolgend eine Übersicht der drei Volume-Typen: . | Name | Read Bytes Sec | Read IOPS Sec | Write Bytes Sec | Write IOPS Sec | . | high-iops | 524288000 | 10000 | 524288000 | 10000 | . | default | 209715200 | 2500 | 209715200 | 2500 | . | low-iops | 52428800 | 300 | 52428800 | 300 | . ",
    "url": "/optimist/specs/volume_specification/#volumen-typen-liste",
    
    "relUrl": "/optimist/specs/volume_specification/#volumen-typen-liste"
  },"330": {
    "doc": "Volumenspezifikationen",
    "title": "Auswählen eines Volume-Typs",
    "content": "Sie können beim Erstellen eines Volumes mit dem folgenden Befehl einen der drei Volume-Typen auswählen (Wenn nicht anders angegeben, wird immer der Typ „Standard“ verwendet): $ openstack volume create &lt;volume-name&gt; --size 10 --type high-iops . ",
    "url": "/optimist/specs/volume_specification/#ausw%C3%A4hlen-eines-volume-typs",
    
    "relUrl": "/optimist/specs/volume_specification/#auswählen-eines-volume-typs"
  },"331": {
    "doc": "Images",
    "title": "Images",
    "content": "Es gibt 4 Arten von Images in OpenStack: . | Public Images: Diese Images werden von uns gepflegt, sind für alle Benutzer verfügbar, werden regelmäßig aktualisiert und zur Verwendung empfohlen. | Community Images: Ehemals öffentliche Images, die durch neuere Versionen ersetzt wurden. Wir behalten diese Images, bis sie nicht mehr verwendet werden, um Ihre Deployments nicht zu gefährden. | Private Images: Von Ihnen hochgeladene Images, die nur für Ihr Projekt verfügbar sind. | Shared Images: Private Images, die entweder durch Sie oder oder mit Ihnen in mehreren Projekten gemeinsam genutzt werden. | . Nur die ersten beiden Typen werden von uns verwaltet. ",
    "url": "/optimist/specs/images/",
    
    "relUrl": "/optimist/specs/images/"
  },"332": {
    "doc": "Images",
    "title": "Public und community images",
    "content": "Um Ihren Aufwand so gering wie möglich zu halten, stellen wir Ihnen eine Reihe ausgewählter Images zur Verfügung. Aktuell enthält diese Liste: . | Ubuntu 24.04 LTS (Noble Numbat) | Ubuntu Minimal 24.04 LTS (Noble Numbat) | Ubuntu 22.04 LTS (Jammy Jellyfish) | Ubuntu Minimal 22.04 LTS (Jammy Jellyfish) | Ubuntu 20.04 LTS (Focal Fossa) | Debian 12 (Bookworm) | Debian 11 (Bullseye) | Debian 10 (Buster) | CentOS 8 | CentOS 7 | CoreOS (stable) | Flatcar Linux | Windows Server 2019 (GUI/Core) | . Diese Images werden täglich auf neue Versionen überprüft. Die neueste verfügbare Version ist immer ein “public image” und endet auf Latest. Alle vorherigen Versionen eines Images werden durch unseren Automatismus in “community images” umgewandelt, umbenannt (latest wird durch das Datum des ersten Uploads ersetzt), und bei ausbleibender Verwendung (keinerlei Nutzung) schlussendlich gelöscht. OpenStack und viele Deployment-Tools unterstützen die Verwendung dieser Images entweder über den Namen oder über ihre UUID. Durch die Verwendung eines Namens, z.B. Ubuntu 22.04 Jammy Jellyfish - Latest, Erhalten sie jeweils die aktuellste Version des jeweiligen Images, indem Sie Ihre Instanzen neu bereitstellen oder neu aufbauen, selbst wenn wir das Image zwischendurch ersetzen. Sie können dieses Verhalten vermeiden, indem Sie stattdessen die UUID verwenden. Dies kann für Cluster-Einsätze nützlich sein, bei denen Sie sicherstellen wollen, dass auf allen Instanzen die gleiche Version des Images läuft. ",
    "url": "/optimist/specs/images/#public-und-community-images",
    
    "relUrl": "/optimist/specs/images/#public-und-community-images"
  },"333": {
    "doc": "Images",
    "title": "Linux Images",
    "content": "Alle von uns zur Verfügung gestellten Linux-Images sind unmodifiziert und kommen direkt von ihren offiziellen Maintainern. Wir testen sie während des Upload-Prozesses auf Kompatibilität. ",
    "url": "/optimist/specs/images/#linux-images",
    
    "relUrl": "/optimist/specs/images/#linux-images"
  },"334": {
    "doc": "Images",
    "title": "Windows Images",
    "content": "Was ist drin? . Leider gibt es keine vorgefertigten Images für Windows-Deployments, haben wir eigene gebaut. Unsere Anpassungen sind minimal, gerade genug, um eine einfache Nutzung innerhalb unserer Instanzen zu ermöglichen. Unsere Images basieren auf einer regulären Installation von Windows Server 2019 Standard Edition, Version 1809 (LTSC). Unsere Image Builds enthalten die aktuellsten Treiber für unsere Virtualisierungsinfrastruktur, für die Netzwerkkarte und Festplatten hinzugefügt. Außerdem haben wir die neueste OpenSSH-Version für Windows und die neueste Version der PowerShell installiert. Beide sind für die folgenden Schritte erforderlich und ermöglichen Ihnen die erste Verbindung mit Ihrer Instanz. Des weiteren ist der RDP-Dienst aktiviert, der für eine Remote-Desktop-Verbindung erforderlich ist. Vergessen Sie nicht, die dafür erforderlichen Sicherheitsgruppen hinzuzufügen, und achten Sie darauf, den Zugriff so weit wie möglich einzuschränken. Außerdem haben wir aus Sicherheitsgründen AutoLogon deaktiviert. Unsere Images sind außerdem mit aktivierten Spectre- und Meltdown-Mitigations ausgestattet. Außerdem mussten wir die Nutzung von zufälligen MAC-Adressen deaktivieren, da unsere virtuellen Netzwerke feste MAC-Adressen voraussetzen. Für einen schnellen Start und Ihre Sicherheit stellen wir diese Windows-Images mit den neuesten kumulativen Updates für Windows und das .NET Framework bereit. Nach dem Hochfahren einer Instanz müssen Sie wahrscheinlich nur die Windows-Defender-Definitionen aktualisieren. Schließlich haben wir die verfügbaren DotNetAssemblies optimiert, Firewall-Regeln hinzugefügt, um ICMP-Echoantworten zuzulassen, und cloud-init installiert. Letzteres ist für das Hinzufügen Ihrer ssh-Schlüssel zu den neuen Instanzen verantwortlich. Und wie? . Fast genauso einfach wie mit unseren Linux-Instanzen. Importieren Sie Ihren SSH-Schlüssel in OpenStack (CLI oder Dashboard) und starten Sie Ihre Instanzen. Danach können Sie sich mit folgendem Befehl anmelden: . ssh -i ~/.ssh/id_rsa $instanceIP -l Administrator . Einmal eingeloggt, können Sie nun ein neues Administrator-Passwort vergeben, mit dem Sie sich am Remote-Desktop einloggen können: . net user Administrator $password . Wir raten dringend davon ab, veraltete Verfahren wie z.B. ein admin_pass über die Instanz-Metadaten zu setzen. Hierbei wird nichts verschlüsselt oder anderweitig gesichert, und wird außerdem nicht funktionieren, sollte Ihr Passwort nicht den nötigen Sicherheitsrichtlinien entsprechen. Achtung: Unsere Windows-Images enthalten weder Produkt-Schlüssel, noch Lizenzen. Sie werden Ihre eigenen verwenden müssen. ",
    "url": "/optimist/specs/images/#windows-images",
    
    "relUrl": "/optimist/specs/images/#windows-images"
  },"335": {
    "doc": "Images",
    "title": "Upload von eigenen Images",
    "content": "Sie können jederzeit Ihre eigenen Images hochladen, anstatt die von uns bereit gestellten zu nutzen. Am einfachsten funktioniert das über die OpenStack-CLI. openstack image create \\ --property hw_disk_bus=scsi \\ --property hw_qemu_guest_agent=True \\ --property hw_scsi_model=virtio-scsi \\ --property os_require_quiesce=True \\ --private \\ --disk-format qcow2 \\ --container-format bare \\ --file ~/my-image.qcow2 \\ my-image . Dabei müssen mindestens folgende Parameter spezifiziert werden: . | --disk-format: Das Format Ihres Quell-Images, z.B. qcow2 | --file: Das Quell-Image auf Ihrem System | Name des Abbilds: my-image als Beispiel. | . Um die Erstellung von Snapshots für laufende Instanzen zu ermöglichen ist es notwendig, dass Sie das Property --property hw_qemu_guest_agent=True an den von Ihnen genutzten Images setzen und qemu-guest-agent auf dem System installieren. Weitere Details finden Sie in unseren FAQ. Das gleiche funktioniert auch über das Dashboard. Achten Sie hier darauf, alle der obigen Parameter anzugeben. ",
    "url": "/optimist/specs/images/#upload-von-eigenen-images",
    
    "relUrl": "/optimist/specs/images/#upload-von-eigenen-images"
  },"336": {
    "doc": "Shelving-Instanzen",
    "title": "Shelving-Instanzen",
    "content": " ",
    "url": "/optimist/specs/shelving_instances/",
    
    "relUrl": "/optimist/specs/shelving_instances/"
  },"337": {
    "doc": "Shelving-Instanzen",
    "title": "Einführung",
    "content": "Auf der OpenStack-Plattform haben Sie die Möglichkeit, eine Instanz zurückzustellen. Shelving-Instanzen ermöglichen es Ihnen, eine Instanz zu stoppen, ohne dass sie Ressourcen verbraucht. Eine zurückgestellte Instanz sowie die ihr zugewiesenen Ressourcen (z.B. IP-Adresse, usw.) werden als bootfähige Instanz beibehalten. Diese Funktion kann als Teil eines Lifecycle-Prozesses einer Instanz oder zum Einsparen von Ressourcen verwendet werden. Warnung . Dies gilt nicht für l1 (localstorage) Flavors. Weitere Informationen finden Sie unter Storage → Localstorage. ",
    "url": "/optimist/specs/shelving_instances/#einf%C3%BChrung",
    
    "relUrl": "/optimist/specs/shelving_instances/#einführung"
  },"338": {
    "doc": "Shelving-Instanzen",
    "title": "Shelving einer Instanz",
    "content": "Instanzen auf Openstack können wie folgt abgelegt werden: $ openstack server shelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#shelving-einer-instanz",
    
    "relUrl": "/optimist/specs/shelving_instances/#shelving-einer-instanz"
  },"339": {
    "doc": "Shelving-Instanzen",
    "title": "Unshelving einer Instanz",
    "content": "Instanzen können mit dem folgenden Befehl Unshelved werden: $ openstack server unshelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#unshelving-einer-instanz",
    
    "relUrl": "/optimist/specs/shelving_instances/#unshelving-einer-instanz"
  },"340": {
    "doc": "Shelving-Instanzen",
    "title": "Eventliste für eine Instanz anzeigen",
    "content": "Sie können den Shelving/Unshelving-Verlauf jedes Servers anzeigen, indem Sie sich die Ereignisliste anzeigen lassen: . $ openstack server event list &lt;server-id&gt; +------------------------------------------+--------------------------------------+--------+----------------------------+ | Request ID | Server ID | Action | Start Time | +------------------------------------------+--------------------------------------+--------+----------------------------+ | req-8d593999-a09b-41a7-8916-1d7c28cd4dc0 | 846112be-d107-4c75-db75-a32eb47a78c5 | shelve | 2022-07-17T15:28:08.000000 | req-076969ee-15a4-470e-8913-051c6f9d4bd3 | 846112be-d107-4c75-db75-a32eb47a78c5 | create | 2022-07-19T16:15:22.000000 | +------------------------------------------+--------------------------------------+--------+----------------------------+ . ",
    "url": "/optimist/specs/shelving_instances/#eventliste-f%C3%BCr-eine-instanz-anzeigen",
    
    "relUrl": "/optimist/specs/shelving_instances/#eventliste-für-eine-instanz-anzeigen"
  },"341": {
    "doc": "Shelving-Instanzen",
    "title": "Warum Shelving verwenden?",
    "content": "Diese Funktion ist nützlich, um Instanzen zu archivieren, die Sie derzeit nicht verwenden, aber nicht löschen möchten. Das Shelving einer Instanz ermöglicht es Ihnen, die Instanzdaten und Ressourcenzuordnungen beizubehalten, gibt aber CPU und Arbeitsspeicher Ressourcen der Instanz frei. Wenn Sie eine Instanz zurückstellen, generiert der Compute-Dienst ein Snapshot-Image, das den Status der Instanz erfasst, und lädt es in die Glance-Library hoch. Wenn die Instanz unshelved wird, wird sie mithilfe des Snapshots neu erstellt. Das Snapshot-Image wird gelöscht, wenn die Instanz unshelved oder gelöscht wird. ",
    "url": "/optimist/specs/shelving_instances/#warum-shelving-verwenden",
    
    "relUrl": "/optimist/specs/shelving_instances/#warum-shelving-verwenden"
  },"342": {
    "doc": "Shelving-Instanzen",
    "title": "Abrechnung von Shelved Instances",
    "content": "Bei einer Shelved Instance wird nur die Root Disk der Instanz weiterhin abgerechnet. CPU- und Arbeitsspeicher- Ressourcen aus dem Flavor der Instanz werden ab dem Zeitpunkt des Shelvings nicht mehr in Rechnung gestellt und nach dem unshelving automatisch wieder berechnet. Shelving hat keine Auswirkungen auf die Auslastung der Quotas im Projekt. Shelved Ressourcen werden nicht in der Quota freigegeben, um jederzeit ausreichend Ressourcen für das unshelving der Instanz im Projekt zu gewährleisten. ",
    "url": "/optimist/specs/shelving_instances/#abrechnung-von-shelved-instances",
    
    "relUrl": "/optimist/specs/shelving_instances/#abrechnung-von-shelved-instances"
  },"343": {
    "doc": "Changelog",
    "title": "Changelog Optimist",
    "content": "All notable changes to the Optimist Platform are documented on this page. ",
    "url": "/optimist/changelog/#changelog-optimist",
    
    "relUrl": "/optimist/changelog/#changelog-optimist"
  },"344": {
    "doc": "Changelog",
    "title": "Upcoming",
    "content": "Upcoming changes to the Optimist platform are listed here . ",
    "url": "/optimist/changelog/#upcoming",
    
    "relUrl": "/optimist/changelog/#upcoming"
  },"345": {
    "doc": "Changelog",
    "title": "Completed",
    "content": "2022-04-28 . | Optimist Horizon Upgrade (Train) | . 2022-04-27 . | Optimist Heat Upgrade (Train) | . 2022-04-21 . | Optimist Neutron Upgrade (Train) | . 2022-04-05 . | Optimist Nova Upgrade (Train) | . 2022-03-01 . | Optimist Cinder Upgrade (Train) | . 2022-02-23 . | Optimist Designate Upgrade (Train) | . 2022-02-22 . | Optimist Glance Upgrade (Train) | . 2022-02-10 . | Neutron LBaaS removed from Optimist | . 2022-01-25 . | Optimist Keystone Upgrade (Train) | . 2021-08-24 . | Optimist Cinder Upgrade (Stein) | . 2021-08-18 . | Optimist Neutron Feature: . We activated the internal DNS feature. This allows customers to assign dns names to neutron ports. Nova will automatically add the instance name as dns name to the neutron port. | . 2021-07-20 . | Optimist Neutron Upgrade (Stein) | . 2021-06-23 . | Optimist Nova Upgrade (Stein) | . 2021-06-02 . | Optimist Glance upgrade (Stein) | . 2021-06-01 . | Optimist Keystone upgrade (Stein) | . ",
    "url": "/optimist/changelog/#completed",
    
    "relUrl": "/optimist/changelog/#completed"
  },"346": {
    "doc": "Changelog",
    "title": "Changelog",
    "content": " ",
    "url": "/optimist/changelog/",
    
    "relUrl": "/optimist/changelog/"
  },"347": {
    "doc": "Ueber Managed Kubernetes",
    "title": "Ueber Managed Kubernetes",
    "content": "Es wird keine Deutsche übersetzung der Dokumentation geben, bitte benutzt die englische Variante . ",
    "url": "/managedk8s/about/",
    
    "relUrl": "/managedk8s/about/"
  },"348": {
    "doc": "Support",
    "title": "Support Channels",
    "content": "During the onboarding process, we will choose the suitable communication channel for your organization. The following support channels are available for you to choose from: . Microsoft Teams . We can create a dedicated support channel within Microsoft Teams for your organization. This channel enables real-time communication with our support engineers, allowing for quick response. It’s ideally for organizations that use Microsoft Teams as their primary communication tool. Availability: During standard business hours. Slack . If your organization prefers Slack, we can set up a private Slack channel for direct communication with our support team. This channel provides a space for interactive support, allowing for faster response times and more collaborative problem-solving. It’s suited for teams already using Slack for internal communication. Availability: During standard business hours. Email . For formal communication, you can reach our support team via email. Response Time: Typically, you will receive a response during the working hours. The response time may vary depending on the nature and severity of the issue. Phone Support . For critical issues that require immediate attention, we provide a oncall support. This is ideal for high-priority incidents that may impact your production environment during the non working hours. Availability: 24/7 for critical issues, depending on your SLA. During the onboarding process, we will discuss and agree upon the preferred support channels for your organization. We will also define the service level agreement (SLA) that aligns with your business needs. ",
    "url": "/managedk8s/about/support/#support-channels",
    
    "relUrl": "/managedk8s/about/support/#support-channels"
  },"349": {
    "doc": "Support",
    "title": "Support",
    "content": " ",
    "url": "/managedk8s/about/support/",
    
    "relUrl": "/managedk8s/about/support/"
  },"350": {
    "doc": "Supported Kubernetes Versions",
    "title": "Supported Kubernetes Versions",
    "content": "As part of our Managed Kubernetes service, we ensure that your clusters are always running on supported and stable Kubernetes versions. To provide clarity on our version support policy, here is an overview of our supported Kubernetes versions, deprecation, and end-of-life (EOL). | Version | Deprecation | End-of-Life | . | v1.31 |   |   | . | v1.30 |   |   | . | v1.29 |   |   | . ",
    "url": "/managedk8s/about/kubernetesversions/",
    
    "relUrl": "/managedk8s/about/kubernetesversions/"
  },"351": {
    "doc": "Supported Kubernetes Versions",
    "title": "Force Upgrade Policy",
    "content": "If a customer cluster is not updated by its owner until the announced End-of-Life date, it will be automatically upgraded to the next supported version. You can read more about the deprecation and force upgrade policy here. ",
    "url": "/managedk8s/about/kubernetesversions/#force-upgrade-policy",
    
    "relUrl": "/managedk8s/about/kubernetesversions/#force-upgrade-policy"
  },"352": {
    "doc": "Cluster Lifecycle",
    "title": "Cluster Lifecycle",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/",
    
    "relUrl": "/managedk8s/clusterlifecycle/"
  },"353": {
    "doc": "Cluster Creation",
    "title": "Cluster creation",
    "content": "To successfully create a cluster, please provide the required information outlined below. Ensure all details are filled correctly before proceeding with the cluster creation. Copy the block below and complete it with the necessary information for your new cluster: . # required infromation cluster_name: customer_id: # other k8s version (by default latest k8s version provided by us) kubernetes_version: # controlplane flavor # by default s1.medium flavor: ## Machine deployments one block per MD (default: 1 md, 3 replicas, s1.large, random AZ) # machine deployment name # by default clusterName-az-md md_name: # replicas by default 3 replicas: # by default s1.large flavor: # by default random-az availability_zone: ## add the autoscaler (default: disabled) min_size: max_size: . Detailed information about options can be found below. What You Will Receive . After the cluster is created, you will receive an admin kubeconfig through a secure method, along with a unique Cluster ID. For further communication we need the cluster id and cluster name to identify your cluster. ",
    "url": "/managedk8s/clusterlifecycle/clustercreation/#cluster-creation",
    
    "relUrl": "/managedk8s/clusterlifecycle/clustercreation/#cluster-creation"
  },"354": {
    "doc": "Cluster Creation",
    "title": "Detailed Information",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/clustercreation/#detailed-information",
    
    "relUrl": "/managedk8s/clusterlifecycle/clustercreation/#detailed-information"
  },"355": {
    "doc": "Cluster Creation",
    "title": "Prerequisites",
    "content": "OpenStack Credentials . You must have an existing or newly created OpenStack tenant. In this tenant, create and provide the following: . | Application credentialslinked to the project where you want the new cluster to be created. | . Required Information for the New Cluster . | Cluster Name: A maximum of 22 characters. | Customer ID: Defaults to the company ID number if not specified. | Application Credentials: As mentioned in the prerequisites. | . Optional Requirements and Configurable Features (with Default Values) . Kubernetes Version . If not specified, the cluster will be deployed with the latest supported Kubernetes minor version. For more details on supported versions, deprecations, EOL, or other version concerns, click here . OpenStack Network . Provide an existing network ID, or we will create a new network for you. Machine Deployments, Worker Nodes and Autoscaling . For Machine Deployment look into the more detailed docs . As ther are a some more options. Default will get you a single Machine Deployment with 3 Nodes on 1 AZ . Cluster autoscaler (the same for each machineDeployment if more than one) . We support the Cluster Autoscalar which we can activate seperatly for each Machine Deploment. Details can be found here . ",
    "url": "/managedk8s/clusterlifecycle/clustercreation/#prerequisites",
    
    "relUrl": "/managedk8s/clusterlifecycle/clustercreation/#prerequisites"
  },"356": {
    "doc": "Cluster Creation",
    "title": "Cluster Creation",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/clustercreation/",
    
    "relUrl": "/managedk8s/clusterlifecycle/clustercreation/"
  },"357": {
    "doc": "Openstack Application Credentials",
    "title": "Openstack Application Credentials",
    "content": "To set up a managed Kubernetes cluster on OpenStack, we require a pair of OpenStack application credentials to ensure proper access within the OpenStack project. Follow the guidelines Openstack Documentation to create and manage these credentials securely. Please be aware : . | Do not send the credentials through unsecured channels, use only the tools and methods agreed upon during onboarding. | Application credentials should be associated with a dedicated service account, not a personalized user account. This is to avoid issues related to credentials being revoked when a user is offboarded. | If needed, we can rotate the application credentials within the cluster. | . ",
    "url": "/managedk8s/clusterlifecycle/appcredentials/",
    
    "relUrl": "/managedk8s/clusterlifecycle/appcredentials/"
  },"358": {
    "doc": "OIDC",
    "title": "OIDC",
    "content": "You can add a custom oidc configuration for the cluster. This can only be done in the cluster creation process. Available variables that can be passed to the kubernetes API: . oidc_ca_file: \"path_to_file\" oidc_client_id: \"12345\" oidc_groups_claim: \"email\" oidc_groups_prefix: \"oidc:\" oidc_issuer_url: \"https://...\" oidc_required_claims: - 'key=value' oidc_signing_algs: \"RS256\" oidc_username_claim: \"sub\" oidc_username_prefix: \"...\" . You don’t need all variables. This is highly dependent on your oidc Provider. Please check your provider Documentation for details. Example for gitlab as oidc provider: . oidc_client_id: &lt;asdasdasdasdasdasdasdasdasd&gt; oidc_groups_claim: groups oidc_groups_prefix: 'oidc:' oidc_issuer_url: https://gitlab.address.example oidc_signing_algs: RS256 oidc_username_claim: sub oidc_username_prefix: https://gitlab.address.example# . ",
    "url": "/managedk8s/clusterlifecycle/oidc/",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/"
  },"359": {
    "doc": "OIDC",
    "title": "Requirements",
    "content": "To use the oidc login for kubernetes you need a kubectl plugin, a valid / prepared kubeconfig and RBAC permissions. ",
    "url": "/managedk8s/clusterlifecycle/oidc/#requirements",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/#requirements"
  },"360": {
    "doc": "OIDC",
    "title": "kubectl plugin",
    "content": "To handle the auth part automaticly you need an plugin for kubectl. this can be found here: int128/kubelogin . Hint: There is an kubectl plugin manager: krew this could be usefull if you handle more than one plugin. ",
    "url": "/managedk8s/clusterlifecycle/oidc/#kubectl-plugin",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/#kubectl-plugin"
  },"361": {
    "doc": "OIDC",
    "title": "kubeconf",
    "content": "The kubeconf must reflect the oidc . apiVersion: v1 kind: Config preferences: {} clusters: - cluster: certificate-authority-data: ABC= server: https://api.cluster.example:6443 name: example-cluster-0 contexts: - context: cluster: example-cluster-0 namespace: mi5 user: kubernetes-admin name: example-cluster-0 users: - name: oidc user: exec: apiVersion: client.authentication.k8s.io/v1beta1 args: - oidc-login - get-token - --oidc-issuer-url=https://gitlab.address.example - --oidc-client-id=asdasdasdasd - --oidc-client-secret=qweqweqweqweqweqweqwe command: kubectl env: null interactiveMode: IfAvailable provideClusterInfo: false . ",
    "url": "/managedk8s/clusterlifecycle/oidc/#kubeconf",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/#kubeconf"
  },"362": {
    "doc": "OIDC",
    "title": "RBAC",
    "content": "You need the proper RBAC configuration / permissions. Check out the offial documentation for this topic. Roles / ClusterRoles . You need a Role / ClusterRole to define the access of the oidc users. As an example: . apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: example-reader rules: - apiGroups: - '*' resources: - '*' verbs: # we don't want to delete with normal roles - get - list - watch - nonResourceURLs: - '*' verbs: # we don't want to delete with normal roles - get . Rolebinding / ClusterRoleBinding . You need a RoleBinding / ClusterRoleBinding to bind the role to an oidc user. for example: . kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-oidc-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: example-reader subjects: - kind: User # James Bond name: https://gitlab.address.example#007 . ",
    "url": "/managedk8s/clusterlifecycle/oidc/#rbac",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/#rbac"
  },"363": {
    "doc": "Root Disks",
    "title": "Root Disks",
    "content": "You can define disk size and disk type for the controle plane and the worker groups. Please provide this information in the following format: . # Controle plane disk_size_cp: &lt;INT&gt; volume_type_cp: &lt;vType&gt; # Worker # default disk_size: &lt;INT&gt; volume_type: &lt;vType&gt; . &lt;INT&gt; replace this with in positive integer. This is the size of the root disk in Gigabyte. &lt;vType&gt; is dependent on the used plattform. Please consult the plattform documentation for this information. | optimist | . The default value schould be default and only be changed if you know that you have different requirements. ",
    "url": "/managedk8s/clusterlifecycle/rootdisk/",
    
    "relUrl": "/managedk8s/clusterlifecycle/rootdisk/"
  },"364": {
    "doc": "Root Disks",
    "title": "Considerations",
    "content": ". | While choosing a disksize make sure you take in to account that the pulled docker images are living on this disks. So if you have a lot of big docekr images adjust your disk size accordingly. | If you use a lot of ephemeral-volumes or store / process large amounts of data adjust your disk size accordingly. | if you know you have a lot of k8s API changes consider a higher iops controle plane root disk. | If you specify a root disk it will create the corresponding disks in your tenant. | . Hint: The default volume is if nothing is specified type: default and size: 20GB . ",
    "url": "/managedk8s/clusterlifecycle/rootdisk/#considerations",
    
    "relUrl": "/managedk8s/clusterlifecycle/rootdisk/#considerations"
  },"365": {
    "doc": "Machine Deployments",
    "title": "Machine Deployments",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/"
  },"366": {
    "doc": "Machine Deployments",
    "title": "Machine Deployment Creation",
    "content": ". | We are responsible for creating machine deployments based on the your requirements. | Each machine deployment will be configured to meet specific needs such as compute capacity, storage, and availibily zone. | Please provide the name of the machineDeployment(s). If the machine deployment name is not provided, the name will default to the format clusterName-az-md (e.g., cluster-test-ix2-md). | We can enable autoscaler for you, please see (here) [/managedk8s/clusterlifecycle/autoscaling/] | We can enable pre cordoned (clusterwide) so all nodes get cordoned before rotating. Detail see Node Rotation Section. | The default machine deployment configuration is the following: ```yaml workers: | name: clusterName-az-md replicas: 3 autoscaler: min: 0 max: 0 availability_zones: . | roles: | “worker” restrictions: [] machine_type: s1.large use_custom_disk: false ``` If you need any other changes in the configuration, please mention them when requesting a new machine deployment. However, first, please review the following configuration details and limitations: | . | Flavor/Machine Type - the default type is s1.large (8 cores, 16GB RAM and 20GB disk size), you can select each flavor that you can see in your OpenStack project, but make sure the flavor: . | has at least 2 cores and 2 GB RAM | is not a windows image | . | Number of replicas the default is 3 nodes. | Availability zone (AZ) - if preferred, specify the AZ from ix1, ix2 or es1 available zones. The default value will be a random AZ. | If there are multiple AZs configured, there will be a machineDeployment created for each AZ with the specified replica count. For example if you want to use 3 replicas with AZs ix1 and ix2, there will be 2 machineDeployments each with 3 replicas. | . | Custom root disk size and/or volume type - if not enabled foe more information please see here[/managedk8s/clusterlifecycle/rootdisks] | roles are set as labels on the nodes. where the labels have the following format: node-role.kubernetes.io/&lt;ROLENAME&gt;: \"\" | restrictions are set as labels on the nodes. where the labels have the following format: node-role.kubernetes.io/&lt;RESTRICTIONNAME&gt;: \"\" | . Please be aware that we only accept this format for role and restriction: node-role.kubernetes.io/NAME: ““ . ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#machine-deployment-creation",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#machine-deployment-creation"
  },"367": {
    "doc": "Machine Deployments",
    "title": "Machine Deployment Updates",
    "content": ". | We can update the machine deployment for you, you can: . | update the flavor | update replicas | update the application credentials | enable/disable the autoscaler | enable/disable oidc | . | We can create a new machine deployment(s) for you. | Please be aware that: . | We can not update the availibity zone , as the volumes will be stack in the old availibility zone. | We can not create a machine deployment in different availibility zone, if you need multiple availibility zone you need to create multiple machinedeployment in each availibility zone. | We will regularly update the os to ensure they are running the latest software versions, patches, and security updates. | . | . ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#machine-deployment-updates",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#machine-deployment-updates"
  },"368": {
    "doc": "Machine Deployments",
    "title": "Node Rotation",
    "content": "We are rotating the nodes in your Kubernetes cluster to ensure optimal performance, security, and reliability. We are rotating the nodes of all the machinedeployments on the same time. The process is the following: . | if configured all nodes in the machinedeployment will be cordoned (only triggered by an image update) | Creating a new one(s): A new node(s) is created to replace the drained one | Draining the nodes(s): We start by safely draining each node(s) to gracefully evict all running pods. | Deleting the old node(s): Once the new node is fully operational and all workloads are running smoothly, the old node is deleted. | . This process is applied to all nodes in the cluster’s machine deployments, and it’s performed periodically or whenever needed to maintain the desired state of the cluster. Node rotation is necessary for several reasons, including: . Security Updates: Applying critical security patches and updates to the operating system. Performance Improvements: Upgrading to newer versions of the Kubernetes. Resource Optimization: Adjusting node configurations. ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#node-rotation",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#node-rotation"
  },"369": {
    "doc": "Machine Deployments",
    "title": "What You Need to Take Care Of:",
    "content": "Workload Stability: Ensure that your applications are resilient to potential disruptions. Kubernetes will attempt to reschedule your workloads automatically, but some applications may experience brief downtime or require manual intervention. Pod Disruption Budgets: Review and, if necessary, update your Pod Disruption Budgets (PDBs) to control the number of pods that can be safely evicted during the rotation without impacting application availability. By taking these steps, you can help ensure a smooth node rotation with minimal impact on your services. If you have any questions or need further assistance, please let us know. ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#what-you-need-to-take-care-of",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#what-you-need-to-take-care-of"
  },"370": {
    "doc": "Machine Deployments",
    "title": "pre cordoned",
    "content": "This optinal feature was implemented to reduce the pod restarts in an node rotation triggered by an image update. New nodes didn’t get cordoned. That results in the following behavior: Drained pods are only sheduled on new nodes. So there should be only one restart per pod per node rotation (if the pods are stable and there is no other reason a pod restart is triggered). ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#pre-cordoned",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#pre-cordoned"
  },"371": {
    "doc": "Cluster Autoscaler",
    "title": "Cluster Node Autoscaler",
    "content": "The Cluster Node Autoscaler automatically adjusts the number of nodes in a Kubernetes cluster to match the current workload. It ensures efficient resource utilization by scaling the cluster up when there is insufficient capacity to run all scheduled pods and scaling it down when there are unused nodes. ",
    "url": "/managedk8s/clusterlifecycle/autoscaling/#cluster-node-autoscaler",
    
    "relUrl": "/managedk8s/clusterlifecycle/autoscaling/#cluster-node-autoscaler"
  },"372": {
    "doc": "Cluster Autoscaler",
    "title": "How the Cluster Autoscaler Works:",
    "content": "Scaling Up: When the autoscaler detects that some pods cannot be scheduled due to insufficient resources (CPU, memory), it will automatically add new nodes to the cluster to provide the necessary capacity. Scaling Down: If the autoscaler identifies nodes that are underutilized or completely not used for a configurable period, it will remove those nodes to optimize resource usage and reduce costs. Before scaling down, it ensures that there are no critical pods running on those nodes and that workloads can be safely moved to other nodes. The Cluster Nodes Autoscaler is not enabled by default. To enable it, please contact us using the instructions provided here. When reaching out, include the following information: . | The cluster ID and Name | The Specific Machine Deployment(s) Name | Minimum Number of Worker Nodes in the specific Machine Deployment: Define the minimum number of nodes that should always be maintained in the specific Machine Deployment. | Maximum Number of Worker Nodes: Define the maximum number of nodes in the specific Machine Deployment the cluster can scale up to. | . It is possible to enable or disable the cluster-autoscaler feature anytime in one or multiple machineDeployments. Note: Ensure pods have appropriate resource requests and limits to make the autoscaling effective. ",
    "url": "/managedk8s/clusterlifecycle/autoscaling/#how-the-cluster-autoscaler-works",
    
    "relUrl": "/managedk8s/clusterlifecycle/autoscaling/#how-the-cluster-autoscaler-works"
  },"373": {
    "doc": "Cluster Autoscaler",
    "title": "Cluster Autoscaler",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/autoscaling/",
    
    "relUrl": "/managedk8s/clusterlifecycle/autoscaling/"
  },"374": {
    "doc": "Deprecation Policy",
    "title": "Deprecation Policy",
    "content": "The upstream Kubernetes project releases approximately three Kubernetes versions a year and deprecates the same number of old versions. Kubernetes follows an N-2 support policy (meaning that the 3 most recent minor versions receive security and bug fixes). A good visualization of the support period for each release is available below: . Managed Kubernetes aligns loosely to this lifecycle by continuously introducing new versions and deprecating older ones. After a given Kubernetes version has reached End-of-Life, it will not get any bugfixes or security updates. Hence, we cannot support it anymore either and have to deprecate it. ",
    "url": "/managedk8s/clusterlifecycle/deprecationpolicy/",
    
    "relUrl": "/managedk8s/clusterlifecycle/deprecationpolicy/"
  },"375": {
    "doc": "Deprecation Policy",
    "title": "Deprecation Process",
    "content": "If we decide to deprecate a specific Kubernetes version, we will notify you well in advance. The notification will include an End-of-Life announcement outlining the version’s deprecation timeline. During that time you should plan and prepare for the upgrade of your cluster to a supported Kubernetes version before the deprecated version is removed. You can find the list of supported Kubernetes versions and their planned End-of-Life dates here. What Does an End-of-Life Announcement Mean for Me? . If an End-of-Life announcement has been made for a specific Kubernetes version, we suggest customers to contact us for upgrading their clusters to a newer version, preferably the latest one. What Happens If I Do Not Update Before the EOL Date? . After the EOL period has ended: . | The EOL Kubernetes version will no longer be available in the managed service. | Any clusters still running the EOL version will be automatically upgraded to a supported version. | New clusters cannot be created with the EOL version. | . Can I Stay on an EOL Version Forever? . No, as this would possibly mean serious security issues in the future. ",
    "url": "/managedk8s/clusterlifecycle/deprecationpolicy/#deprecation-process",
    
    "relUrl": "/managedk8s/clusterlifecycle/deprecationpolicy/#deprecation-process"
  },"376": {
    "doc": "Deprecation Policy",
    "title": "Force Upgrade Policy",
    "content": "If a Kubernetes version reaches End-of-Life, we have to remove its support from Managed Kubernetes since it will not receive any bugfixes or security updates anymore. It is important to emphasize the following technical limitations in Kubernetes: . | A (control plane of a) Kubernetes cluster can be upgraded by one version at a time, e.g. from v1.30 -&gt; v1.31. | It is not possible to upgrade more than one versions at once. | It is not possible to downgrade a cluster. | . This means that if customers do not update their clusters before the removal of the next EOL version, they risk not being able to upgrade after the removal of the next deprecated version. This would imply a serious problem as their only alternative would be to create a new cluster and migrate the workload from the old one as upgrading would not be possible (since it would require upgrading two versions at once). To overcome this issue, we need to actively force customers to upgrade clusters running on an EOL Kubernetes version, before we remove the next deprecated version. What Happens to My Clusters During Force Upgrade? . We will initiate an automated Kubernetes upgrade for the control plane and the Machine Deployment(s). While this should work, it cannot be guaranteed to work, given the diversity of applications and use cases. Breaking changes in the Kubernetes API can lead to broken/incompatible applications inside the Kubernetes cluster. We can not overtake responsibility for such problems. To ensure optimal performance and security, we strongly advise all customers to keep their Kubernetes clusters up to date. Please contact us for executing your cluster upgrades. ",
    "url": "/managedk8s/clusterlifecycle/deprecationpolicy/#force-upgrade-policy",
    
    "relUrl": "/managedk8s/clusterlifecycle/deprecationpolicy/#force-upgrade-policy"
  },"377": {
    "doc": "Cluster Changes",
    "title": "How to request support for your GKSv3 cluster",
    "content": "You can find the list of the Support here . In case of requesting changes in the machineDeployment, please specify the machineDeployment where the changes should be applied. A format like md-name-az-md or cluster-name-az-mdis required so that we refer to the correct machineDeployment. We will handle the requested changes or reach out for more details if needed before applying them to your cluster. ",
    "url": "/managedk8s/clusterlifecycle/clusterchanges/#how-to-request-support-for-your-gksv3-cluster",
    
    "relUrl": "/managedk8s/clusterlifecycle/clusterchanges/#how-to-request-support-for-your-gksv3-cluster"
  },"378": {
    "doc": "Cluster Changes",
    "title": "Cluster Changes",
    "content": "We can support the following updates in a running GKSv3 cluster: . Kubernetes Version Upgrade . | Upgrade your cluster to a newer Kubernetes version. | . Kubeconfig Rotation . | Rotate the kubeconfig for security or operational needs. | . Control Plane Changes . | Update the flavor type of the control plane. | . Machine Deployment Changes . | Add new machine deployments. | Delete existing machine deployments. | Update existing machine deployments, including: . | Name | Replicas | Flavor type | . | . Please Note: We cannot migrate a volume from one Availability Zone (AZ) to another. If you plan to update the machine deployment’s AZ, be aware that this may cause issues with the volumes associated with that machine deployment. Custom Root Volume Modifications . | Add or remove custom root volumes for the control plane or worker nodes, including: . | Disk size | Volume type | . | . Cluster Autoscaler Feature . | Enable or disable the cluster-autoscaler for a specific machine deployment. | Adjust the minimum and maximum size of worker nodes to enable autoscaling. | . ",
    "url": "/managedk8s/clusterlifecycle/clusterchanges/",
    
    "relUrl": "/managedk8s/clusterlifecycle/clusterchanges/"
  },"379": {
    "doc": "Cluster Deletion",
    "title": "Cluster Deletion",
    "content": "As a customer, you are responsible for cleaning up all applications and resources, and stopping any automation running inside the cluster before requesting its deletion. Once all applications, resources, and automation processes have been deleted or stopped, you can request cluster deletion via your preferred support channel (Microsoft Teams, Slack, Email) with providing the cluster ID and name. Then, we will proceed with deleting the cluster. If any applications or resources are still running inside the cluster, we will perform a forced deletion. You will then be responsible for manually cleaning up any leftover resources in Openstack. ",
    "url": "/managedk8s/clusterlifecycle/clusterdeletion/",
    
    "relUrl": "/managedk8s/clusterlifecycle/clusterdeletion/"
  },"380": {
    "doc": "FAQ",
    "title": "Frequently Asked Questions",
    "content": " ",
    "url": "/managedk8s/faq/#frequently-asked-questions",
    
    "relUrl": "/managedk8s/faq/#frequently-asked-questions"
  },"381": {
    "doc": "FAQ",
    "title": "What’s the recommanded cluster size?",
    "content": "For high availability and fault tolerance, a common recommendation is to have: . | 3 control plane Nodes. | 3 Worker Nodes: Having at least two worker nodes per machine deployment helps distribute the workload and provides redundancy. If one worker node fails, the other can continue to run the applications, minimizing downtime. | . The recommended approach for machine deployment is to create separate machine deployments in each Availability Zones (AZs). This helps ensure that your application remains available even if one or more AZs experience failures. For example, create three machine deployments: . | Machine Deployment in ix1 | Machine Deployment in ix2 | Machine Deployment in es1 | . For each machine deployment, configure at least 2 replicas. This ensures that there is redundancy within each AZ. ",
    "url": "/managedk8s/faq/#whats-the-recommanded-cluster-size",
    
    "relUrl": "/managedk8s/faq/#whats-the-recommanded-cluster-size"
  },"382": {
    "doc": "FAQ",
    "title": "What is Cluster Autoscaler?",
    "content": "Cluster Autoscaler is a standalone program that adjusts the size of a Kubernetes cluster to meet the current needs. ",
    "url": "/managedk8s/faq/#what-is-cluster-autoscaler",
    
    "relUrl": "/managedk8s/faq/#what-is-cluster-autoscaler"
  },"383": {
    "doc": "FAQ",
    "title": "When does Cluster Autoscaler change the size of a cluster?",
    "content": "Cluster Autoscaler increases the size of the cluster when: . | there are pods that failed to schedule on any of the current nodes due to insufficient resources. | adding a node similar to the nodes currently present in the cluster would help. | . Cluster Autoscaler decreases the size of the cluster when some nodes are consistently unneeded for a significant amount of time. A node is unneeded when it has low utilization and all of its important pods can be moved elsewhere. ",
    "url": "/managedk8s/faq/#when-does-cluster-autoscaler-change-the-size-of-a-cluster",
    
    "relUrl": "/managedk8s/faq/#when-does-cluster-autoscaler-change-the-size-of-a-cluster"
  },"384": {
    "doc": "FAQ",
    "title": "Can We Reserve a Specific IP for a Kubernetes Service of Type LoadBalancer?",
    "content": "Yes, you can use the loadbalancer.openstack.org/keep-floatingip annotation to ensure that the floating IP remains associated with your project and is reused by the Kubernetes service. Here’s an example of how to configure a Kubernetes service with a reserved IP address using the annotation: . apiVersion: v1 kind: Service metadata: name: nginx-internet annotations: loadbalancer.openstack.org/keep-floatingip: \"true\" # Annotation to keep the floating IP in the project spec: type: LoadBalancer selector: app: nginx ports: - port: 80 targetPort: 80 loadBalancerIP: 45.94.08.9 # Specific floating IP to be reserved for the service . ",
    "url": "/managedk8s/faq/#can-we-reserve-a-specific-ip-for-a-kubernetes-service-of-type-loadbalancer",
    
    "relUrl": "/managedk8s/faq/#can-we-reserve-a-specific-ip-for-a-kubernetes-service-of-type-loadbalancer"
  },"385": {
    "doc": "FAQ",
    "title": "Can We Create a Kubernetes Service with a Specific Floating IP?",
    "content": "Yes, you can create a Kubernetes Service that uses a specific floating IP by setting the loadBalancerIP field in the service definition. The IP you specify as loadBalancerIP must already be allocated as a floating IP in OpenStack. Here’s an example of how you can specify a floating IP for a Kubernetes Service: . apiVersion: v1 kind: Service metadata: name: nginx-internet spec: type: LoadBalancer selector: app: nginx ports: - port: 80 targetPort: 80 loadBalancerIP: 45.94.08.9 # Specific floating IP to be reserved for the service . By using the loadBalancerIP field, you ensure that the service will use the specified floating IP when a load balancer is provisioned. ",
    "url": "/managedk8s/faq/#can-we-create-a-kubernetes-service-with-a-specific-floating-ip",
    
    "relUrl": "/managedk8s/faq/#can-we-create-a-kubernetes-service-with-a-specific-floating-ip"
  },"386": {
    "doc": "FAQ",
    "title": "Can I use external-dns and openstack designate for automatic dns?",
    "content": "Yes, a few installation and configuration steps are necessary for this which are explained here. ",
    "url": "/managedk8s/faq/#can-i-use-external-dns-and-openstack-designate-for-automatic-dns",
    
    "relUrl": "/managedk8s/faq/#can-i-use-external-dns-and-openstack-designate-for-automatic-dns"
  },"387": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/managedk8s/faq/",
    
    "relUrl": "/managedk8s/faq/"
  },"388": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Can I use external-dns and openstack designate for automatic dns?",
    "content": "Yes, to reduce manual effort and automate the configuration of DNS zones, you may want to use external-dns. In summary, external-dns allows you to control DNS records dynamically with Kubernetes resources in a DNS provider-agnostic way. external-dns is not a DNS server by itself, but merely configures other DNS providers (for example, OpenStack Designate, Amazon Route53, Google Cloud DNS, and so on.) . ",
    "url": "/managedk8s/faq/automatic_dns",
    
    "relUrl": "/managedk8s/faq/automatic_dns"
  },"389": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Prerequisites",
    "content": "To successfully complete the following steps, you need the following: . | kubectl latest version | A running Kubernetes cluster on our openstack | A valid kubeconfig for your cluster | Installed OpenStack CLI tools | OpenStack API access | A valid domain | . ",
    "url": "/managedk8s/faq/automatic_dns#prerequisites",
    
    "relUrl": "/managedk8s/faq/automatic_dns#prerequisites"
  },"390": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Configure Your domain to use designate",
    "content": "Delegate your domains from your DNS provider to the following DNS name servers so that Designate can control the DNS resources of your domain. dns1.ddns.innovo.cloud dns2.ddns.innovo.cloud . ",
    "url": "/managedk8s/faq/automatic_dns#configure-your-domain-to-use-designate",
    
    "relUrl": "/managedk8s/faq/automatic_dns#configure-your-domain-to-use-designate"
  },"391": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Create a new DNS Zone",
    "content": "Before you use ExternalDNS, you need to add your DNS zones to your DNS provider, in this case, Designate DNS. In our example we use the test domain name example.foo. It is important to create the zones before starting to control the DNS resources with Kubernetes. Note: You must include the final . at the end of the zone/domain to be created. $ openstack zone create --email webmaster@example.foo example.foo. Next, make sure that the zone was created successfully and the status is active. $ openstack zone list $ openstack zone show example.foo. ",
    "url": "/managedk8s/faq/automatic_dns#create-a-new-dns-zone",
    
    "relUrl": "/managedk8s/faq/automatic_dns#create-a-new-dns-zone"
  },"392": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Create application credentials for external-dns",
    "content": "Attention: It is imported to login with your service user into your openstack project, because Application Credentials are user specific. Visit the WebGui and go to Identity -&gt; Application Credentials, then click on the Button + CREATE APPLICATION CREDENTIAL or create credentials via cli: . $ openstack application credential create &lt;name_of_app_credentials&gt; . ",
    "url": "/managedk8s/faq/automatic_dns#create-application-credentials-for-external-dns",
    
    "relUrl": "/managedk8s/faq/automatic_dns#create-application-credentials-for-external-dns"
  },"393": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Install external-dns into your cluster",
    "content": "Note: Don’t forget to change the openstack application credentials and the command line arguments in the external-dns deployment --domain-filter=example.foo and the --txt-owner-id=&lt;owner-id&gt;. The domain-filter is the dns zone. With the txt-owner-id external-dns can identify the entries managed by itself. You should change the image to a new version if available (and shedule updates) if you want to use it in production. | Namespace: | . apiVersion: v1 kind: Namespace metadata: name: external-dns . | RBAC: | . --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns namespace: external-dns --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\"\"] resources: [\"services\", \"endpoints\", \"pods\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"extensions\", \"networking.k8s.io\"] resources: [\"ingresses\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: external-dns . | Secret: | . apiVersion: v1 kind: Secret type: Opaque metadata: name: designate-openstack-credentials namespace: external-dns stringData: OS_AUTH_URL: &lt;auth-url&gt; OS_REGION_NAME: &lt;region&gt; OS_AUTH_TYPE: v3applicationcredential OS_APPLICATION_CREDENTIAL_ID: &lt;appcred_id&gt; OS_APPLICATION_CREDENTIAL_SECRET: &lt;appcred_secret&gt; . | Deployment: | . apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: external-dns spec: selector: matchLabels: app: external-dns strategy: type: Recreate template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - args: - --source=service - --source=ingress - --registry=txt - --provider=designate - --domain-filter=example.foo - --txt-owner-id=&lt;owner-id&gt; - --log-level=debug envFrom: - secretRef: name: designate-openstack-credentials image: registry.k8s.io/external-dns/external-dns:v0.14.2 imagePullPolicy: IfNotPresent name: external-dns . ",
    "url": "/managedk8s/faq/automatic_dns#install-external-dns-into-your-cluster",
    
    "relUrl": "/managedk8s/faq/automatic_dns#install-external-dns-into-your-cluster"
  },"394": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Annotate the service or ingress",
    "content": "To add the dns entry to designate the service or ingress needs to be annotated with external-dns.alpha.kubernetes.io/hostname: my-app.example.foo. For example: . apiVersion: v1 kind: Namespace metadata: name: my-app . apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: my-app spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ports: - containerPort: 80 . apiVersion: v1 kind: Service metadata: name: nginx namespace: my-app annotations: external-dns.alpha.kubernetes.io/hostname: my-app.example.foo spec: selector: app: nginx type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 . ",
    "url": "/managedk8s/faq/automatic_dns#annotate-the-service-or-ingress",
    
    "relUrl": "/managedk8s/faq/automatic_dns#annotate-the-service-or-ingress"
  },"395": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "DNS Lookup",
    "content": "Make the dns record will be looked up correct: . $ openstack recordset list example.foo. $ dig my-app.example.foo @dns1.ddns.innovo.cloud. $ dig my-app.example.foo @dns2.ddns.innovo.cloud. $ dig my-app.example.foo . ",
    "url": "/managedk8s/faq/automatic_dns#dns-lookup",
    
    "relUrl": "/managedk8s/faq/automatic_dns#dns-lookup"
  },"396": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Further information",
    "content": "For further information, please have a look at: . | kubernetes-sigs.github.io/external-dns/latest/docs/tutorials/designate | github.com/kubernetes-sigs/external-dns | docs.openstack.org/python-designateclient | . ",
    "url": "/managedk8s/faq/automatic_dns#further-information",
    
    "relUrl": "/managedk8s/faq/automatic_dns#further-information"
  },"397": {
    "doc": "Logging and Metrics",
    "title": "User Guide for Logging and Metrics",
    "content": "This document provides an overview of logging and metrics capabilities in our Managed Kubernetes Service. Please note that we only monitor the control plane, we do not collect or store any user application data or user-generated logs from within customer-deployed workloads. Our focus is exclusively on metrics and logs related to the cluster control plane’s health, performance, and operational efficiency. Please note that you can access all logs/metrics endpoints even from the control plane. Customers are responsible for managing their application logs, pod metrics, and data generated within their workloads. This includes: . | Collecting Application Logs: Customer may need to configure their logging solutions (e.g., Fluentd, Filebeat) to gather logs from applications and store them in a preferred location. | Monitoring Application Metrics: For custom application metrics, customer can set up their own Prometheus or other monitoring solutions. | . We recommend the following practices for managing application logs and metrics independently: . | Deploy Logging Solutions: Configure logging agents (e.g., Fluentd, Logstash) on each node to collect logs from your applications and forward them to a centralized logging service of your choice (e.g., Elasticsearch, Loki), we recommand to ship the data to our central logging system Elastic . | Set Up Prometheus and Grafana: For a complete metrics solution, deploy Prometheus and Grafana to monitor and visualize resource usage, custom metrics, and application performance. | . ",
    "url": "/managedk8s/faq/logging_metrics/#user-guide-for-logging-and-metrics",
    
    "relUrl": "/managedk8s/faq/logging_metrics/#user-guide-for-logging-and-metrics"
  },"398": {
    "doc": "Logging and Metrics",
    "title": "Logging and Metrics",
    "content": " ",
    "url": "/managedk8s/faq/logging_metrics/",
    
    "relUrl": "/managedk8s/faq/logging_metrics/"
  },"399": {
    "doc": "StorageClass Setup",
    "title": "StorageClass Setup",
    "content": "We provide one default storage class per Cluster. Caution: This is managed by WIIT and can be overwritten at any time. Please create a separate storage class for your changes. kubectl get storageclasses.storage.k8s.io NAME PROVISIONER AGE cinder-csi (default) cinder.csi.openstack.org 6h45m . ",
    "url": "/managedk8s/storageclasses/",
    
    "relUrl": "/managedk8s/storageclasses/"
  },"400": {
    "doc": "StorageClass Setup",
    "title": "Openstack Volume Types",
    "content": "The Openstack volume types sorted by maximum possible IOPS: . | low-iops | default &lt;- used in the default class | high-iops | . ",
    "url": "/managedk8s/storageclasses/#openstack-volume-types",
    
    "relUrl": "/managedk8s/storageclasses/#openstack-volume-types"
  },"401": {
    "doc": "StorageClass Setup",
    "title": "Volume Features",
    "content": "We don’t provide Read-Write-Many Volumes. All Volumes are Read-Write-Once! . ",
    "url": "/managedk8s/storageclasses/#volume-features",
    
    "relUrl": "/managedk8s/storageclasses/#volume-features"
  },"402": {
    "doc": "StorageClass Setup",
    "title": "Adding Your Own Classes",
    "content": "If you need use one of the other types, you can add your own definitions. Example: . apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-high-iops-class provisioner: cinder.csi.openstack.org parameters: type: high-iops . Apply with kubectl apply -f storage-class.yaml. | name: Choose a unique one, as we don’t want to interfere with the default names. | provisioner: Use the one of your cluster. You can always have a look in the default class to verify the right provider. | type: Use one of the official provided types from the Optimist platform (at the time of writing low-iops and high-iops). | . To use the new storage class you need to change your volumes definitions and add the new StorageClass name. ",
    "url": "/managedk8s/storageclasses/#adding-your-own-classes",
    
    "relUrl": "/managedk8s/storageclasses/#adding-your-own-classes"
  }
}
