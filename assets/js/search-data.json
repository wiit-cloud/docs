{"0": {
    "doc": "Not found",
    "title": "404 Not Found",
    "content": "We cannot find the page you are looking for! . Click here to go back home . ",
    "url": "/404.html#404-not-found",
    
    "relUrl": "/404.html#404-not-found"
  },"1": {
    "doc": "Not found",
    "title": "Not found",
    "content": " ",
    "url": "/404.html",
    
    "relUrl": "/404.html"
  },"2": {
    "doc": "Welcome!",
    "title": "Welcome!",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"3": {
    "doc": "Introduction",
    "title": "Introduction",
    "content": "Elastic Cloud Enterprise by WIIT (ECE) is a cloud platform hosted and operated by WIIT, on which you can run your Elastic workloads in the form of deployments. A deployment is an isolated Elastic Cluster managed by ECE. It consists of several components of the Elastic Stack. For each deployment you can specify the following individually: . | Which of the Elastic Stack components below to use | The size of each component | The number of Availability Zones (1 through 3) over which to distribute them | Which of the WIIT provided versions for the Elastic Stack to use | . Each deployment gets its own publicly accessible URLs for Elasticsearch, Kibana, Fleet, and Enterprise Search. ",
    "url": "/ece/intro",
    
    "relUrl": "/ece/intro"
  },"4": {
    "doc": "Introduction",
    "title": "Components of the Elastic Stack",
    "content": "ECE includes all server components currently offered by Elastic: . | Elasticsearch in the 4 tiers Hot, Warm, Cold and Frozen as well as master nodes and coordinating/ingest nodes | Kibana | Machine Learning | Integration Server (Fleet and APM) | Enterprise Search | . Information on the Elasticsearch data tiers is available here: https://www.elastic.co/guide/en/elasticsearch/reference/current/data-tiers.html. ",
    "url": "/ece/intro#components-of-the-elastic-stack",
    
    "relUrl": "/ece/intro#components-of-the-elastic-stack"
  },"5": {
    "doc": "Introduction",
    "title": "Functional Scope of the Components",
    "content": "All ECE deployments are equipped with the Elastic Enterprise license, which means that all components contain all the functionalities of the highest-level Elastic license. This includes machine learning, searchable snapshots in the frozen tier (a very cost-efficient variant for storing rarely accessed data), cross-cluster search, all SIEM (Security Information and Event Management) functionalities of the Elastic Stack, Watcher and Alerting, the brand-new AI Assistant and much more. You can find an overview of the components here: https://www.elastic.co/de/pricing/, and a detailed list here: https://www.elastic.co/de/subscriptions. ",
    "url": "/ece/intro#functional-scope-of-the-components",
    
    "relUrl": "/ece/intro#functional-scope-of-the-components"
  },"6": {
    "doc": "Introduction",
    "title": "Calculation of Deployment Size and Pricing Model",
    "content": "The deployment size is measured in GB RAM. Each Elastic component is also allocated CPU and storage, derived from the RAM size by certain factors. This mapping factor can be different for each component. In most use cases, RAM is the main determining factor for the performance of elastic workloads. Since the mapping is based on fixed factors, one parameter (GB RAM) is sufficient to calculate the other parameters (CPU, storage). This makes the pricing model very transparent and easy to understand. All you need to know is the RAM size of the deployments. Your cost is based on GB of RAM used in your deployment. Consumption is recorded and billed to the hour. The snapshot storage usage in our object storage cluster is billed separately. Internet traffic (in and out) is free of charge. ",
    "url": "/ece/intro#calculation-of-deployment-size-and-pricing-model",
    
    "relUrl": "/ece/intro#calculation-of-deployment-size-and-pricing-model"
  },"7": {
    "doc": "Introduction",
    "title": "Autoscaling Feature",
    "content": "The Elasticsearch and machine learning components come with autoscaling enabled. You don’t have this feature with a self-hosted Elastic Stack or you would have to develop it yourself. With autoscaling, ECE monitors the disk space used by the Elasticsearch components and automatically adds more resources when a threshold is exceeded. As a result, you do not have to worry about the otherwise very common case of a deployment outage due to lack of disk space, even when your data volume grows. Machine learning workloads are initiated at the proper size only when actually needed. The resources for your deployment and thus your costs grow with your data volume. This enables efficient use of resources and cost-effective operation of your workloads on the ECE platform. For each component with autoscaling, an upper limit for scaling can be defined. This way, your invoice amount does not get out of control, e.g. in case of a misconfiguration. ",
    "url": "/ece/intro#autoscaling-feature",
    
    "relUrl": "/ece/intro#autoscaling-feature"
  },"8": {
    "doc": "Introduction",
    "title": "Your Advantages with ECE from WIIT",
    "content": "With the ECE solution from WIIT you get all the functionalities of the software from Elastic, the market leader in search, SIEM and observability, including the enterprise license. WIIT hosts exclusively in highly secure German data centers and carries out all operational tasks from Germany. You get everything from one source from a German contractual partner. In contrast to a “classic” deployment of the Elastic Stack in a cluster with virtual machines, which are limited to the sizes specified by the respective cloud platform, ECE uses Docker containers. These can scale automatically and quickly, if necessary. Fine-grained autoscaling allows you to use resources efficiently and run workloads cost-effectively. ",
    "url": "/ece/intro#your-advantages-with-ece-from-wiit",
    
    "relUrl": "/ece/intro#your-advantages-with-ece-from-wiit"
  },"9": {
    "doc": "Introduction",
    "title": "Services Offered by WIIT",
    "content": ". | WIIT provides the cloud environment on which your Elastic workloads run in its secure and certified data centers in Germany. | WIIT allows you to distribute your workloads across 3 Availability Zones for high availability. | WIIT takes care that enough resources are available, in case your deployment needs to scale up. | WIIT monitors the ECE environment and ensures that it is available (99.85 %). Outside office hours, an on-call service is automatically alerted to solve unforeseen problems with the ECE platform as quickly as possible. | WIIT ensures regular updates and security patches for the servers. | WIIT makes new versions of the Elastic Stack available for installation, typically only a few days after Elastic releases them. However, we do not perform automatic or unsolicited version updates of your deployment (except for EOL versions). | WIIT provides URLs that can be reached from the Internet for all deployments, including a TLS certificate for encrypted communication (customer-specific certificates are unfortunately not possible). | WIIT offers backup and snapshot storage in the redundant WIIT Object Storage Cluster. By default, each deployment has a snapshot policy enabled (which you can change) that automatically backs up your Elasticsearch data to the object storage at regular intervals. This protects you very well against data loss. | All your deployments in the WIIT ECE cloud come with the Elastic Enterprise license. You don’t have to buy licenses from Elastic. WIIT provides you with everything from a single source. | . ",
    "url": "/ece/intro#services-offered-by-wiit",
    
    "relUrl": "/ece/intro#services-offered-by-wiit"
  },"10": {
    "doc": "Introduction",
    "title": "Your Responsibilities as a Customer",
    "content": "As a customer, you are responsible for everything that happens within your deployment, in particular: . | Collecting data from client systems and loading that data into Elasticsearch | The configuration of Lifecycle Policies | The creation of users and the assignment of authorizations | Monitoring shard health and repairing it if necessary (for more information, see the elastic.co documentation) | Configuration of alerts - if desired | Restore snapshots when needed | Monitoring the performance of your deployment if you have special requirements for this. We cannot know which performance is acceptable for your application or use case. | . More provisions can be found in the General Terms and Conditions of WIIT. ",
    "url": "/ece/intro#your-responsibilities-as-a-customer",
    
    "relUrl": "/ece/intro#your-responsibilities-as-a-customer"
  },"11": {
    "doc": "Introduction",
    "title": "Service Description",
    "content": "For more information, please contact Sales. ",
    "url": "/ece/intro#service-description",
    
    "relUrl": "/ece/intro#service-description"
  },"12": {
    "doc": "Introduction",
    "title": "Steps To Be Performed After the ECE Deployment",
    "content": "We recommend performing the following steps to increase the security of your ECE deployment and load data into Elastic: . Increasing Security . | Create the required spaces, roles and users in Kibana. For more information, see: https://www.elastic.co/guide/en/kibana/current/tutorial-secure-access-to-kibana.html. | Do not use the elastic superuser for daily work in Kibana or to deliver data. | If desired, configure Single Sign-On using SAML, LDAP, Active Directory, OpenID Connect or Kerberos as well as dynamic role mappings, if you need role-based or attribute-based access control. For the time being, to set those parameters in your deployment, contact our support. | If desired, have our support configure traffic filters. This prevents unwanted access to your deployment. | Create service accounts for data delivery (when not using Agent and Fleet, cf. below) and generate API keys for them. For more information see: Grant access using API keys. | . Loading Data into Elastic . | Load data into your cluster. We recommend using the Elastic Agent in cooperation with the Fleet Server. You can find detailed instructions here: Ship Data to ECE. | To avoid connection problems to your cluster, make sure to disable sniffing in the Elasticsearch clients used (e.g. fluentd). Elastic Agent, Filebeat etc. automatically behave correctly. | If you want to migrate data from your existing Elastic cluster to your new deployment, you can find some methods in the Elastic documentation: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-migrating-data.html. | In Kibana, review your index templates and lifecycle policies so you can assign your data to your desired Hot, Warm, Cold, Frozen (and delete) tiers. The lifecycle policies automatically created by Elastic often only have one hot tier configured without an expiry date (see: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html). This applies in particular to the automatically deployed lifecycle policies logs and metrics, which are the most relevant policies when using the Elastic Agent. For an example of configuring a policy that deletes data after 70 days, see: Update the “Default” ILM. | . ",
    "url": "/ece/intro#steps-to-be-performed-after-the-ece-deployment",
    
    "relUrl": "/ece/intro#steps-to-be-performed-after-the-ece-deployment"
  },"13": {
    "doc": "Introduction",
    "title": "Support Services by WIIT",
    "content": "For support requests, you can contact our support team, which is available on workdays from 8 a.m. to 6 p.m. The WIIT Support can assist you with all topics that are WIIT’s responsibility within the scope of the ECE service: . | Availability and accessibility of the platform | Questions about using ECE | Bugs in the Elastic software (we would forward these to Elastic accordingly) | . While we are working on providing you with a self-service interface for managing your ECE deployment in the future, you must initially request the following settings for your deployment from our support. During this time, the services are also included in the scope of support: . | Extended configuration of the deployment (i.e. adjustments to the elasticsearch.yml or kibana.yml) | Entries in the Elasticsearch keystore | Update of the Elastic version of your deployment | Configuration of traffic filters for your deployment | Configuration of trust relationships between deployments so you can use cross-cluster search or cross cluster replication | . The following is not covered by WIIT Support: . | General consulting about the Elastic software or using it in your deployment | . If applicable, upon request we may offer some of the uncovered services as paid consulting services. Please contact Sales, if needed. Likewise, we cannot offer 24/7 on-call service for customers. This should also not be necessary, since the proper functionality of the ECE platform is controlled by automated monitoring. If a restriction of the service is noticed, our internal on-call service is automatically alerted and starts rectifying the issue. ",
    "url": "/ece/intro#support-services-by-wiit",
    
    "relUrl": "/ece/intro#support-services-by-wiit"
  },"14": {
    "doc": "Shipping data to Elastic",
    "title": "Shipping data to Elastic",
    "content": "Once your deployment is created, you might want to ship data to it from your log sources with Elastic Agent and Fleet. This document guides you through the process. ",
    "url": "/ece/shipdata/",
    
    "relUrl": "/ece/shipdata/"
  },"15": {
    "doc": "Shipping data to Elastic",
    "title": "Definition of Terms",
    "content": "Before we start, let’s define some terms you will encounter throughout this document. | TERM | DEFINITION | . | Elastic Agent | The log collection component that will be installed client-side on log sources. It is the only component to install since it brings all the beats and manages them and their configurations on the client. | . | Fleet Server | A component in your deployment that manages all agents downstream. All settings in Fleet are made in Kibana. | . | Integration | In this context, it is a pre-configured (mostly provided by Elastic) set of agent settings, ingest pipelines and Kibana dashboards for a specific type of log source (e.g. Tomcat, nginx, MySQL, Cisco Equipment, …). Integrations are readily available for most common log sources (see: https://www.elastic.co/integrations/data-integrations) | . | Agent Policy | A collection of agent settings and integrations (in the form of a policy) that tells the agent and the underlying beats what data to collect and from where. You need a separate agent policy for each type of log source. When, for example, all your MySQL Databases store their logs in the same local path (which they should anyway), one agent policy for MySQL databases is sufficient. An agent policy can contain multiple integrations and thus an agent can collect logs from multiple applications on the same log source. You cannot have more than one policy for one agent. Let’s assume that on some hosts you only have Apache httpd installed, while on other hosts you have httpd and Tomcat installed. Then you need (at least) two agent policies: one for httpd and one for httpd+tomcat. | . | Enrollment Token | Is required by the agent when starting on the client node. It serves two purposes: First, it provides the initial authentication of the agent to the Fleet and Elastic servers. Second, it references exactly one agent policy, so that Fleet server can tell the agent which log sources to collect data from. Each agent policy has at least one enrollment token. Enrollment tokens are sensitive data by nature and should therefore be stored securely. | . ",
    "url": "/ece/shipdata/#definition-of-terms",
    
    "relUrl": "/ece/shipdata/#definition-of-terms"
  },"16": {
    "doc": "Shipping data to Elastic",
    "title": "Create an Agent Policy",
    "content": "As the agent policy defines for the agent which logs to collect, it must be defined first. Define one agent policy per type of log source. In Kibana, go to Fleet → Agent Policies. Create a new agent policy and give it a meaningful name (it should not be necessary to change the Advanced options). Then click on your new integration, and you will see that one integration is already pre-configured: the system integration, which collects system logs and metrics from the client machine itself. Linux, Windows or macOS? No need to worry, the agent will figure this out and configure correctly. Depending on the type of log source you want to collect data from, add other integrations, e.g. here for Apache products: . By clicking on the tile, you will not only get an overview of the integration’s features and components. After clicking on Add Integration, you can also customize its settings on the following page. The content of that page of course depends a lot on the type of Integration. However, if - for example - you store your MySQL Logs in a different path than the default, you can change that here. It will probably look something like this: . ",
    "url": "/ece/shipdata/#create-an-agent-policy",
    
    "relUrl": "/ece/shipdata/#create-an-agent-policy"
  },"17": {
    "doc": "Shipping data to Elastic",
    "title": "Get the Enrollment Token",
    "content": "For each agent policy, an enrollment token named Default is automatically created. It can be retrieved from Kibana anytime. ",
    "url": "/ece/shipdata/#get-the-enrollment-token",
    
    "relUrl": "/ece/shipdata/#get-the-enrollment-token"
  },"18": {
    "doc": "Shipping data to Elastic",
    "title": "Install the Agent on your Client Node",
    "content": "A wizard in Kibana guides you through the process of installing the agent on the client. For the first rollout of such type, start the installation from Kibana → Fleet → Agents → Add Agent. For more agents using the same policy, you can also automate this, as the installation commands and parameters are identical. Select the correct agent policy for the client you want to deploy. Keep Enroll in Fleet checked. Then, Kibana will present you with different options for the commands to run on the client machine: . The URL Parameter is automatically set to the deployment you are currently in, and the enrollment token points to the agent policy. Once these commands are executed on the client machine, the agent registers itself at the Fleet server and appears in the agent list in Kibana. All subsequent deployments of agents with the same agent policy have the same set of commands and parameters. You don’t have to go through the wizard again. This means you can automate this part in the tool of your choice. ",
    "url": "/ece/shipdata/#install-the-agent-on-your-client-node",
    
    "relUrl": "/ece/shipdata/#install-the-agent-on-your-client-node"
  },"19": {
    "doc": "Shipping data to Elastic",
    "title": "Post-Installation Tasks",
    "content": "Make sure to check the Lifecycle Policies (ILM), including all those deployed by the system like logs and metrics. By default, they are deployed with infinite lifetime. We recommend to change that. Discover the fancy new dashboards that come with each additional integration. Familiarize yourself with the Fleet Menu in Kibana. Check for possible updates of agent versions or integrations. All of this can be done through Fleet in Kibana, you don’t have to log in on the client computers. ",
    "url": "/ece/shipdata/#post-installation-tasks",
    
    "relUrl": "/ece/shipdata/#post-installation-tasks"
  },"20": {
    "doc": "Changing Lifecycle Policies",
    "title": "Changing (default) Lifecycle Policies",
    "content": "This little guide will show you how to adapt the standard Index Lifecycle Policies to your specific needs. | Definition of Terms | Getting the Current ILM Policy | Creating a New ILM Policy . | Using the UI . | Hot Phase | Warm Phase | Cold Phase | Frozen Phase | Delete Phase | . | Using Dev Tools | . | Assigning the Policy to an Index | Rollover the Data Stream | . ",
    "url": "/ece/updateilm/#changing-default-lifecycle-policies",
    
    "relUrl": "/ece/updateilm/#changing-default-lifecycle-policies"
  },"21": {
    "doc": "Changing Lifecycle Policies",
    "title": "Definition of Terms",
    "content": "Before we start, let’s define some terms you will encounter throughout this guide. | Term | Definition | . | ILM | Abbreviation for Index Lifecycle Management. Determines the lifecycle of data and its allocation to the Elastic Data tiers. It is defined in Index Lifecycle Policies. | . | Data stream | A collection of single indices. It could be viewed as a small DNS (Domain Name System) for index routing requests. A data stream can contain multiple indices. | . | Rollover | Process of (automatically) creating a new index within a data stream in the Hot Phase so a single index does not grow indefinitely. It is defined in the Hot Phase of the Index Lifecycle Policy. | . ",
    "url": "/ece/updateilm/#definition-of-terms",
    
    "relUrl": "/ece/updateilm/#definition-of-terms"
  },"22": {
    "doc": "Changing Lifecycle Policies",
    "title": "Getting the Current ILM Policy",
    "content": "When using the Elastic Agent and Fleet, default policies are automatically applied to the data streams. These policies are kept simple and contain only a few phases, often only the hot phase. The policy applied to the data streams can be found in Kibana. | Go to Kibana. | Open the menu and go to Stack Management. | Open Index Management and go to Data Streams. | On the popup on the right you can see the assigned ILM. | . ",
    "url": "/ece/updateilm/#getting-the-current-ilm-policy",
    
    "relUrl": "/ece/updateilm/#getting-the-current-ilm-policy"
  },"23": {
    "doc": "Changing Lifecycle Policies",
    "title": "Creating a New ILM Policy",
    "content": "Using the UI . | To create a new ILM, select Stack Management. | Select Index Lifecycle Management and click Create Policy. | Provide a new name for the policy. | . Hot Phase . The hot phase stores all data that is newly indexed and data that is accessed very frequently. It is always the first phase and thus mandatory. In our example, we have deactivated the setting “Use recommended defaults” to reduce the maximum age of data before rollover from the default 30 days to 7 days. Warm Phase . After the data has been collected in the index for 7 days or the index reaches a size of 50 GB, we want the index to immediately transition to the second phase, called the warm phase. In this tier we lower the priority of the indices and set them to read-only. Cold Phase . After 7 days in the warm phase, data moves to the next phase, called the cold phase. In this phase, we enable searchable snapshots to reduce the storage footprint on the elastic instances. The data is now a searchable snapshot stored in the Object Storage which is already highly available itself. Therefore, we can set the number of replicas and the priority to zero. Frozen Phase . In this phase, actual data is removed from the Elasticsearch nodes and only a small cache remains. When the data is accessed, it is retrieved from the snapshot to make it accessible. Delete Phase . This phase removes indices that have reached a certain age. Using Dev Tools . A new ILM Policy can also be created in the Kibana Dev Tools. The console can be found at Management → Dev Tools. PUT _ilm/policy/&lt;add-some-name-here&gt; { \"policy\": { \"phases\": { \"hot\": { \"min_age\": \"0ms\", \"actions\": { \"rollover\": { \"max_primary_shard_size\": \"50gb\", \"max_age\": \"7d\" }, \"set_priority\": { \"priority\": 100 } } }, \"warm\": { \"min_age\": \"0d\", \"actions\": { \"set_priority\": { \"priority\": 50 }, \"readonly\": {} } }, \"cold\": { \"min_age\": \"7d\", \"actions\": { \"readonly\": {}, \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true }, \"set_priority\": { \"priority\": 0 }, \"allocate\": { \"number_of_replicas\": 0 } } }, \"frozen\": { \"min_age\": \"14d\", \"actions\": { \"searchable_snapshot\": { \"snapshot_repository\": \"found-snapshots\", \"force_merge_index\": true } } }, \"delete\": { \"min_age\": \"70d\", \"actions\": { \"delete\": { \"delete_searchable_snapshot\": true } } } } } } . ",
    "url": "/ece/updateilm/#creating-a-new-ilm-policy",
    
    "relUrl": "/ece/updateilm/#creating-a-new-ilm-policy"
  },"24": {
    "doc": "Changing Lifecycle Policies",
    "title": "Assigning the Policy to an Index",
    "content": "The newly created policy can then be assigned to an Index Template from the Index Lifecycle Policies menu. ",
    "url": "/ece/updateilm/#assigning-the-policy-to-an-index",
    
    "relUrl": "/ece/updateilm/#assigning-the-policy-to-an-index"
  },"25": {
    "doc": "Changing Lifecycle Policies",
    "title": "Rollover the Data Stream",
    "content": "In order to use the new ILM Policy immediately, we need to trigger a rollover for the data stream. This can be done in the Dev Tools, here for the data stream auditbeat-8.5.0: . POST auditbeat-8.5.0/_rollover . ",
    "url": "/ece/updateilm/#rollover-the-data-stream",
    
    "relUrl": "/ece/updateilm/#rollover-the-data-stream"
  },"26": {
    "doc": "Changing Lifecycle Policies",
    "title": "Changing Lifecycle Policies",
    "content": " ",
    "url": "/ece/updateilm/",
    
    "relUrl": "/ece/updateilm/"
  },"27": {
    "doc": "Product Overview",
    "title": "Product Overview",
    "content": "The ONCITE Open Edition provides an easy-to-use VM and a container platform based on Open Source. It is scalable from the size of an Edge to the size of a data center and offers full data sovereignty and real-time capabilities. ",
    "url": "/edge/productoverview/",
    
    "relUrl": "/edge/productoverview/"
  },"28": {
    "doc": "Product Overview",
    "title": "Architecture",
    "content": "For providing Virtual Machines on the Edge we use Openstack. The provided Storage is handled by CEPH. To create and manage the VMs the Customer can use the OperationsCenter, which is connected via LDAP for SingleSignOn. Administrators can use the Openstack Horizon, which is not connected to LDAP. ",
    "url": "/edge/productoverview/#architecture",
    
    "relUrl": "/edge/productoverview/#architecture"
  },"29": {
    "doc": "Operations Center",
    "title": "Operations Center",
    "content": " ",
    "url": "/edge/operationscenter/",
    
    "relUrl": "/edge/operationscenter/"
  },"30": {
    "doc": "Release Notes",
    "title": "Release Notes",
    "content": " ",
    "url": "/edge/operationscenter/release_notes/",
    
    "relUrl": "/edge/operationscenter/release_notes/"
  },"31": {
    "doc": "Release Notes",
    "title": "1.5.14",
    "content": "Improvements: . | VPNaaS: Improved monitoring of Server and Gateway VMs | . ",
    "url": "/edge/operationscenter/release_notes/#1514",
    
    "relUrl": "/edge/operationscenter/release_notes/#1514"
  },"32": {
    "doc": "Release Notes",
    "title": "1.5.13",
    "content": "Fixes: . | limit project names to 25 chars (previously: 50) | . ",
    "url": "/edge/operationscenter/release_notes/#1513",
    
    "relUrl": "/edge/operationscenter/release_notes/#1513"
  },"33": {
    "doc": "Release Notes",
    "title": "1.5.11",
    "content": "Fixes: . | Backups are no longer blocked because of old VM with whitespace as the start of the name | Limiting of the Projectname is now working as intended | . ",
    "url": "/edge/operationscenter/release_notes/#1511",
    
    "relUrl": "/edge/operationscenter/release_notes/#1511"
  },"34": {
    "doc": "Release Notes",
    "title": "1.5.9",
    "content": "Improvements: . | Updates to the backend Ingress | . ",
    "url": "/edge/operationscenter/release_notes/#159",
    
    "relUrl": "/edge/operationscenter/release_notes/#159"
  },"35": {
    "doc": "Release Notes",
    "title": "1.5.6",
    "content": "Changes Resource Overview: . Fixes: . | Does not truncate ports of VMs in the tooltip anymore. Shows all ports in the tooltip now | . Features: . | Shows internal ip additionally to floating ip | . ",
    "url": "/edge/operationscenter/release_notes/#156",
    
    "relUrl": "/edge/operationscenter/release_notes/#156"
  },"36": {
    "doc": "Release Notes",
    "title": "1.5.4",
    "content": "Features: . | Flavors can now be set to not be visible in Operations Center again. To set a flavor to invisible update the flavor metadata in openstack and add the custom visibility, set it to false and save the change. The flavor is now not visible in Operations Center any more. Note: The flavor is just hidden from the selection when creating a new VM. VMs using that flavor already are not impacted and can still be managed in Operations Center | . ",
    "url": "/edge/operationscenter/release_notes/#154",
    
    "relUrl": "/edge/operationscenter/release_notes/#154"
  },"37": {
    "doc": "Release Notes",
    "title": "1.5.3",
    "content": "Fixes: . | VPNaaS revoked certs were not correctly appended to the CRL Features: . | Added password generator and info tooltip for password Improvements: . | External user creation blocked for emails ending with fraunhofer.de and added tooltip for info about that | VPNaas certificates validity increased to 10 years | Limit project name length to prevent VPNaaS issues | Updated VPNaaS Version to support newer OpenVPN clients | Pressing the refresh button on the resource Overview displays the IP and status information | Improved Backup performance | . ",
    "url": "/edge/operationscenter/release_notes/#153",
    
    "relUrl": "/edge/operationscenter/release_notes/#153"
  },"38": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/edge/operationscenter/vpnaas/",
    
    "relUrl": "/edge/operationscenter/vpnaas/"
  },"39": {
    "doc": "VPN as a Service",
    "title": "Overview",
    "content": "Here you see how the customer OpenStack projects and the Operations Center communicate via the OpenStack Project VPN Gateway. The OpenStack Project VPN Gateway has one public IP address. It autenticates and leads the traffic by the different ports to the respective VPN server. Usually, each project has one VPN server. In this configuration, each VPN Gateway has one public IP adress that is valid for all VPN servers. ",
    "url": "/edge/operationscenter/vpnaas/#overview",
    
    "relUrl": "/edge/operationscenter/vpnaas/#overview"
  },"40": {
    "doc": "VPN as a Service",
    "title": "Purpose",
    "content": "GEC provides a VPNaaS solution which allows the customer to integrate their applications and systems with project partner systems. An external partner is either a person accessing the project with his or her computer, or a communication system. ",
    "url": "/edge/operationscenter/vpnaas/#purpose",
    
    "relUrl": "/edge/operationscenter/vpnaas/#purpose"
  },"41": {
    "doc": "VPN as a Service",
    "title": "Requirements",
    "content": "The VPNaaS solution must meet the following requirement to the system and the customer: . | The Gateway needs a publicly reachable floating IP. | . ",
    "url": "/edge/operationscenter/vpnaas/#requirements",
    
    "relUrl": "/edge/operationscenter/vpnaas/#requirements"
  },"42": {
    "doc": "VPN as a Service",
    "title": "Limitations",
    "content": "The VPNaaS solution has the following limitations: . | The amount of VPN server counts is limited by the preconfigured port range in the gateway. | The amount of users in a network is limited to 250. | The gateway and server configuration cannot be changed after creation and have to be recreated. | After recreation, all OVPN configurations are invalid. | . ",
    "url": "/edge/operationscenter/vpnaas/#limitations",
    
    "relUrl": "/edge/operationscenter/vpnaas/#limitations"
  },"43": {
    "doc": "VPN as a Service",
    "title": "Components and Communication Flow",
    "content": "Here you see the communication flow between the Operations Center and the Customer VPN Servers. The customer sends a request, and the VPN gateway authenticates and distributes the requests to the respective VPN server. VPN Gateway . The VPN gateway is the GEC central management hub. It is used to manage VPN servers and allows external connections. Since only one public IP address is available, IPtable rules are used to make sure that the connections arrive at the correct VPN server. This requires a Wide Area Network (WAN) network and a VPN Transfer Network connected to the VPN Gateway. The VPN Gateway only manages vpn servers and proxy connections to and from the VPN servers, so traffic only ends up where it is needed. VPN Server . The VPN Server uses OpenVPN for the VPN connection to the customer network. That network connection is in a different Virtual Routing and Forwarding (VRF) to provide additional isolation without opening any other ports. Outside connections are routed via the VPN Gateway. The API Server for the VPN Server manages the OpenVPN Server that is running on the machine. ",
    "url": "/edge/operationscenter/vpnaas/#components-and-communication-flow",
    
    "relUrl": "/edge/operationscenter/vpnaas/#components-and-communication-flow"
  },"44": {
    "doc": "VPN as a Service",
    "title": "Security",
    "content": "WAN . | Only ports that are required for the existing VPN Servers are open | SSH is not used | Virtual Routing and Forwarding (VRF) separation is available | Security Groups only allow the VPN Server port ranges | FW rules are in place and adjusted as needed | . VPNaaS . | Separate VRF to WAN | Firewall rules are in place and adjusted as needed | SSH and API are available | SSH-Key is used | . ",
    "url": "/edge/operationscenter/vpnaas/#security",
    
    "relUrl": "/edge/operationscenter/vpnaas/#security"
  },"45": {
    "doc": "VPN as a Service",
    "title": "Server and User Management",
    "content": "Server Status . | Click on the VPN resource. | Under Properties, check the status. To be able to edit the configuration, the status has to be active. | . Add a User . Note: The project owner is automatically added for remote access and cannot be removed. Prerequisites: You can only select users who have previously been added to the project. | Go to the VPN configuration. | Select the user you want to add as VPN User. | Click Save. The user is added immediately. | . Get the OVPN Configuration and Passphrase . Note: You can only download the OVPN configuration and reveal a passphrase for yourself or as project owner for external users. You cannot download the OVPN configuration or reveal passphrases for other users. | Go to the VPN configuration. | Click on the world icon link and choose between Download and Passphrase. | . Delete a User . | Click on the Delete icon next to the user you want to delete. | Click Save. | . ",
    "url": "/edge/operationscenter/vpnaas/#server-and-user-management",
    
    "relUrl": "/edge/operationscenter/vpnaas/#server-and-user-management"
  },"46": {
    "doc": "Openstack",
    "title": "Openstack",
    "content": "OpenStack is the software used for the Infrastructure as a Service layer. All VMs are created in OpenStack, which is directly connected to CEPH for providing storage. ",
    "url": "/edge/openstack/",
    
    "relUrl": "/edge/openstack/"
  },"47": {
    "doc": "Openstack",
    "title": "Openstack - Horizon",
    "content": "The Dashboard for Openstack is called Horizon. Horizon Login . All Admins gain login credentials to the Openstack Dashboard after Handover . Horizon Instances . In the instance overview, you can view the VMs running in the selected project. Horizon Network . In the “Network” tab, you can see the networks in the current project, including the provider networks. The provider networks are the networks that connect the VMs to the customer network. Furthermore, you can view the network topology, showing how the VMs are connected and how the networks are interconnected. Horizon New Network . To create a new network, click on “Create Network” in the “Network” tab. Then, you can choose the name. And you need to create a subnet. To work with the network or connect the network externally, a router must be created. The router must then be connected to a network. This is done by creating an interface on the router, which then connects the router to a network. To connect two networks, the router needs an interface in both networks. ",
    "url": "/edge/openstack/#openstack---horizon",
    
    "relUrl": "/edge/openstack/#openstack---horizon"
  },"48": {
    "doc": "Create VM",
    "title": "Creation of a VM",
    "content": "To create a new Virtual Machine in OpenStack, you need to log in to the OpenStack Dashboard. This access is only available to administrators. After logging in, new VMs can be created in the “Instances” tab. Click on “Launch Instance” to be guided through the creation of the VM using a wizard. ",
    "url": "/edge/openstack/create_vm/#creation-of-a-vm",
    
    "relUrl": "/edge/openstack/create_vm/#creation-of-a-vm"
  },"49": {
    "doc": "Create VM",
    "title": "Details",
    "content": "Here, the name for the VM can be chosen. Additionally, the number of VMs can be changed; the default is one. ",
    "url": "/edge/openstack/create_vm/#details",
    
    "relUrl": "/edge/openstack/create_vm/#details"
  },"50": {
    "doc": "Create VM",
    "title": "Source",
    "content": "Here, you can choose whether the instance should be created from an existing volume. Or whether the VM should be created from an image. Furthermore, you can set whether a new volume should be created for the VM and whether this volume should be deleted along with the VM. ",
    "url": "/edge/openstack/create_vm/#source",
    
    "relUrl": "/edge/openstack/create_vm/#source"
  },"51": {
    "doc": "Create VM",
    "title": "Flavor",
    "content": "Here, you can specify the size of the VM and whether it should be equipped with a graphics card or TSN card if necessary. ",
    "url": "/edge/openstack/create_vm/#flavor",
    
    "relUrl": "/edge/openstack/create_vm/#flavor"
  },"52": {
    "doc": "Create VM",
    "title": "Network",
    "content": "Here, you can select the network to which the VM will be connected during creation. ",
    "url": "/edge/openstack/create_vm/#network",
    
    "relUrl": "/edge/openstack/create_vm/#network"
  },"53": {
    "doc": "Create VM",
    "title": "Key Pair",
    "content": "Here, the SSH key can be selected for connecting via SSH. Additionally, new SSH keys can be imported here. ",
    "url": "/edge/openstack/create_vm/#key-pair",
    
    "relUrl": "/edge/openstack/create_vm/#key-pair"
  },"54": {
    "doc": "Create VM",
    "title": "Create VM",
    "content": " ",
    "url": "/edge/openstack/create_vm/",
    
    "relUrl": "/edge/openstack/create_vm/"
  },"55": {
    "doc": "Resource Metrics",
    "title": "Metrics from Libvirt and Operations Center",
    "content": " ",
    "url": "/edge/openstack/metrics/#metrics-from-libvirt-and-operations-center",
    
    "relUrl": "/edge/openstack/metrics/#metrics-from-libvirt-and-operations-center"
  },"56": {
    "doc": "Resource Metrics",
    "title": "Overview",
    "content": "The feature enables the provision of metrics from Libvirt and the Operations Center. These metrics include information on the use of flavors, CPU, RAM and other resources as well as data on the project owners. ",
    "url": "/edge/openstack/metrics/#overview",
    
    "relUrl": "/edge/openstack/metrics/#overview"
  },"57": {
    "doc": "Resource Metrics",
    "title": "Endpoint",
    "content": "URL: https://resources._EDGE_.gecgo.net/federate . Credentials are required for access. These can be obtained from the helpdesk. Example configuration Prometheus . Further details in the Prometheus documentation on Federation and scrape configuration. scrape_configs: - job_name: 'edge_metrics' scrape_interval: 15s honor_labels: true metrics_path: '/federate' scheme: https basic_auth: username: scrape password_file: ./scrape_user_edge params: 'match[]': - '{__name__=~\".+\"}' static_configs: - targets: - 'resources._EDGE_.gecgo.net' . ",
    "url": "/edge/openstack/metrics/#endpoint",
    
    "relUrl": "/edge/openstack/metrics/#endpoint"
  },"58": {
    "doc": "Resource Metrics",
    "title": "Metrics from Libvirt",
    "content": "The metrics from Libvirt provide detailed information about the virtual machines (VMs), including: . | Flavors used: Information about the allocated resource profiles of the VMs. | CPU usage: Current utilization of the CPU resources. | RAM usage: Current memory usage. | Other metrics: Additional information about the VMs available via Libvirt. | . ",
    "url": "/edge/openstack/metrics/#metrics-from-libvirt",
    
    "relUrl": "/edge/openstack/metrics/#metrics-from-libvirt"
  },"59": {
    "doc": "Resource Metrics",
    "title": "Metrics from the Operations Center",
    "content": "In addition to the Libvirt metrics, metrics about the project owners from the Operations Center are also provided. An example of such a metric is: . sql_oc_membership{job=\"oc-exporter\",instance=\"de-host-rack-01\",owner=\"jemand@userdomain.de\",project_name=\"projectname-dea9e633-9a03-4130-900b-dfde07734bff\"} 1 . This metric contains the following information: . | Job: The name of the export job (oc-exporter). | Instance: The instance ID (de-host-rack-01). | Owner: The email address of the project owner (someone@userdomain.de). | Project Name: The name of the project (projectname-dea9e633-9a03-4130-900b-dfde07734bff). | Value: The value of the metric (1). | . ",
    "url": "/edge/openstack/metrics/#metrics-from-the-operations-center",
    
    "relUrl": "/edge/openstack/metrics/#metrics-from-the-operations-center"
  },"60": {
    "doc": "Resource Metrics",
    "title": "Metrics",
    "content": "| Name | Description | Type | . | libvirt_domain_block_meta | Block device metadata info. Device name, source file, serial. | gauge | . | libvirt_domain_block_stats_allocation | Offset of the highest written sector on a block device. | gauge | . | libvirt_domain_block_stats_capacity_bytes | Logical size in bytes of the block device backing image. | gauge | . | libvirt_domain_block_stats_flush_requests_total | Total flush requests from a block device. | counter | . | libvirt_domain_block_stats_flush_time_seconds_total | Total time in seconds spent on cache flushing to a block device | counter | . | libvirt_domain_block_stats_limit_burst_length_read_requests_seconds | Read requests per second burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_length_total_requests_seconds | Total requests per second burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_length_write_requests_seconds | Write requests per second burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_read_bytes | Read throughput burst limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_burst_read_bytes_length_seconds | Read throughput burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_read_requests | Read requests per second burst limit | gauge | . | libvirt_domain_block_stats_limit_burst_total_bytes | Total throughput burst limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_burst_total_bytes_length_seconds | Total throughput burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_total_requests | Total requests per second burst limit | gauge | . | libvirt_domain_block_stats_limit_burst_write_bytes | Write throughput burst limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_burst_write_bytes_length_seconds | Write throughput burst time in seconds | gauge | . | libvirt_domain_block_stats_limit_burst_write_requests | Write requests per second burst limit | gauge | . | libvirt_domain_block_stats_limit_read_bytes | Read throughput limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_read_requests | Read requests per second limit | gauge | . | libvirt_domain_block_stats_limit_total_bytes | Total throughput limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_total_requests | Total requests per second limit | gauge | . | libvirt_domain_block_stats_limit_write_bytes | Write throughput limit in bytes per second | gauge | . | libvirt_domain_block_stats_limit_write_requests | Write requests per second limit | gauge | . | libvirt_domain_block_stats_physicalsize_bytes | Physical size in bytes of the container of the backing image. | gauge | . | libvirt_domain_block_stats_read_bytes_total | Number of bytes read from a block device, in bytes. | counter | . | libvirt_domain_block_stats_read_requests_total | Number of read requests from a block device. | counter | . | libvirt_domain_block_stats_read_time_seconds_total | Total time spent on reads from a block device, in seconds. | counter | . | libvirt_domain_block_stats_size_iops_bytes | The size of IO operations per second permitted through a block device | gauge | . | libvirt_domain_block_stats_write_bytes_total | Number of bytes written to a block device, in bytes. | counter | . | libvirt_domain_block_stats_write_requests_total | Number of write requests to a block device. | counter | . | libvirt_domain_block_stats_write_time_seconds_total | Total time spent on writes on a block device, in seconds | counter | . | libvirt_domain_info_cpu_time_seconds_total | Amount of CPU time used by the domain, in seconds. | counter | . | libvirt_domain_info_maximum_memory_bytes | Maximum allowed memory of the domain, in bytes. | gauge | . | libvirt_domain_info_memory_usage_bytes | Memory usage of the domain, in bytes. | gauge | . | libvirt_domain_info_meta | Domain metadata | gauge | . | libvirt_domain_info_virtual_cpus | Number of virtual CPUs for the domain. | gauge | . | libvirt_domain_info_vstate | Virtual domain state. 0: no state, 1: the domain is running, 2: the domain is blocked on resource, 3: the domain is paused by user, 4: the domain is down, 5: the domain is shut off,6: the domain is crashed, 7: the domain is suspended by guest power management | gauge | . | libvirt_domain_interface_meta | Interfaces metadata. Source bridge, target device, interface uuid | gauge | . | libvirt_domain_interface_stats_receive_bytes_total | Number of bytes received on a network interface, in bytes. | counter | . | libvirt_domain_interface_stats_receive_drops_total | Number of packet receive drops on a network interface. | counter | . | libvirt_domain_interface_stats_receive_errors_total | Number of packet receive errors on a network interface. | counter | . | libvirt_domain_interface_stats_receive_packets_total | Number of packets received on a network interface. | counter | . | libvirt_domain_interface_stats_transmit_bytes_total | Number of bytes transmitted on a network interface, in bytes. | counter | . | libvirt_domain_interface_stats_transmit_drops_total | Number of packet transmit drops on a network interface. | counter | . | libvirt_domain_interface_stats_transmit_errors_total | Number of packet transmit errors on a network interface. | counter | . | libvirt_domain_interface_stats_transmit_packets_total | Number of packets transmitted on a network interface. | counter | . | libvirt_domain_memory_stats_actual_balloon_bytes | Current balloon value (in bytes). | gauge | . | libvirt_domain_memory_stats_available_bytes | The total amount of usable memory as seen by the domain. This value may be less than the amount of memory assigned to the domain if driver is in use or if the guest OS does not initialize all assigned pages. This value is expressed in bytes. | gauge | . | libvirt_domain_memory_stats_disk_cache_bytes | The amount of memory, that can be quickly reclaimed without additional I/O (in bytes).Typically these pages are used for caching disk | gauge | . | libvirt_domain_memory_stats_major_fault_total | Page faults occur when a process makes a valid access to virtual memory that is not available. When servicing the page fault, if is required, it is considered a major fault. | counter | . | libvirt_domain_memory_stats_minor_fault_total | Page faults occur when a process makes a valid access to virtual memory that is not available. When servicing the page not fault, IO is required, it is considered a minor fault. | counter | . | libvirt_domain_memory_stats_rss_bytes | Resident Set Size of the process running the domain. This value is in bytes | gauge | . | libvirt_domain_memory_stats_unused_bytes | The amount of memory left completely unused by the system. Memory that is available but used for reclaimable caches should NOT be free. This value is expressed in bytes. | gauge | . | libvirt_domain_memory_stats_usable_bytes | How much the balloon can be inflated without pushing the guest system to swap, corresponds to ‘Available’ in /proc/meminfo | gauge | . | libvirt_domain_memory_stats_used_percent | The amount of memory in percent, that used by domain. | gauge | . | libvirt_domain_vcpu_cpu | Real CPU number, or one of the values from virVcpuHostCpuState | gauge | . | libvirt_domain_vcpu_state | VCPU state. 0: offline, 1: running, 2: blocked | gauge | . | libvirt_domain_vcpu_time_seconds_total | Amount of CPU time used by the domain’s VCPU, in seconds. | counter | . | libvirt_domain_vcpu_wait_seconds_total | Vcpu’s wait_sum metric. CONFIG_SCHEDSTATS has to be enabled | counter | . | libvirt_up | Whether scraping libvirt’s metrics was successful. | gauge | . | libvirt_versions_info | Versions of virtualization components | gauge | . | sql_oc_membership | Project owner information of Operations Center | counter | . ",
    "url": "/edge/openstack/metrics/#metrics",
    
    "relUrl": "/edge/openstack/metrics/#metrics"
  },"61": {
    "doc": "Resource Metrics",
    "title": "Resource Metrics",
    "content": " ",
    "url": "/edge/openstack/metrics/",
    
    "relUrl": "/edge/openstack/metrics/"
  },"62": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/edge/faq/",
    
    "relUrl": "/edge/faq/"
  },"63": {
    "doc": "FAQ",
    "title": "Openstack",
    "content": "How are users added in Openstack? . Currently, users are added only upon request via Jira Helpdesk Ticket since every OpenStack user is also an admin. How can VMs from other users/projects be deleted? . In the Horizon dashboard, you can access all VMs via the admin section and delete them. How can the disk size of an existing VM be increased? . This is not possible in the current version of OpenStack. What subnets are available? . There are always the following subnets: . | public | shared | private/internal | . Upon request, a fourth network can be created: Lab. This is intended for direct connection without the Barracuda Firewall. How are subnet separations managed? . The networks are purely virtualized via OpenVSwitch. Compliance with zones is managed in the OperationsCenter via Security Rules and Security Groups, which are IPFilter rules in OpenStack Neutron. ",
    "url": "/edge/faq/#openstack",
    
    "relUrl": "/edge/faq/#openstack"
  },"64": {
    "doc": "FAQ",
    "title": "Operations Center",
    "content": "How are users added in the OperationsCenter? . Access to the Operations Center is realized via SSO. After users have logged in, they can be added to a project by project owners. Groups vs Roles: Difference between Roles and Groups . The Operations Center currently has 4 user groups: . | External Users | Users (SSO) | Admins (SSO) | Global Admins (SSO) | . Can admin rights be restricted to one project only? How? . Users automatically have full access to projects they create. Other project owners can pass on the right. How can I change the owner of a project? . Any project owner and admin with access to the project can change the permissions. What happens when a user leaves the company? . Through SSO integration, the user automatically loses access. It may be necessary to reassign their projects. See the previous point. VPN accesses must be deleted manually. Can specifications be made on how a project should be named? . During creation. There is no provision for changes. How is a VPN created? . Global administrators can activate the function for individual projects. Project owners can further enable users added to the project. How are project shares created? . On the home page of each project via the member list. How are flavors managed? . Flavors are managed in OpenStack. Each “public” flavor is available for configuration in Operations Center. If a “public” flavor should no longer be selectable in Operations Center when creating a VM, the metadata of the flavor can be changed accordingly. To do this, select the flavor in Horizon, click on “Update Metadata”, enter visibility in the “Custom” field and add it with “+”. The field now appears under “Existing Metadata”. Enter the value “false” here and save the change. The flavor can now no longer be selected when creating a new VM. Existing VMs that use this flavor are not affected and can still be managed in Operations Center. Flavors cannot be restricted to individual users. Are flavors allowed to be deleted? . Yes, but only if it’s ensured that no VM is using the flavor. Deletion is done via Horizon. If a flavor is deleted that is still in use, each project that has a VM using that flavor cannot be managed in Operations Center any more. Where can logs be viewed if a VM has been created but doesn’t appear? . It’s possible to observe and repeat the VM startup process via Horizon. How can VMs be deleted from other users/projects? . Only users and global administrators with project access can delete VMs. Project list in Operations Center vs Horizon . Projects from the Operations Center are also visible in Horizon. Projects created in Horizon are not visible in the Operations Center. How can VMs be started? . VMs can be started via the “Start” button in the Operations Center. How can the disk size of an existing VM be increased in Operations Center? . It’s not possible. How can external/internal users be deleted? . Internal users are authenticated via SSO. If they are disabled there, they are automatically blocked. External users can be deleted by administrators in the overview of external users in the Operations Center. How can the router be edited and configured? . The Operations Center automatically configures routers according to project specifications. How can routes be edited in the Operations Center? . Additional routes cannot be set up in the Operations Center. How can an SSH key be added to an existing VM? . It’s not possible to add SSH keys via the Operations Center after VM creation. They must be added directly to the VM. Volume Sizes . The Operations Center allows the creation of volumes in different sizes. The list can be expanded upon request. Can the Operations Center be configured via API? . No, that is not intended. How can I log in to a Windows VM? The user ‘operation’ does not work with the entered password. Instead of the ‘operation’ user, for Windows VMs, the user of the selected image must be used. Often Administrator. ",
    "url": "/edge/faq/#operations-center",
    
    "relUrl": "/edge/faq/#operations-center"
  },"65": {
    "doc": "FAQ",
    "title": "General",
    "content": "Zones . Security zones are structured in hierarchically descending order, starting with the Private/Internal Zone, followed by the Shared Zone and Public Zone. Based on this, virtual machines (VMs) can communicate hierarchically descending by assigning floating IPs. If VMs are located in the Private/Internal Zones, communication with floating IPs in the Shared and Public Zones is possible. However, the reverse, from the Shared Zone to the Private/Internal Zones, is not allowed. Similarly, communication from the Public Zone to the Shared Zone to the Private/Internal Zones is not allowed. Communication always follows the principle from secure to insecure. This hierarchical structure allows orderly and security-conscious communication between the different zones. Furthermore, floating IPs from the Private/Internal Zones are only reachable from the institute network, services with a floating IP from the Public IP can be provided to the internet through the institute IT. On request, a Lab Zone can be set up as a special zone with a direct connection to the switches. The zones are managed by the Operations Center. For projects created in OpenStack, the user is responsible. How can the hardware servers be shut down/restarted? For example, during a temporary power outage. Currently only through a request in the Jira Helpdesk. Do servers start automatically after a power outage? . The servers start automatically, but the edge is not automatically usable, as CEPH replication is disabled during a controlled shutdown. This must be re-enabled upon restart. If the attached Windows image is selected, does it already include a license? What kind of license is it? . It’s a trial license from Microsoft. For Windows VMs, the customer is responsible for acquiring and adding a license. NVLink and NVSwitches . NVLink and NVSwitches connect NVidia graphics cards for direct data exchange between the graphics cards. Currently, we use Supermicro GPU servers, which do not have NVLink or NVSwitches between the individual GPUs. At present, NVLink and NVSwitches are only installed in NVidia’s own servers of the DGX or HGX brand. ",
    "url": "/edge/faq/#general",
    
    "relUrl": "/edge/faq/#general"
  },"66": {
    "doc": "Guided Tour",
    "title": "Guided Tour",
    "content": "From the browser to a self written heat template . ",
    "url": "/optimist/guided_tour/",
    
    "relUrl": "/optimist/guided_tour/"
  },"67": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Step 1: The Horizon (Dashboard)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/#step-1-the-horizon-dashboard",
    
    "relUrl": "/optimist/guided_tour/step01/#step-1-the-horizon-dashboard"
  },"68": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Start",
    "content": "In this step-by-step tutorial, you will learn how to use OpenStack. You will start with the Horizon dashboard, then switch to the console and finish with writing your own Heat template. ",
    "url": "/optimist/guided_tour/step01/#start",
    
    "relUrl": "/optimist/guided_tour/step01/#start"
  },"69": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Login",
    "content": "After you have received your credentials, you can log on to the dashboard. IMPORTANT: You cannot automatically reset the password. If you need a new password, write an e-mail to support@gec.io . The URL for the dashboard is https://optimist.gec.io . The login form appears. Use default for the Domain field, and enter your user name and password in the relevant fields. To log in, click Connect. ",
    "url": "/optimist/guided_tour/step01/#login",
    
    "relUrl": "/optimist/guided_tour/step01/#login"
  },"70": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Change Password",
    "content": "For security reasons we recommend changing your password after receiving it from WIIT. To change the password, click your User name (1) in right corner of Horizon and then Settings (2). The Settings window appears, where you can change various settings. To change the password, click Change Password (1) in the left-side navigation menu. Enter your old password (2), then enter the new one (3), and confirm the new one (4). To save it, click Change (5). ",
    "url": "/optimist/guided_tour/step01/#change-password",
    
    "relUrl": "/optimist/guided_tour/step01/#change-password"
  },"71": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "Conclusion",
    "content": "You have taken your first steps in the dashboard and successfully changed your password. ",
    "url": "/optimist/guided_tour/step01/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step01/#conclusion"
  },"72": {
    "doc": "01: The Horizon (Dashboard)",
    "title": "01: The Horizon (Dashboard)",
    "content": " ",
    "url": "/optimist/guided_tour/step01/",
    
    "relUrl": "/optimist/guided_tour/step01/"
  },"73": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "Step 2: Create an SSH-Key with the Horizon (Dashboard)",
    "content": " ",
    "url": "/optimist/guided_tour/step02/#step-2-create-an-ssh-key-with-the-horizon-dashboard",
    
    "relUrl": "/optimist/guided_tour/step02/#step-2-create-an-ssh-key-with-the-horizon-dashboard"
  },"74": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "Start",
    "content": "To continue, you need an SSH keypair. If you already have a keypair and know how to use it, you can skip this section and continue with Step 3. ",
    "url": "/optimist/guided_tour/step02/#start",
    
    "relUrl": "/optimist/guided_tour/step02/#start"
  },"75": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "Installation",
    "content": "There are several ways to generate an SSH keypair. Later in this guided tour, we will show you how to create a keypair manually. In this step, however, you will learn how to create it from the dashboard. To create the SSH keypair, go to Compute -&gt; Key Pairs and click Create Key Pair. A window appears where you can name the key. In this example, the name BeispielKey is used. After you have entered the name, click Create Keypair. ",
    "url": "/optimist/guided_tour/step02/#installation",
    
    "relUrl": "/optimist/guided_tour/step02/#installation"
  },"76": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "Conclusion",
    "content": "You have now created your SSH keypair. ",
    "url": "/optimist/guided_tour/step02/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step02/#conclusion"
  },"77": {
    "doc": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "title": "02: Create an SSH-Key with the Horizon (Dashboard)",
    "content": " ",
    "url": "/optimist/guided_tour/step02/",
    
    "relUrl": "/optimist/guided_tour/step02/"
  },"78": {
    "doc": "03: Spawn a new Stack",
    "title": "Step 3: Spawn a new Stack",
    "content": " ",
    "url": "/optimist/guided_tour/step03/#step-3-spawn-a-new-stack",
    
    "relUrl": "/optimist/guided_tour/step03/#step-3-spawn-a-new-stack"
  },"79": {
    "doc": "03: Spawn a new Stack",
    "title": "Start",
    "content": "In this step you will use the dashboard to spawn a stack that includes a VM. You will also get better acquainted with the dashboard. For this step you need the SSH keypair created in Step 2. To spawn a new stack, you need a template that starts a VM. We recommend using SingleServer.yaml from the GECio Github Repository. Once you have acquired the template, log in to the dashboad with your new password (see Step 1). Go to Orchestration → Stacks and click LAUNCH STACK. In the following dialog window, for Template Source select File, and for Template File, use the downloaded SingleServer.yaml file. Then click Next. In the next dialog window, provide the following data: . | Stack Name: BeispielServer | Creation Timeout: 60 | Password for User: Please use your own password | availability_zone: ix1 | flavor_name: m1.micro | key_name: BeispielKey | machine_name: singleserver | public_network_id: provider | . After you have completed all fields, click Launch to spawn the stack. The stack will spawn and looks like this. You can verify if the stack has correctly started the instance. Navigate to Compute -&gt; Instances. The overview should look like this: . You have spawned the stack. Now we will show you how to delete it, including the VM. It is also possible to delete the instance itself, but this may cause problems if you want to delete the stack afterwards. To delete a stack, navigate to Orchestration → Stack, and click the down arrow symbol behind the Example Stack. Then choose Delete Stack. ",
    "url": "/optimist/guided_tour/step03/#start",
    
    "relUrl": "/optimist/guided_tour/step03/#start"
  },"80": {
    "doc": "03: Spawn a new Stack",
    "title": "Conclusion",
    "content": "You have created and deleted your first stack. ",
    "url": "/optimist/guided_tour/step03/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step03/#conclusion"
  },"81": {
    "doc": "03: Spawn a new Stack",
    "title": "03: Spawn a new Stack",
    "content": " ",
    "url": "/optimist/guided_tour/step03/",
    
    "relUrl": "/optimist/guided_tour/step03/"
  },"82": {
    "doc": "04: Your way to the console",
    "title": "Step 4: Your way to the console",
    "content": " ",
    "url": "/optimist/guided_tour/step04/#step-4-your-way-to-the-console",
    
    "relUrl": "/optimist/guided_tour/step04/#step-4-your-way-to-the-console"
  },"83": {
    "doc": "04: Your way to the console",
    "title": "Start",
    "content": "To make the administration of OpenStack as simple as possible, we recommend using the OpenStackClient. For simple, non-recurring tasks, it may be easier to use the Horizon dashboard. With recurring tasks, or when you want to manage a complex stack, it is better to use the OpenStack client and Heat. This may be unfamiliar at first, but with some practice, you can quickly and efficiently manage stacks. The client is helpful in the administration of the OpenStack environment. It contains Nova, Glance, Cinder, and Neutron. As we use the client heavily in our Guided Tour, we will install it in the next step. ",
    "url": "/optimist/guided_tour/step04/#start",
    
    "relUrl": "/optimist/guided_tour/step04/#start"
  },"84": {
    "doc": "04: Your way to the console",
    "title": "Installation",
    "content": "To install the OpenStackClient, you need at least Python 2.7 and Python Setuptools (which are included in macOS). There are several ways to install the OpenStackClient. In our example, we use pip, and we recommend that you do the same. “pip” is easy to use and you can also use it as an update manager for pip. You can install the client as root (the administrative user), but that may cause problems. Therefore, you can install it in a virtual environment. macOS . To install the OpenStackClient, you need to install pip. Start the console (Launchpad → Console) and type the following command: . $ easy_install pip Searching for pip Best match: pip 9.0.1 Adding pip 9.0.1 to easy-install.pth file Installing pip script to /usr/local/bin Installing pip2.7 script to /usr/local/bin Installing pip2 script to /usr/local/bin Using /usr/local/lib/python2.7/site-packages Processing dependencies for pip Finished processing dependencies for pip . Now you can install virtualenv. $ pip install virtualenv Collecting virtualenv Downloading virtualenv-15.1.0-py2.py3-none-any.whl (1.8MB) 100% |????????????????????????????????| 1.8MB 619kB/s Installing collected packages: virtualenv Successfully installed virtualenv-15.1.0 . After you have installed virtualenv, you can create the virtual environment. $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Now you may activate the virtual environment. $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Now you can install the OpenStack client. (openstack) $ pip install python-openstackclient . Since we use other services in our documentation, you can install these clients as well. (openstack) $ pip install python-heatclient python-designateclient python-octaviaclient . Now that you are done, you can deactivate your environment. (openstack) $ deactivate . Finally, ensure that you can use the client outside of your virtual environment. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Now you can check that everything works. It should look like this: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . Windows . If Python is already installed, you need to navigate to installation folder (standard installation folder C:\\Python27\\Scripts). To install pip, use the command easy_install pip: . Once pip is installed, you can install the OpenStack client: . Linux (in our example Ubuntu) . First, install pip. $ sudo apt-get install python3-pip Reading package lists... Done Building dependency tree Reading state information... Done . Next, install virtualenv, which is required to set up your virtual environment. $ sudo apt-get install python3-virtualenv Reading package lists... Done Building dependency tree Reading state information... Done . Now you can create a virtual environment where you install the OpenStack client. $ virtualenv ~/.virtualenvs/openstack New python executable in /Users/iNNOVO/.virtualenvs/openstack/bin/python Installing setuptools, pip, wheel...done. Then you can activate your newly created environment. $ source ~/.virtualenvs/openstack/bin/activate (openstack) $ . Once activated, you can install the OpenStackClient: . (openstack) $ pip install python-openstackclient . As we use Heat in our documentation, you will also install the Heat client. (openstack) $ pip install python-heatclient . Once you are done, you can deactivate your virtual environment. (openstack) $ deactivate . Finally, ensure that you can use your newly installed software. export PATH=\"$HOME/.virtualenvs/openstack/bin/:$PATH\" . Now you can check that everything works. It should look like this: . $ type -a openstack openstack is /home/iNNOVO/.virtualenvs/openstack/bin/openstack . ",
    "url": "/optimist/guided_tour/step04/#installation",
    
    "relUrl": "/optimist/guided_tour/step04/#installation"
  },"85": {
    "doc": "04: Your way to the console",
    "title": "Credentials",
    "content": "For the OpenStack client to work, you need to supply it with the credentials. You can download the credentials directly from the Horizon dashboard. After the login, click on your mail address in the right corner. Then click  Download OpenStack RC File v3. macOS | Linux . You need to source the credentials, which can be easily done with this command (IMPORTANT: The command can only be used in the folder where the RC file was downloaded):   . source EXAMPLE.sh . Windows . To source the credentials on Windows, you must use PowerShell, Git for Windows or Linux on Windows. If you use Git for Windows or Linux on Windows, you can use the same commands described in the macOS | Linux section. source EXAMPLE.sh . If you use PowerShell, you need to set each variable individually. All required variables are in the previously downloaded Beispiel.sh. To set the variables, use the following command: . set-item env:OS_AUTH_URL -value \"https://identity.optimist.gec.io/v3\" set-item env:OS_PROJECT_ID -value \"Projekt ID eintragen\" set-item env:OS_PROJECT_NAME -value \"Namen eintrage\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_USERNAME -value \"Usernamen eintragen\" set-item env:OS_PASSWORD -value \"Passwort eingeben\" set-item env:OS_USER_DOMAIN_NAME -value \"Default\" set-item env:OS_REGION_NAME -value \"fra\" set-item env:OS_INTERFACE -value \"public\" set-item env:OS_IDENTITY_API_VERSION -value \"3\" . ",
    "url": "/optimist/guided_tour/step04/#credentials",
    
    "relUrl": "/optimist/guided_tour/step04/#credentials"
  },"86": {
    "doc": "04: Your way to the console",
    "title": "Conclusion",
    "content": "You now have an OpenStack client with working credentials, and can test the commands. To get an overview of all OpenStack commands, run the following command: . openstack --help . ",
    "url": "/optimist/guided_tour/step04/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step04/#conclusion"
  },"87": {
    "doc": "04: Your way to the console",
    "title": "04: Your way to the console",
    "content": " ",
    "url": "/optimist/guided_tour/step04/",
    
    "relUrl": "/optimist/guided_tour/step04/"
  },"88": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Step 5: An overview of the most important OpenStackClient commands",
    "content": " ",
    "url": "/optimist/guided_tour/step05/#step-5-an-overview-of-the-most-important-openstackclient-commands",
    
    "relUrl": "/optimist/guided_tour/step05/#step-5-an-overview-of-the-most-important-openstackclient-commands"
  },"89": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Start",
    "content": "Now that you have installed the OpenStack client in Step 4, you will learn some important commands. To get more information about a specific subcommand, append the --help flag to it. To list all commands, you can use the --help flag: . openstack --help . ",
    "url": "/optimist/guided_tour/step05/#start",
    
    "relUrl": "/optimist/guided_tour/step05/#start"
  },"90": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Server",
    "content": "With the command openstack server you can create, administrate, or delete a VM. Here is a list of some common commands: . | openstack server add Adds parameters (Fixed IP, Floating IP, Security group, Volume) to a VM | openstack server create Creates a VM | openstack server delete Deletes a VM | openstack server list Shows a list of all VMs | openstack server remove Removes parameters (Fixed IP, Floating IP, Security group, Volume) from a VM | openstack server show Shows all important information about the specified VM | . ",
    "url": "/optimist/guided_tour/step05/#server",
    
    "relUrl": "/optimist/guided_tour/step05/#server"
  },"91": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Stack",
    "content": "With the command openstack stack you are able to administrate complete stacks, like openstack server for instances. Here is a list for some common commands: . | openstack stack create Creates a new stack | openstack stack list Shows a list of all stacks | openstack stack show Shows all important information about the specified stack | openstack stack delete Deletes the specified stack | . ",
    "url": "/optimist/guided_tour/step05/#stack",
    
    "relUrl": "/optimist/guided_tour/step05/#stack"
  },"92": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Security Group",
    "content": "Security Groups are used to allow or deny incoming and outgoing network traffic based on IP adresses and ports for VMs. You can also manage security groups in the OpenStackClient. Here are some common commands: . | openstack security group create Creates a new security group. | openstack security group delete Deletes a security group | openstack security group list Shows a list of all security groups | openstack security group show Shows all important information about a security group | openstack security group rule create Adds a rule for a security group | openstack security group rule delete Deletes a rule in a security group | . ",
    "url": "/optimist/guided_tour/step05/#security-group",
    
    "relUrl": "/optimist/guided_tour/step05/#security-group"
  },"93": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Network",
    "content": "To create VMs, a network is required. Here are some common network commands: . | openstack network create Creates a new network | openstack network list Shows a list of all networks | openstack network show Shows all important information about a network | openstack network delete Deletes a network | . ",
    "url": "/optimist/guided_tour/step05/#network",
    
    "relUrl": "/optimist/guided_tour/step05/#network"
  },"94": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Router",
    "content": "For the VMs on your network to reach the internet, you need a router. Here are some common router commands. | openstack router create Creates a new router | openstack router delete Deletes a router | openstack router add port Adds a port to a router | openstack router add subnet Adds a subnet to a router | . ",
    "url": "/optimist/guided_tour/step05/#router",
    
    "relUrl": "/optimist/guided_tour/step05/#router"
  },"95": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Subnet",
    "content": "To use a virtual router correctly, you need a subnet that can be administrated with openstack subnet. Here are some common commands: . | openstack subnet create Creates a new subnet | openstack subnet delete Deletes a subnet | openstack subnet show Shows all infomation about a subnet | . ",
    "url": "/optimist/guided_tour/step05/#subnet",
    
    "relUrl": "/optimist/guided_tour/step05/#subnet"
  },"96": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Port",
    "content": "Ports connect your VMs to your network. Here are some common commands: . | openstack port create Create a new port | openstack port delete Deletes a port | openstack port show Shows all infomation about a port | . ",
    "url": "/optimist/guided_tour/step05/#port",
    
    "relUrl": "/optimist/guided_tour/step05/#port"
  },"97": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Volume",
    "content": "Volumes are storage locations that persist across the existence of individual instances. Here are some common commands: . | openstack volume create Creates a new Volume | openstack volume delete Deletes a volume | openstack volume show Shows all infomation about a volume | . ",
    "url": "/optimist/guided_tour/step05/#volume",
    
    "relUrl": "/optimist/guided_tour/step05/#volume"
  },"98": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "Conclusion",
    "content": "Now you know some common OpenStack commands, and have a better overview of the system. These commands are required in the next steps and form the basis for the rest of the guided tour. In Step 6, you will create and use your own SSH key pairs. ",
    "url": "/optimist/guided_tour/step05/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step05/#conclusion"
  },"99": {
    "doc": "05: An overview of the most important OpenStackClient commands",
    "title": "05: An overview of the most important OpenStackClient commands",
    "content": " ",
    "url": "/optimist/guided_tour/step05/",
    
    "relUrl": "/optimist/guided_tour/step05/"
  },"100": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Step 6: Create and use your own SSH-Key",
    "content": " ",
    "url": "/optimist/guided_tour/step06/#step-6-create-and-use-your-own-ssh-key",
    
    "relUrl": "/optimist/guided_tour/step06/#step-6-create-and-use-your-own-ssh-key"
  },"101": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Start",
    "content": "To access your VMs with SSH you need to create an SSH keypair. If you already have a keypair, you do not need to create a new one. ",
    "url": "/optimist/guided_tour/step06/#start",
    
    "relUrl": "/optimist/guided_tour/step06/#start"
  },"102": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Creation",
    "content": "As mentioned in Step 2, there are several ways to create an SSH keypair. You may create one from the console using the following command: . $ ssh-keygen -t rsa -f Beispiel.key Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in Beispiel.key. Your public key has been saved in Beispiel.key.pub. The key fingerprint is: SHA256:UKSodmr6MFCO1fSqNYAoyM7uX8n/O5a43cPEV5vJXW8 The key's randomart image is: +---[RSA 2048]----+ | .o |+. o o o |=.+ o + |+= o ..|oo+ = S . o B|o. =... o . =E|o.+ + . + . |.= ...+.o |.oo. o++o.. | +----[SHA256]-----+ . The command above generates two files, i.e. the aforementioned key pair. The two generated files are Beispiel.key (private key) and Beispiel,key.pub (public key). You should always keep your private key in a secure location, while distributing the public key to places you require access to. ",
    "url": "/optimist/guided_tour/step06/#creation",
    
    "relUrl": "/optimist/guided_tour/step06/#creation"
  },"103": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Installation",
    "content": "To start using your new keypair, you need to add it to your OpenStack environment. You can do this with the OpenStack client. Use the command below (in our example, the created keypair is stored in ~/.ssh/. If your keys are saved in a different location, you need to copy the keypair to ~/.ssh/). $ openstack keypair create --public-key ~/.ssh/Beispiel.key.pub Beispiel +-------------+-------------------------------------------------+ | Field | Value | +-------------+-------------------------------------------------+ | fingerprint | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | name | Beispiel | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | +-------------+-------------------------------------------------+ . You can check that this worked by listing the keys. The one you just uploaded should be also visible. $ openstack keypair list +----------+-------------------------------------------------+ | Name | Fingerprint | +----------+-------------------------------------------------+ | Beispiel | ec:a6:75:f9:33:4b:e0:ba:e7:bb:b6:8a:a1:5d:48:ff | +----------+-------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step06/#installation",
    
    "relUrl": "/optimist/guided_tour/step06/#installation"
  },"104": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "Conclusion",
    "content": "You have now generated a keypair and uploaded the public key. You can use it to log in to your new Instances. We will explain this in Step 7. ",
    "url": "/optimist/guided_tour/step06/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step06/#conclusion"
  },"105": {
    "doc": "06: Create and use your own SSH-Key",
    "title": "06: Create and use your own SSH-Key",
    "content": " ",
    "url": "/optimist/guided_tour/step06/",
    
    "relUrl": "/optimist/guided_tour/step06/"
  },"106": {
    "doc": "07: The first VM",
    "title": "Step 7: The first VM",
    "content": " ",
    "url": "/optimist/guided_tour/step07/#step-7-the-first-vm",
    
    "relUrl": "/optimist/guided_tour/step07/#step-7-the-first-vm"
  },"107": {
    "doc": "07: The first VM",
    "title": "Start",
    "content": "In the previous steps, you learned everything you need to create a VM. In general, it is best to create VMs as part of a stack, and to create these stacks with Heat, or other automation tools like Terraform. To ensure that you know the basics, this step deals with creating a single VM manually. ",
    "url": "/optimist/guided_tour/step07/#start",
    
    "relUrl": "/optimist/guided_tour/step07/#start"
  },"108": {
    "doc": "07: The first VM",
    "title": "Installation",
    "content": "The basic command to create a single VM is: . openstack server create test . If you execute the above command, the following error is returned: . usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; openstack server create: error: argument --flavor is required . It tells us that you have not specified a flavor for your VM. To specify a flavor, you need to add the flag --flavor with a flavor argument. With the following command you can see the available flavors: . $ openstack flavor list +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | ID | Name | RAM | Disk | Ephemeral | VCPUs | Is Public | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ | 090bcc91-6207-465d-aff0-bfcc10a9e063 | m1.medium | 8192 | 20 | 0 | 4 | True | 4ade7a50-f829-4bf6-af15-266798ea8d6f | win.large | 32768 | 80 | 0 | 8 | True | 5dd72380-088e-48cd-9a18-112cb5a9cab5 | win.small | 8192 | 80 | 0 | 2 | True | 884d5b93-1467-4bc1-a445-ff7c74271cbd | m1.micro | 1024 | 20 | 0 | 1 | True | b7c4fa0b-7960-4311-a86b-507dbf58e8ac | m1.small | 4096 | 20 | 0 | 2 | True | d45e3029-8364-4e4c-beab-242e8b4622a3 | win.medium | 16384 | 80 | 0 | 4 | True | dfead62e-96a8-46e9-bdae-342ecce32d41 | win.micro | 2048 | 80 | 0 | 1 | True | ed18c320-324a-487f-88e1-3e9eb9244509 | m1.large | 16384 | 20 | 0 | 8 | True | +--------------------------------------+------------+-------+------+-----------+-------+-----------+ . If you add --flavor m1.micro to the command and execute it, it will still return an error as OpenStack needs more data to start a new VM. In addition to flavors, you need to supply the key (--key-name) and the operating system image to be installed (--image), the network the VM will run on (--network), and the security group to be applied to it (--security-group). You already created a security group in a previous step, so you need to acquire an image and a network to create your first VM. With the following command you can see the available images: . $ openstack image list +--------------------------------------+---------------------------------------+--------+ | ID | Name | Status | +--------------------------------------+---------------------------------------+--------+ | fd8ad5aa-6b33-4198-a05d-8be42fc0f20e | CentOS 7 - Latest | active | 82242d21-d990-4fc2-92a5-c7bd7820e790 | Ubuntu 16.04 Xenial Xerus - Latest | active | 8e82fd42-3d6f-44a7-9f20-92f5661823cf | Windows Server 2012 R2 Std - Latest | active | 536c086c-d2a4-43dd-80ea-a9d05ee2b97f | Windows Server 2016 Std - Latest | active | c94ced87-a03e-4eec-89f7-48f2c0ec6cd2 | debian-9.1.5-20170910-openstack-amd64 | active | b1195ddf-9336-42a7-a134-4f2e7ea57710 | iNNOVO-OPNsense-17.7.8 | active | 9134b6ed-8c5a-4a9a-907e-733dc2b5f0ef | iNNOVO_pfSense 2.3.4 | active | +--------------------------------------+---------------------------------------+--------+ . Next, you have to select a nework. You can create a simple network with the following command: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T08:32:44Z | description | | dns_domain | None | id | a783d691-7efe-4f67-9226-99a014fa8926 | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T08:32:44Z | +---------------------------+--------------------------------------+ . Note that this network has no internet connection, and no additional configuration. You will learn how to create a functional network in Step 10. Now we will put everything together, and create a VM. In this example, you will use the default security group, the Ubuntu 16.04 image (you use the ID in the command line), and the previously created network and key: . $ openstack server create BeispielServer --flavor m1.small --key-name Beispiel --image 82242d21-d990-4fc2-92a5-c7bd7820e790 --network=BeispielNetzwerk --security-group default +-----------------------------+--------------------------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | config_drive | | created | 2017-12-06T14:15:02Z | flavor | m1.small (676d2587-b5aa-49eb-998d-d91c1bd6c056) | hostId | | id | 44ff2688-4ce5-417d-962b-3a80199bf1bc | image | cirros-tempest1 (2fbe66ef-adc8-44d0-b2e2-03d95dc36936) | key_name | cg | name | BeispielServer | progress | 0 | project_id | 1e775e2cc71a461991be42d4fad8a5cb | properties | | security_groups | name='3265503b-ac24-4f60-a8d0-466b7c812916' | status | BUILD | updated | 2017-12-06T14:15:02Z | user_id | b54fda3f4d1a484797b3ad4de9b3f4f9 | volumes_attached | +-----------------------------+--------------------------------------------------------+ . To see all possible parameters during the creation of a VM, you can use --help: . $ openstack server create --help usage: openstack server create [-h] [-f {json,shell,table,value,yaml}] [-c COLUMN] [--max-width &lt;integer&gt;] [--print-empty] [--noindent] [--prefix PREFIX] (--image &lt;image&gt; | --volume &lt;volume&gt;) --flavor &lt;flavor&gt; [--security-group &lt;security-group-name&gt;] [--key-name &lt;key-name&gt;] [--property &lt;key=value&gt;] [--file &lt;dest-filename=source-filename&gt;] [--user-data &lt;user-data&gt;] [--availability-zone &lt;zone-name&gt;] [--block-device-mapping &lt;dev-name=mapping&gt;] [--nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt;] [--hint &lt;key=value&gt;] [--config-drive &lt;config-drive-volume&gt;|True] [--min &lt;count&gt;] [--max &lt;count&gt;] [--wait] &lt;server-name&gt; Create a new server positional arguments: &lt;server-name&gt; New server name optional arguments: -h, --help show this help message and exit --image &lt;image&gt; Create server boot disk from this image (name or ID) --volume &lt;volume&gt; Create server using this volume as the boot disk (name or ID) --flavor &lt;flavor&gt; Create server with this flavor (name or ID) --security-group &lt;security-group-name&gt; Security group to assign to this server (name or ID) (repeat option to set multiple groups) --key-name &lt;key-name&gt; Keypair to inject into this server (optional extension) --property &lt;key=value&gt; Set a property on this server (repeat option to set multiple values) --file &lt;dest-filename=source-filename&gt; File to inject into image before boot (repeat option to set multiple files) --user-data &lt;user-data&gt; User data file to serve from the metadata server --availability-zone &lt;zone-name&gt; Select an availability zone for the server --block-device-mapping &lt;dev-name=mapping&gt; Map block devices; map is &lt;id&gt;:&lt;type&gt;:&lt;size(GB)&gt;:&lt;delete_on_terminate&gt; (optional extension) --nic &lt;net-id=net-uuid,v4-fixed-ip=ip-addr,v6-fixed-ip=ip-addr,port-id=port-uuid,auto,none&gt; Create a NIC on the server. Specify option multiple times to create multiple NICs. Either net-id or port- id must be provided, but not both. net-id: attach NIC to network with this UUID, port-id: attach NIC to port with this UUID, v4-fixed-ip: IPv4 fixed address for NIC (optional), v6-fixed-ip: IPv6 fixed address for NIC (optional), none: (v2.37+) no network is attached, auto: (v2.37+) the compute service will automatically allocate a network. Specifying a --nic of auto or none cannot be used with any other --nic value. --hint &lt;key=value&gt; Hints for the scheduler (optional extension) --config-drive &lt;config-drive-volume&gt;|True Use specified volume as the config drive, or 'True' to use an ephemeral drive --min &lt;count&gt; Minimum number of servers to launch (default=1) --max &lt;count&gt; Maximum number of servers to launch (default=1) --wait Wait for build to complete output formatters: output formatter options -f {json,shell,table,value,yaml}, --format {json,shell,table,value,yaml} the output format, defaults to table -c COLUMN, --column COLUMN specify the column(s) to include, can be repeated table formatter: --max-width &lt;integer&gt; Maximum display width, &lt;1 to disable. You can also use the CLIFF_MAX_TERM_WIDTH environment variable, but the parameter takes precedence. --print-empty Print empty table if there is no data to show. json formatter: --noindent whether to disable indenting the JSON shell formatter: a format a UNIX shell can parse (variable=\"value\") --prefix PREFIX add a prefix to all variable names . ",
    "url": "/optimist/guided_tour/step07/#installation",
    
    "relUrl": "/optimist/guided_tour/step07/#installation"
  },"109": {
    "doc": "07: The first VM",
    "title": "Conclusion",
    "content": "You have now created your first Instance using basic OpenStack commands. In the next step, you will delete this instance. ",
    "url": "/optimist/guided_tour/step07/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step07/#conclusion"
  },"110": {
    "doc": "07: The first VM",
    "title": "07: The first VM",
    "content": " ",
    "url": "/optimist/guided_tour/step07/",
    
    "relUrl": "/optimist/guided_tour/step07/"
  },"111": {
    "doc": "08: Delete the first VM",
    "title": "Step 8: Delete the first VM",
    "content": " ",
    "url": "/optimist/guided_tour/step08/#step-8-delete-the-first-vm",
    
    "relUrl": "/optimist/guided_tour/step08/#step-8-delete-the-first-vm"
  },"112": {
    "doc": "08: Delete the first VM",
    "title": "Start",
    "content": "In the previous step, you created a VM. In this step, you will learn how to delete it so that you can reuse its resources. First of all, you need to acquire the name or the ID of the VM. If you only have few VMs, you can use the name. Since names are not unique, we strongly recommend using the ID. Let’s get a list of all your VMs: . $ openstack server list +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | ID | Name | Status | Networks | Image Name | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ | 801b3021-0c00-4566-881e-b50d47152e63 | singleserver | ACTIVE | single_internal_network=10.0.0.12, 185.116.245.39 | Ubuntu 16.04 Xenial Xerus - Latest | +--------------------------------------+--------------+--------+---------------------------------------------------+------------------------------------+ . This returns a list of all your VMs. You can find the ID in column ID, and the name in column Name. Now that you have this information, you can delete it: . openstack server delete 801b3021-0c00-4566-881e-b50d47152e63 . If you re-run the command, it should return nothing at all: . $ openstack server list $ . ",
    "url": "/optimist/guided_tour/step08/#start",
    
    "relUrl": "/optimist/guided_tour/step08/#start"
  },"113": {
    "doc": "08: Delete the first VM",
    "title": "Conclusion",
    "content": "You have now learned how to delete instances. Additionally, with the command openstack server list you can get an overview of all instances. In Step 9, we will focus on security groups. ",
    "url": "/optimist/guided_tour/step08/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step08/#conclusion"
  },"114": {
    "doc": "08: Delete the first VM",
    "title": "08: Delete the first VM",
    "content": " ",
    "url": "/optimist/guided_tour/step08/",
    
    "relUrl": "/optimist/guided_tour/step08/"
  },"115": {
    "doc": "09: Security Groups",
    "title": "Step 9: Security Groups",
    "content": " ",
    "url": "/optimist/guided_tour/step09/#step-9-security-groups",
    
    "relUrl": "/optimist/guided_tour/step09/#step-9-security-groups"
  },"116": {
    "doc": "09: Security Groups",
    "title": "Start",
    "content": "By default, any incoming traffic to a VM is denied. To allow access to an instance, at least one security group must be created and assigned to the instance. While you can add all access rules to a single security group, we recommend using a separate security group for each service. ",
    "url": "/optimist/guided_tour/step09/#start",
    
    "relUrl": "/optimist/guided_tour/step09/#start"
  },"117": {
    "doc": "09: Security Groups",
    "title": "Create a Security Group",
    "content": "The base command for creating a security group is openstack security group create, for example: . openstack security group create allow-ssh-from-anywhere --description Beispiel +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 2 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:01:42Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . Now that you have created an empty security group, you need to add some rules. Some commonly used options are: . | --protocol: The protocol that this rule matches (example arguments: tcp, udp, icmp) | --dst-port: Destination port range to give access to (example arguments: 22:22 for port 22 100:200 for ports 100 through 200) | --remote-ip: Remote IP to allow access from (example arguments: 0.0.0.0/0 for all IP addresses, 1.2.3.0/24 for all IP addresses starting with 1.2.3.) | --ingress or --egress: ingress is incoming traffic and egress is outgoing traffic (no arguments possible) | . You can use these options to create a rule for your new security group to allow SSH from anywhere: . $ openstack security group rule create allow-ssh-from-anywhere --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0 +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:02:15Z | description | | direction | ingress | ether_type | IPv4 | id | 694a0573-b4c3-423c-847d-550f79e83f2b | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | 0.0.0.0/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:02:15Z | +-------------------+--------------------------------------+ . Next, verify if your security group was created correctly: . $ openstack security group show allow-ssh-from-anywhere +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2017-12-08T12:01:42Z | description | Beispiel | id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | name | allow-ssh-from-anywhere | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 3 | rules | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv6', id='5a852e4b-1d79-4fe9-b359-64ca54c98501', | | updated_at='2017-12-08T12:01:42Z' | | created_at='2017-12-08T12:02:15Z', direction='ingress', ethertype='IPv4', id='694a0573-b4c3-423c-847d-550f79e83f2b', port_range_max='22', | | port_range_min='22', protocol='tcp', remote_ip_prefix='0.0.0.0/0', updated_at='2017-12-08T12:02:15Z' | | created_at='2017-12-08T12:01:42Z', direction='egress', ethertype='IPv4', id='fa90a1ee-d3b9-40d4-9bb5-89fdd5005c02', | | updated_at='2017-12-08T12:01:42Z' | updated_at | 2017-12-08T12:02:15Z | +-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step09/#create-a-security-group",
    
    "relUrl": "/optimist/guided_tour/step09/#create-a-security-group"
  },"118": {
    "doc": "09: Security Groups",
    "title": "Conclusion",
    "content": "You have successfully created a security group. In the next step, you learn how to add a network. ",
    "url": "/optimist/guided_tour/step09/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step09/#conclusion"
  },"119": {
    "doc": "09: Security Groups",
    "title": "09: Security Groups",
    "content": " ",
    "url": "/optimist/guided_tour/step09/",
    
    "relUrl": "/optimist/guided_tour/step09/"
  },"120": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Step 10: Get access to the Internet: Create a network",
    "content": " ",
    "url": "/optimist/guided_tour/step10/#step-10-get-access-to-the-internet-create-a-network",
    
    "relUrl": "/optimist/guided_tour/step10/#step-10-get-access-to-the-internet-create-a-network"
  },"121": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Start",
    "content": "So far, you have created a VM and a security group. The next step is to create a network. ",
    "url": "/optimist/guided_tour/step10/#start",
    
    "relUrl": "/optimist/guided_tour/step10/#start"
  },"122": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "The network",
    "content": "You will start with the network. As with previous commands, you have additional options you can list with --help. To create your network use the following command: . $ openstack network create BeispielNetzwerk +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:06:38Z | description | | dns_domain | None | id | ff6d8654-66d6-4881-9528-2686bddcb6dc | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | mtu | 1500 | name | BeispielNetzwerk | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 2 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | updated_at | 2017-12-08T12:06:38Z | +---------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#the-network",
    
    "relUrl": "/optimist/guided_tour/step10/#the-network"
  },"123": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Subnet",
    "content": "Now that you have a network, you need to create a subnet for it. The subnet creation command also has a few options. In our example, we use: . | --network: Specifies the network where the subnet will be created | --subnet-range: The CIDR range for the subnet. In our example it is 192.168.2.0/24 | . To create a subnet in your existing network, run the following command: . $ openstack subnet create BeispielSubnet --network BeispielNetzwerk --subnet-range 192.168.2.0/24 +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | allocation_pools | 192.168.2.2-192.0.2.254 | cidr | 192.168.2.0/24 | created_at | 2017-12-08T12:09:07Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 192.168.2.1 | host_routes | | id | 984b24bf-db60-46a9-83c3-d68f6f1062e4 | ip_version | 4 | ipv6_address_mode | None | ipv6_ra_mode | None | name | BeispielSubnet | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | updated_at | 2017-12-08T12:09:07Z | use_default_subnet_pool | None | +-------------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#subnet",
    
    "relUrl": "/optimist/guided_tour/step10/#subnet"
  },"124": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Router",
    "content": "For your virtual network to be able to reach the internet, you need to create a router: . $ openstack router create BeispielRouter +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2017-12-08T12:09:49Z | description | | distributed | False | external_gateway_info | None | flavor_id | None | ha | False | id | bfb91c7f-acca-450a-aae0-c519ab563d38 | name | BeispielRouter | project_id | b15cde70d85749689e08106f973bb002 | revision_number | None | routes | | status | ACTIVE | updated_at | 2017-12-08T12:09:49Z | +-------------------------+--------------------------------------+ . To be able to access the internet, you need to define the external gateway: . openstack router set BeispielRouter --external-gateway provider . Next, add the subnet to the router: . openstack router add subnet BeispielRouter BeispielSubnet . ",
    "url": "/optimist/guided_tour/step10/#router",
    
    "relUrl": "/optimist/guided_tour/step10/#router"
  },"125": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Port",
    "content": "Now that you have your subnet and router, you need to create a port for the network. You can link the port using the --network option: . $ openstack port create BeispielPort --network BeispielNetzwerk +-----------------------+----------------------------------------------------------------------------+ | Field | Value | +-----------------------+----------------------------------------------------------------------------+ | admin_state_up | UP | allowed_address_pairs | | binding_host_id | None | binding_profile | None | binding_vif_details | None | binding_vif_type | None | binding_vnic_type | normal | created_at | 2017-12-08T12:12:13Z | description | | device_id | | device_owner | | dns_assignment | None | dns_name | None | extra_dhcp_opts | | fixed_ips | ip_address='192.168.2.8', subnet_id='984b24bf-db60-46a9-83c3-d68f6f1062e4' | id | 31777c0a-a952-43ca-bb7f-11ad33926dae | ip_address | None | mac_address | fa:16:3e:09:88:c8 | name | BeispielPort | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | option_name | None | option_value | None | port_security_enabled | True | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 3 | security_group_ids | 3d3e3074-3087-4965-9a64-34a6d56193b9 | status | DOWN | subnet_id | None | updated_at | 2017-12-08T12:12:13Z | +-----------------------+----------------------------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step10/#port",
    
    "relUrl": "/optimist/guided_tour/step10/#port"
  },"126": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "Conclusion",
    "content": "After the router, subnet, and port have been created and linked together, the setup of the sample network is complete. In the next step, you will add IPv6 access. ",
    "url": "/optimist/guided_tour/step10/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step10/#conclusion"
  },"127": {
    "doc": "10: Get access to the Internet; Create a network",
    "title": "10: Get access to the Internet; Create a network",
    "content": " ",
    "url": "/optimist/guided_tour/step10/",
    
    "relUrl": "/optimist/guided_tour/step10/"
  },"128": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Step 11: Prepare access to the Internet: Add IPv6 to your network",
    "content": " ",
    "url": "/optimist/guided_tour/step11/#step-11-prepare-access-to-the-internet-add-ipv6-to-your-network",
    
    "relUrl": "/optimist/guided_tour/step11/#step-11-prepare-access-to-the-internet-add-ipv6-to-your-network"
  },"129": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Start",
    "content": "Now that you have a working network, the next step is to expand it by enabling IPv6 on your setup. You do not have to create a new router, as the existing one will be used. The cloud images we supply have a predefined primary network interface with DHCP enabled. Once you have completed this step, IPv6 will work as well. ",
    "url": "/optimist/guided_tour/step11/#start",
    
    "relUrl": "/optimist/guided_tour/step11/#start"
  },"130": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Subnet",
    "content": "We have already defined an IPv6 pool. It will be used to create a new subnet. Let’s list all existing pools: . $ openstack subnet pool list +--------------------------------------+---------------+---------------------+ | ID | Name | Prefixes | +--------------------------------------+---------------+---------------------+ | f541f3b6-af22-435a-9cbb-b233d12e74f4 | customer-ipv6 | 2a00:c320:1000::/48 | +--------------------------------------+---------------+---------------------+ . You can now use the pool to generate a subnet. The 64 bit prefix length is fixed for each generated subnet. You can use the subnet in the creation process, or you can accept the default from OpenStack. Let’s create your subnet now: . $ openstack subnet create --network BeispielNetzwerk --ip-version 6 --use-default-subnet-pool --ipv6-address-mode dhcpv6-stateful --ipv6-ra-mode dhcpv6-stateful BeispielSubnetIPv6 +-------------------------+----------------------------------------------------------+ | Field | Value | +-------------------------+----------------------------------------------------------+ | allocation_pools | 2a00:c320:1000:2::2-2a00:c320:1000:2:ffff:ffff:ffff:ffff | cidr | 2a00:c320:1000:2::/64 | created_at | 2017-12-08T12:41:42Z | description | | dns_nameservers | | enable_dhcp | True | gateway_ip | 2a00:c320:1000:2::1 | host_routes | | id | 0046c29b-a9b0-47c3-b5dd-704aa801704d | ip_version | 6 | ipv6_address_mode | dhcpv6-stateful | ipv6_ra_mode | dhcpv6-stateful | name | BeispielSubnetIPv6 | network_id | ff6d8654-66d6-4881-9528-2686bddcb6dc | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | f541f3b6-af22-435a-9cbb-b233d12e74f4 | updated_at | 2017-12-08T12:41:42Z | use_default_subnet_pool | True | +-------------------------+----------------------------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#subnet",
    
    "relUrl": "/optimist/guided_tour/step11/#subnet"
  },"131": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Router",
    "content": "Now that the subnet has been created, it can be added to the router. To do so, execute the following command: . openstack router add subnet BeispielRouter BeispielSubnetIPv6 . ",
    "url": "/optimist/guided_tour/step11/#router",
    
    "relUrl": "/optimist/guided_tour/step11/#router"
  },"132": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Security Group",
    "content": "The security group rules that you created in Step 9 were IPv4 rules. Now you need to add two more rules for IPv6. First, allow SSH access using IPv6 (::/0 is the equivalent of 0.0.0.0/0 but for IPv6): . $ openstack security group rule create --remote-ip \"::/0\" --protocol tcp --dst-port 22:22 --ethertype IPv6 --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:04Z | description | | direction | ingress | ether_type | IPv6 | id | 7d871e85-05fa-4620-b558-c6fc64076cde | name | None | port_range_max | 22 | port_range_min | 22 | project_id | b15cde70d85749689e08106f973bb002 | protocol | tcp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:04Z | +-------------------+--------------------------------------+ . For completion’s sake, we will allow ICMP access so that you can ping your VM with IPv6: . $ openstack security group rule create --remote-ip \"::/0\" --protocol ipv6-icmp --ingress allow-ssh-from-anywhere +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | created_at | 2017-12-08T12:44:44Z | description | | direction | ingress | ether_type | IPv6 | id | f63e4787-9965-4732-b9d2-20ce0fedc974 | name | None | port_range_max | None | port_range_min | None | project_id | b15cde70d85749689e08106f973bb002 | protocol | ipv6-icmp | remote_group_id | None | remote_ip_prefix | ::/0 | revision_number | 0 | security_group_id | 1cab4a62-0fda-40d9-bac8-fd73275b472d | updated_at | 2017-12-08T12:44:44Z | +-------------------+--------------------------------------+ . ",
    "url": "/optimist/guided_tour/step11/#security-group",
    
    "relUrl": "/optimist/guided_tour/step11/#security-group"
  },"133": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Adjustments to the operating system",
    "content": "Any new VM based on our images will now have both IPv4 and IPv6 configured, and our provided heat templates will also enable IPv6. Many standard vendor images do not have IPv6 configured and only have IPv4 enabled by default. If you want to enable IPv6 on a VM where it is not already enabled, you can follow the instructions below. Ubuntu 16.04 . To use IPv6 correctly, the following files must be created with the specified content. | /etc/dhcp/dhclient6.conf . timeout 30; . | /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg . network: {config: disabled} . | /etc/network/interfaces.d/lo.cfg . auto lo iface lo inet loopback . | /etc/network/interfaces.d/ens3.cfg . iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true . | . Now that you have created the files, you can reenable the interface: . sudo ifdown ens3 &amp;&amp; sudo ifup ens3 . Once complete, you will have working IPv4 and IPv6 addresses. If you want to automate the actions above, you can add this to the cloud-init part of our heat template (we will go over cloud-init in Step 19: . #cloud-config write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] . CentOS 7 . To use IPv6 correctly, the following files must be created with the specified content. | /etc/sysconfig/network . NETWORKING_IPV6=yes . | /etc/sysconfig/network-scripts/ifcfg-eth0 . IPV6INIT=yes DHCPV6C=yes . | . Now that you have created the files, you can reenable the interface: . sudo ifdown eth0 &amp;&amp; sudo ifup eth0 . Once complete, you will have working IPv4 and IPv6 addresses. If you want to automate the actions above, you can add this to the cloud-init part of our heat template (we will go over cloud-init in Step 19: . #cloud-config write_files: - path: /etc/sysconfig/network owner: root:root permissions: '0644' content: | NETWORKING=yes NOZEROCONF=yes NETWORKING_IPV6=yes - path: /etc/sysconfig/network-scripts/ifcfg-eth0 owner: root:root permissions: '0644' content: | DEVICE=\"eth0\" BOOTPROTO=\"dhcp\" ONBOOT=\"yes\" TYPE=\"Ethernet\" USERCTL=\"yes\" PEERDNS=\"yes\" PERSISTENT_DHCLIENT=\"1\" IPV6INIT=yes DHCPV6C=yes runcmd: - [ ifdown, eth0] - [ ifup, eth0] . ",
    "url": "/optimist/guided_tour/step11/#adjustments-to-the-operating-system",
    
    "relUrl": "/optimist/guided_tour/step11/#adjustments-to-the-operating-system"
  },"134": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "External access",
    "content": "Important: This VM can now be reached from anywhere in the world via its IPv6 address (only on the ports that you allowed in the security group). Unlike IPv4, you do not need to assign a floating IP address to be able to reach the VM. If you want to reach the VM with IPv4, you must assign a floating IP address. If you want to test the IPv6 reachability but do not have access to a machine with IPv6, you can use certain web-based tools, for example: https://www.subnetonline.com/pages/ipv6-network-tools/online-ipv6-ping.php . ",
    "url": "/optimist/guided_tour/step11/#external-access",
    
    "relUrl": "/optimist/guided_tour/step11/#external-access"
  },"135": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "Conclusion",
    "content": "In the previous step, you established a connection with IPv4. Access via IPv6 has now also been added. In the next step, the instance from Step 7 will be used as a template and made accessible from outside. ",
    "url": "/optimist/guided_tour/step11/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step11/#conclusion"
  },"136": {
    "doc": "11: Prepare access to the Internet; Add IPv6 to your network",
    "title": "11: Prepare access to the Internet; Add IPv6 to your network",
    "content": " ",
    "url": "/optimist/guided_tour/step11/",
    
    "relUrl": "/optimist/guided_tour/step11/"
  },"137": {
    "doc": "12: A usable VM",
    "title": "Step 12: A usable VM",
    "content": " ",
    "url": "/optimist/guided_tour/step12/#step-12-a-usable-vm",
    
    "relUrl": "/optimist/guided_tour/step12/#step-12-a-usable-vm"
  },"138": {
    "doc": "12: A usable VM",
    "title": "Start",
    "content": "Although you already created a VM in Step 7, the VM was not usable as it was not connected to a network, let alone the internet. Let’s create one that you can actually log on to. ",
    "url": "/optimist/guided_tour/step12/#start",
    
    "relUrl": "/optimist/guided_tour/step12/#start"
  },"139": {
    "doc": "12: A usable VM",
    "title": "Installation",
    "content": "To create this VM, add some parameters to the command we used in Step 7: . $ openstack server create BeispielInstanz --flavor m1.small --key-name Beispiel --image \"Ubuntu 16.04 Xenial Xerus - Latest\" --security-group allow-ssh-from-anywhere --network=BeispielNetzwerk +-----------------------------+---------------------------------------------------------------------------+ | Field | Value | +-----------------------------+---------------------------------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | OS-EXT-AZ:availability_zone | es1 | OS-EXT-STS:power_state | NOSTATE | OS-EXT-STS:task_state | scheduling | OS-EXT-STS:vm_state | building | OS-SRV-USG:launched_at | None | OS-SRV-USG:terminated_at | None | accessIPv4 | | accessIPv6 | | addresses | | config_drive | | created | 2017-12-08T12:52:37Z | flavor | m1.small (b7c4fa0b-7960-4311-a86b-507dbf58e8ac) | hostId | | id | 1de98aa4-7d2b-4427-a8a5-d369ea8bdaf5 | image | Ubuntu 16.04 Xenial Xerus - Latest (82242d21-d990-4fc2-92a5-c7bd7820e790) | key_name | Beispiel | name | BeispielInstanz | progress | 0 | project_id | b15cde70d85749689e08106f973bb002 | properties | | security_groups | name='allow-ssh-from-anywhere' | status | BUILD | updated | 2017-12-08T12:52:37Z | user_id | 9bf501f4c3d14b7eb0f1443efe80f656 | volumes_attached | +-----------------------------+---------------------------------------------------------------------------+ . The following parameters are included: . | --flavor: The flavor of the the VM. You get all available flavors with openstack flavor list. | --key-name: The key to install on the VM. | --image: The operating system image to install on the VM. You can get all available images with openstack image list. | --security-group: Specifies the security group. | --network: Specifies the network to attach the VM to. | . If you want to reach your VM from the internet, you need a floating IP address. You can create one as follows: . $ openstack floating ip create provider +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | created_at | 2017-12-08T12:53:37Z | description | | fixed_ip_address | None | floating_ip_address | 185.116.245.65 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 84eca140-9ac1-42c3-baf6-860ba920a23c | name | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | revision_number | 0 | router_id | None | status | DOWN | updated_at | 2017-12-08T12:53:37Z | +---------------------+--------------------------------------+ . The created IP must be associated with your VM: . openstack server add floating ip BeispielInstanz 185.116.245.145 . ",
    "url": "/optimist/guided_tour/step12/#installation",
    
    "relUrl": "/optimist/guided_tour/step12/#installation"
  },"140": {
    "doc": "12: A usable VM",
    "title": "Usage",
    "content": "You should now have a reachable VM. To check if all worked correctly, log in to your VM with SSH. IMPORTANT: You can only log in if the specified ssh key exists and is accessible (if it does not work, follow the instructions in Step 6). $ ssh ubuntu@185.116.245.145 The authenticity of host '185.116.245.145 (185.116.245.145)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.145' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step12/#usage",
    
    "relUrl": "/optimist/guided_tour/step12/#usage"
  },"141": {
    "doc": "12: A usable VM",
    "title": "Clean-Up",
    "content": "If you want to delete all components you just created, you must delete them in the following order. If you do not delete them in the correct order, you will be unable to delete components that other resources depend on. | Instance . | openstack server delete BeispielInstanz | . | Floating-IP . | openstack floating ip delete 185.116.245.145 | . | Router Port . | openstack port delete BeispielPort | . | Router . | openstack router delete BeispielRouter | . | Subnet . | openstack subnet delete BeispielSubnet | . | Network . | openstack network delete BeispielNetzwerk | . | . ",
    "url": "/optimist/guided_tour/step12/#clean-up",
    
    "relUrl": "/optimist/guided_tour/step12/#clean-up"
  },"142": {
    "doc": "12: A usable VM",
    "title": "Conclusion",
    "content": "You have now created a VM based on your knowledge from steps 7 to 11, you can reach it from the internet, and have logged in with SSH. In the next step, you will break away from individual instances and will create a stack. ",
    "url": "/optimist/guided_tour/step12/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step12/#conclusion"
  },"143": {
    "doc": "12: A usable VM",
    "title": "12: A usable VM",
    "content": " ",
    "url": "/optimist/guided_tour/step12/",
    
    "relUrl": "/optimist/guided_tour/step12/"
  },"144": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "Step 13: The structured way to create an instance (with stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/#step-13-the-structured-way-to-create-an-instance-with-stacks",
    
    "relUrl": "/optimist/guided_tour/step13/#step-13-the-structured-way-to-create-an-instance-with-stacks"
  },"145": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "Start",
    "content": "Previously, you created a VM, a security group, and a virtual network separately. Now you will learn how to create all of them at once in an integrated way. This requires a pre-installed python-heatclient, which was already installed in Step 4: Our way to the console. ",
    "url": "/optimist/guided_tour/step13/#start",
    
    "relUrl": "/optimist/guided_tour/step13/#start"
  },"146": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "Installation",
    "content": "Instead of creating individual manually, any OpenStack resources (e.g. instances, networks, routers, security groups) can also be operated in a defined network; a stack (or heat stack). This makes it easy to compose an entire setup, which you can then easily create and delete at will. In this step, you will use a pre-made heat template and later, will learn how to write one yourself. Everything you created in steps 9 through 11 are easily expressed in a single template. Let’s start with an example template. This template creates a stack that includes a VM, two security groups, a virtual network (including router, port, and subnet), and a floating-IP. When you create the stack, make sure that you are in the same directory as the template: . $ openstack stack create -t SingleServer.yaml --parameter key_name=Beispiel SingleServer --wait 2017-12-08 13:13:43Z [SingleServer]: CREATE_IN_PROGRESS Stack CREATE started 2017-12-08 13:13:44Z [SingleServer.router]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:45Z [SingleServer.enable_traffic]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:46Z [SingleServer.router]: CREATE_COMPLETE state changed 2017-12-08 13:13:46Z [SingleServer.internal_network_id]: CREATE_COMPLETE state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.subnet]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:47Z [SingleServer.enable_ssh]: CREATE_COMPLETE state changed 2017-12-08 13:13:48Z [SingleServer.start-config]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:48Z [SingleServer.subnet]: CREATE_COMPLETE state changed 2017-12-08 13:13:49Z [SingleServer.router_subnet_bridge]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:49Z [SingleServer.port]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:50Z [SingleServer.start-config]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.port]: CREATE_COMPLETE state changed 2017-12-08 13:13:50Z [SingleServer.host]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:52Z [SingleServer.router_subnet_bridge]: CREATE_COMPLETE state changed 2017-12-08 13:13:53Z [SingleServer.floating_ip]: CREATE_IN_PROGRESS state changed 2017-12-08 13:13:55Z [SingleServer.floating_ip]: CREATE_COMPLETE state changed 2017-12-08 13:14:05Z [SingleServer.host]: CREATE_COMPLETE state changed 2017-12-08 13:14:06Z [SingleServer]: CREATE_COMPLETE Stack CREATE completed successfully +---------------------+-------------------------------------------------+ | Field | Value | +---------------------+-------------------------------------------------+ | id | 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a | stack_name | SingleServer | description | A simple template to deploy your first instance | creation_time | 2017-12-08T13:13:42Z | updated_time | None | stack_status | CREATE_COMPLETE | stack_status_reason | Stack CREATE completed successfully | +---------------------+-------------------------------------------------+ . Here is a short explanation of the executed command: . The command openstack stack create creates the stack, according to the template defined with -t SingleServer.yaml . We set the parameter key_name with --parameter key_name=BEISPIEL to fill the key_name parameter with BEISPIEL (in this template that installs our BEISPIEL key into your VM). We also named our stack SingleServer. Finally, we used the --wait option to wait and observe the creation process. If you do not add this option, the command completes immediately while the creation process continues in the background. Once the command has completed, you should be able to connect to your VM. First, we acquire the floating IP of the VM: . $ openstack stack output show 0f5cdf0e-24cc-4292-a0bc-adf2e9f8618a instance_fip +--------------+---------------------------------+ | Field | Value | +--------------+---------------------------------+ | description | External IP address of instance | output_key | instance_fip | output_value | 185.116.245.70 | +--------------+---------------------------------+ . Now you can log in to your VM: . $ ssh ubuntu@185.116.245.70 The authenticity of host '185.116.245.70 (185.116.245.70)' can't be established. ECDSA key fingerprint is SHA256:kbSkm8eJA0748911RkbWK2/pBVQOjJBASD1oOOXalk. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '185.116.245.70' (ECDSA) to the list of known hosts. Enter passphrase for key '/Users/ubuntu/.ssh/id_rsa': . ",
    "url": "/optimist/guided_tour/step13/#installation",
    
    "relUrl": "/optimist/guided_tour/step13/#installation"
  },"147": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "Conclusion",
    "content": "Using a heat stack, you combined steps 9 through 11 in a single command. In the following steps we will look into Heat in more detail. ",
    "url": "/optimist/guided_tour/step13/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step13/#conclusion"
  },"148": {
    "doc": "13: The structured way to create an instance (with stacks)",
    "title": "13: The structured way to create an instance (with stacks)",
    "content": " ",
    "url": "/optimist/guided_tour/step13/",
    
    "relUrl": "/optimist/guided_tour/step13/"
  },"149": {
    "doc": "14: Your first steps with HEAT",
    "title": "Step 14: Your first steps with HEAT",
    "content": " ",
    "url": "/optimist/guided_tour/step14/#step-14-your-first-steps-with-heat",
    
    "relUrl": "/optimist/guided_tour/step14/#step-14-your-first-steps-with-heat"
  },"150": {
    "doc": "14: Your first steps with HEAT",
    "title": "Start",
    "content": "In the previous steps, you used a pre-made heat template. The next step is to understand how heat templates are built. ",
    "url": "/optimist/guided_tour/step14/#start",
    
    "relUrl": "/optimist/guided_tour/step14/#start"
  },"151": {
    "doc": "14: Your first steps with HEAT",
    "title": "The Template",
    "content": "Every heat template follows the following structure: . heat_template_version:   description: # The template description   parameter_groups: # Group definitions and their order   parameters: # Parameter definitions   resources: # Resource definitions   outputs: # Definitions of possible outputs   conditions: # Definitions of conditions . ",
    "url": "/optimist/guided_tour/step14/#the-template",
    
    "relUrl": "/optimist/guided_tour/step14/#the-template"
  },"152": {
    "doc": "14: Your first steps with HEAT",
    "title": "Heat Template Version",
    "content": "Template versions cannot be chosen arbitrarily as they have fixed specifications that define which commands are available to use. You can use any of the following versions, although we recommended using the latest one: . | 2013-05-23 | 2014-10-16 | 2015-04-30 | 2015-10-15 | 2016-04-08 | 2016-10-14 | 2017-02-24 | . ",
    "url": "/optimist/guided_tour/step14/#heat-template-version",
    
    "relUrl": "/optimist/guided_tour/step14/#heat-template-version"
  },"153": {
    "doc": "14: Your first steps with HEAT",
    "title": "Description",
    "content": "The description is an optional section that describes the stack. We recommend adding a description of how to use the template, what it is about, any other useful information. This makes it easier to share with others and However, it makes sense, because the template can be described in its basic features and any special features can be pointed out directly. You can also add comments by starting them with the # character. It can be used to temporarily disable lines, or to add more documentation to the template. ",
    "url": "/optimist/guided_tour/step14/#description",
    
    "relUrl": "/optimist/guided_tour/step14/#description"
  },"154": {
    "doc": "14: Your first steps with HEAT",
    "title": "Parameter Groups",
    "content": "In this section, you can specify the parameters, how they should group, and their order. The groups are divided into a list containing single parameters. Every parameter should only have one group to avoid possible errors later. Each parameter group is structured as follows: . parameter_groups: - label: &lt;name of the group&gt; description: &lt;description of the group&gt; parameters: - &lt;name of the parameter&gt; - &lt;name of the parameter&gt; . | label: Name of the group | description: Description of the group | parameter: A list of all parameters in this group | name of the parameter: Name of the parameter that was defined in the parameter section | . ",
    "url": "/optimist/guided_tour/step14/#parameter-groups",
    
    "relUrl": "/optimist/guided_tour/step14/#parameter-groups"
  },"155": {
    "doc": "14: Your first steps with HEAT",
    "title": "Parameters",
    "content": "In this section you can specify the required parameters for your template. Parameters are typically used to make it easy to change certain parts of the template, for example, which SSH key is used. Each parameter is defined separately, starting with the name, with the attributes defined underneath: .  parameters: &lt;Parameter Name&gt;: type: &lt;string | number | json | comma_delimited_list | boolean&gt; label: &lt;Name of the parameter&gt; description: &lt;description of the parameter&gt; default: &lt;default of the parameter&gt; hidden: &lt;true | false&gt; constraints: &lt;constraints for the parameter&gt; immutable: &lt;true | false&gt; . | Parameter Name: Name of the parameter | type: The parameter type (string, number, json, comma_delimited_list, boolean) | label: The parameter name (optional) | description: Description of the parameter (optional) | default: Default value of the parameter. Will be used if the parameter isn’t defined (optional) | hidden:  To hide the parameter in the creation process, you can set hidden: true as a parameter (Optional, and set to false by default) | constraints: You can set a list of constraints. If these aren’t fulfilled, the stack creation will fail. | immutable: If this parameter is set to true, the parameter cannot be changed during a stack update. (This will generate an error if attempted) | . ",
    "url": "/optimist/guided_tour/step14/#parameters",
    
    "relUrl": "/optimist/guided_tour/step14/#parameters"
  },"156": {
    "doc": "14: Your first steps with HEAT",
    "title": "Resources",
    "content": "This block specifies the resources that will be created, with every resource in its own sub block: . resources: &lt;ID of the resource&gt;: type: &lt;resource type&gt; properties: &lt;name of the property&gt;: &lt;value of the property&gt; metadata: &lt;specific metatdata&gt; depends_on: &lt;Resource ID or a list of resources&gt; update_policy: &lt;update rule&gt; deletion_policy: &lt;deletion rule&gt; external_id: &lt;external resource id&gt; condition: &lt;condition name&gt; . | ID of the resource: Must be unique | type: Resource type, for example: OS::NEUTRON::SecurityGroup (for a security group) (required) | properties: A list of properties for resources (optional) | metadata: Metadata for the respective resource (optional) | depends_on: Various dependencies to other resources can be stored here (optional) | update_policy: Update rules can be defined here. As a prerequisite, ensure that the corresponding resource also supports this (optional). | deletion_policy: Specifies rules for deletion. The options are Delete, Retain and Snapshot. With heat_template_version 2016-10-14, you can also enter them in lowercase. | external_id: You can use external resource IDs, if required | condition: You can set specific conditions which need to be met to allow this resource to be created. (optional) | . ",
    "url": "/optimist/guided_tour/step14/#resources",
    
    "relUrl": "/optimist/guided_tour/step14/#resources"
  },"157": {
    "doc": "14: Your first steps with HEAT",
    "title": "Output",
    "content": "With the output block, you can specify the parameters to be shown after creation. Examples include the IP address of a VM, or the URL of a deployed web application. Outputs are specified in sub blocks like this: . outputs: &lt;name of the output&gt;: description: &lt;description&gt; value: &lt;value of the output&gt; condition: &lt;name of the condition&gt; . | name of the output: Must be unique | description: A description can be stored here. (optional) | value: Value of the output (mandatory) | condition: Possible conditions can be specified here (optional) | . ",
    "url": "/optimist/guided_tour/step14/#output",
    
    "relUrl": "/optimist/guided_tour/step14/#output"
  },"158": {
    "doc": "14: Your first steps with HEAT",
    "title": "Conditions",
    "content": "Conditions based on the parameters entered by the user when creating or updating the stack can also be specified in a block. You can set conditions, and if they are not fulfilled, the stack creation fails. The conditions can be linked to resources, resource properties and outputs. conditions: &lt;name of condition 1&gt;: {term1} &lt;name of condition 2&gt;: {term2} . | name of condition: Must be unique | term: true or false are expected as a result | . ",
    "url": "/optimist/guided_tour/step14/#conditions",
    
    "relUrl": "/optimist/guided_tour/step14/#conditions"
  },"159": {
    "doc": "14: Your first steps with HEAT",
    "title": "Conclusion",
    "content": "You have learned the basic structure of a heat template and can now begin creating your own. With this knowledge, you will create your own heat template in the next step. ",
    "url": "/optimist/guided_tour/step14/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step14/#conclusion"
  },"160": {
    "doc": "14: Your first steps with HEAT",
    "title": "14: Your first steps with HEAT",
    "content": " ",
    "url": "/optimist/guided_tour/step14/",
    
    "relUrl": "/optimist/guided_tour/step14/"
  },"161": {
    "doc": "15: The first heat template",
    "title": "Step 15: The first heat template",
    "content": " ",
    "url": "/optimist/guided_tour/step15/#step-15-the-first-heat-template",
    
    "relUrl": "/optimist/guided_tour/step15/#step-15-the-first-heat-template"
  },"162": {
    "doc": "15: The first heat template",
    "title": "Start",
    "content": "In the previous step, you learned the basic layout of a heat template. Now you can create your own. ",
    "url": "/optimist/guided_tour/step15/#start",
    
    "relUrl": "/optimist/guided_tour/step15/#start"
  },"163": {
    "doc": "15: The first heat template",
    "title": "The first template",
    "content": "As stated earlier, your template needs to start with a version definition. Version 2016-10-14 is used for the below example: . heat_template_version: 2016-10-14 . Although it is optional, it is best practice to add a description to your template. heat_template_version: 2016-10-14   description: A simple template to deploy a vm . Next, add the resource “Instanz”. Be sure to pay attention to the structure of our template and to indent the “Instanz” under resources. To indent, use 4 spaces (not tabs). If you use tabs or an inconsistent amount of spaces, it will cause errors that may be hard to locate. Your template should now look like this: . heat_template_version: 2016-10-14 description: A simple template to deploy a vm resources: Instanz: . Next, define the resource type. A detailed list of all types is available in the official OpenStack documentation . In our example, you can define Instanz as a VM: . heat_template_version: 2016-10-14 description: A simple template to deploy a vm resources: Instanz: type: OS::Nova::Server . Now that you have defined the type, you should next define its properties. Let’s define the key, image, and the flavor: . heat_template_version: 2016-10-14 description: A simple template to deploy a vm resources: Instanz: type: OS::Nova::Server properties: key_name: BeispielKey image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step15/#the-first-template",
    
    "relUrl": "/optimist/guided_tour/step15/#the-first-template"
  },"164": {
    "doc": "15: The first heat template",
    "title": "Conclusion",
    "content": "You have now defined a template that creates a single VM instance. If you like, you can run it like you did previously in Step 13: “The structured way to create an instance (with stacks)”. ",
    "url": "/optimist/guided_tour/step15/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step15/#conclusion"
  },"165": {
    "doc": "15: The first heat template",
    "title": "15: The first heat template",
    "content": " ",
    "url": "/optimist/guided_tour/step15/",
    
    "relUrl": "/optimist/guided_tour/step15/"
  },"166": {
    "doc": "16: Get to know HEAT better",
    "title": "Step 16: Get to know HEAT better",
    "content": " ",
    "url": "/optimist/guided_tour/step16/#step-16-get-to-know-heat-better",
    
    "relUrl": "/optimist/guided_tour/step16/#step-16-get-to-know-heat-better"
  },"167": {
    "doc": "16: Get to know HEAT better",
    "title": "Start",
    "content": "At first, it may look like that creating a VM with a Heat template, or alternatively with the OpenStack client takes the same amount of time. While this is true if you only want to create the VM once, the advantage of using Heat is that you can reuse the template. Now that you have a simple template, you can familiarise yourself with Heat by adding a variable parameter to your template. ",
    "url": "/optimist/guided_tour/step16/#start",
    
    "relUrl": "/optimist/guided_tour/step16/#start"
  },"168": {
    "doc": "16: Get to know HEAT better",
    "title": "Parameters",
    "content": "In this example, you will add a parameter for the SSH key. The advantage of this is that you can use a VM with different keys without changing the template. Define the parameter and its type. The correct type for what we want to accomplish here is string: . heat_template_version: 2014-10-16   parameters: key_name: type: string . Now that you have defined the first parameter, you can add the same resource to your template as follows: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: Beispiel image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . Now you can actually use your parameter, and can replace Beispiel with your parameter. You can do this with the get_param syntax (to get the parameter). The template is now ready to use and you can define the key_name from the command line as demonstrated in our previous command: . heat_template_version: 2014-10-16 parameters: key_name: type: string resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step16/#parameters",
    
    "relUrl": "/optimist/guided_tour/step16/#parameters"
  },"169": {
    "doc": "16: Get to know HEAT better",
    "title": "Conclusion",
    "content": "You have now added a variable parameter to your template. In the next step, you will add the network. ",
    "url": "/optimist/guided_tour/step16/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step16/#conclusion"
  },"170": {
    "doc": "16: Get to know HEAT better",
    "title": "16: Get to know HEAT better",
    "content": " ",
    "url": "/optimist/guided_tour/step16/",
    
    "relUrl": "/optimist/guided_tour/step16/"
  },"171": {
    "doc": "17: The network in Heat",
    "title": "Step 17: The network in Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/#step-17-the-network-in-heat",
    
    "relUrl": "/optimist/guided_tour/step17/#step-17-the-network-in-heat"
  },"172": {
    "doc": "17: The network in Heat",
    "title": "Start",
    "content": "Now that you have a simple template with a parameter, you can add the network. ",
    "url": "/optimist/guided_tour/step17/#start",
    
    "relUrl": "/optimist/guided_tour/step17/#start"
  },"173": {
    "doc": "17: The network in Heat",
    "title": "The template",
    "content": "Continue using the template you previously created. First, add a new parameter, the ID of the external network, and name it public_network_id,. Also define a default provider: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small . ",
    "url": "/optimist/guided_tour/step17/#the-template",
    
    "relUrl": "/optimist/guided_tour/step17/#the-template"
  },"174": {
    "doc": "17: The network in Heat",
    "title": "Network",
    "content": "Next, add the network. Like the VM, the network is a resource, it will be added to that block. The type for network resources is OS::Neutron::Net . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk . ",
    "url": "/optimist/guided_tour/step17/#network",
    
    "relUrl": "/optimist/guided_tour/step17/#network"
  },"175": {
    "doc": "17: The network in Heat",
    "title": "The port",
    "content": "Now that you have defined a network you can add the port, which is a resource with type OS::Neutron::Port. To ensure that this port is used by your VM, add the networks property to it. Define a port property that then will use the get_resource function to link it to the Port. Furthermore, you want to link the port to the network by adding a network property that also uses the get_resource function to link it to the Netzwerk. At this point, your template looks like this: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk   Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } . ",
    "url": "/optimist/guided_tour/step17/#the-port",
    
    "relUrl": "/optimist/guided_tour/step17/#the-port"
  },"176": {
    "doc": "17: The network in Heat",
    "title": "The router",
    "content": "Your network needs a Router resource, with the type OS::Neutron::Router. With this type it is important to define the external network it will use: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk }   Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter . ",
    "url": "/optimist/guided_tour/step17/#the-router",
    
    "relUrl": "/optimist/guided_tour/step17/#the-router"
  },"177": {
    "doc": "17: The network in Heat",
    "title": "The subnet",
    "content": "Next, define a subnet for your network. This is the Subnet resource with type OS::Neutron::Subnet. It is in the subnet that you define IP information like nameserver(s), the IP version, the IP range, and other IP-related settings: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter   Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } . ",
    "url": "/optimist/guided_tour/step17/#the-subnet",
    
    "relUrl": "/optimist/guided_tour/step17/#the-subnet"
  },"178": {
    "doc": "17: The network in Heat",
    "title": "Subnet bridge",
    "content": "Finally, define a subnet bridge with type OS::Neutron::RouterInterface. This associates the subnet with the router to ensure that VMs in that subnet will use the router. Additionally, you can define the depends_on property, which ensures that the subnet bridge will only be created if Subnet is available: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 }   Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } . ",
    "url": "/optimist/guided_tour/step17/#subnet-bridge",
    
    "relUrl": "/optimist/guided_tour/step17/#subnet-bridge"
  },"179": {
    "doc": "17: The network in Heat",
    "title": "Conclusion",
    "content": "You have now created the full network. When the stack is created, it will create a VM and all the required components to give it connectivity. The next step is to assign a public IP address to the instance. ",
    "url": "/optimist/guided_tour/step17/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step17/#conclusion"
  },"180": {
    "doc": "17: The network in Heat",
    "title": "17: The network in Heat",
    "content": " ",
    "url": "/optimist/guided_tour/step17/",
    
    "relUrl": "/optimist/guided_tour/step17/"
  },"181": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Step 18: Making your VM reachable via IPv4",
    "content": " ",
    "url": "/optimist/guided_tour/step18/#step-18-making-your-vm-reachable-via-ipv4",
    
    "relUrl": "/optimist/guided_tour/step18/#step-18-making-your-vm-reachable-via-ipv4"
  },"182": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Start",
    "content": "Now that your template has defined the full network and can reach the internet, you next need to ensure that the VM is reachable externally. ",
    "url": "/optimist/guided_tour/step18/#start",
    
    "relUrl": "/optimist/guided_tour/step18/#start"
  },"183": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Floating-IP",
    "content": "Define a floating public IPv4 address, which is a resource with type OS::Neutron::FloatingIP. Note that it is important to define the external network this IP is assigned from and the port this IP leads to: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet }   Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } . ",
    "url": "/optimist/guided_tour/step18/#floating-ip",
    
    "relUrl": "/optimist/guided_tour/step18/#floating-ip"
  },"184": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Security Groups",
    "content": "If you create a stack as outlined above, the VM would start but it would not be reachable. As previously stated, VMs do not receive traffic without a security group in place which explicitly allows this. So, the logical next step is to create a resource with type OS::Neutron::SecurityGroup. The security group must be defined to use the Port. On the resource itself, the rules are specified. These rules include the direction, port range, remote IP prefix, and protocol that these rules intend to allow. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port }   Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . ",
    "url": "/optimist/guided_tour/step18/#security-groups",
    
    "relUrl": "/optimist/guided_tour/step18/#security-groups"
  },"185": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "Conclusion",
    "content": "You can now create a stack that contains a single reachable instance. In the next step, you will customize the instance using CloudConfig. ",
    "url": "/optimist/guided_tour/step18/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step18/#conclusion"
  },"186": {
    "doc": "18: Making your VM reachable via IPv4",
    "title": "18: Making your VM reachable via IPv4",
    "content": " ",
    "url": "/optimist/guided_tour/step18/",
    
    "relUrl": "/optimist/guided_tour/step18/"
  },"187": {
    "doc": "19: Add IPv6 to your template",
    "title": "Step 19: Add IPv6 to your template",
    "content": " ",
    "url": "/optimist/guided_tour/step19/#step-19-add-ipv6-to-your-template",
    
    "relUrl": "/optimist/guided_tour/step19/#step-19-add-ipv6-to-your-template"
  },"188": {
    "doc": "19: Add IPv6 to your template",
    "title": "Start",
    "content": "At this point, you have a VM that is reachable with IPv4. The next step is to add IPv6 support. ",
    "url": "/optimist/guided_tour/step19/#start",
    
    "relUrl": "/optimist/guided_tour/step19/#start"
  },"189": {
    "doc": "19: Add IPv6 to your template",
    "title": "CloudConfig",
    "content": "Cloud config has resource type OS::HEAT::CloudConfig. Cloud config hs a variety of uses, but in this case it will be used to configure IPv6. You will continue using the template that you have been working on in the previous steps. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port }   Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - #MussNochEingetragenWerden network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . The files have been created and the appropriate content added. As stated in Step 11: Prepare access to the internet: Add IPv6 to our network, the interface still needs to be restarted using the command runcmd. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } . The last step is to adjust the security group rules to allow access via IPv6. heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider resources: Instanz: type: OS::Nova::Server properties: key_name: { get_param: key_name } image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: {get_resource: Port } Instanz-Config: type: OS::Heat::CloudConfig properties: cloud_config: write_files: - path: /etc/dhcp/dhclient6.conf content: \"timeout 30;\" - path: /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg content: \"network: {config: disabled}\" - path: /etc/network/interfaces.d/lo.cfg content: | auto lo iface lo inet loopback - path: /etc/network/interfaces.d/ens3.cfg content: | iface ens3 inet6 auto up sleep 5 up dhclient -1 -6 -cf /etc/dhcp/dhclient6.conf -lf /var/lib/dhcp/dhclient6.ens3.leases -v ens3 || true runcmd: - [ ifdown, ens3] - [ ifup, ens3] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } security_groups: { get_resource: Sec_SSH } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron::SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol:tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } - { direction: ingress, remote_ip_prefix: \"::/0\", port_range_min: 22, port_range_max: 22, protocol: tcp, ethertype: IPv6 } - { direction: ingress, remote_ip_prefix: \"::/0\", protocol: ipv6-icmp, ethertype: IPv6 } . ",
    "url": "/optimist/guided_tour/step19/#cloudconfig",
    
    "relUrl": "/optimist/guided_tour/step19/#cloudconfig"
  },"190": {
    "doc": "19: Add IPv6 to your template",
    "title": "Conclusion",
    "content": "You can now customize instances with Cloud Init and use IPv6 usable. In the final step you will learn how to start multiple instances with Heat. ",
    "url": "/optimist/guided_tour/step19/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step19/#conclusion"
  },"191": {
    "doc": "19: Add IPv6 to your template",
    "title": "19: Add IPv6 to your template",
    "content": " ",
    "url": "/optimist/guided_tour/step19/",
    
    "relUrl": "/optimist/guided_tour/step19/"
  },"192": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "Step 20: Build multiple VMs with HEAT",
    "content": " ",
    "url": "/optimist/guided_tour/step20/#step-20-build-multiple-vms-with-heat",
    
    "relUrl": "/optimist/guided_tour/step20/#step-20-build-multiple-vms-with-heat"
  },"193": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "Start",
    "content": "Previous steps outlined how to create a single VM. The next step is to create multiple VMs at the same time. In this step, two VMs that share a common network will be created. ",
    "url": "/optimist/guided_tour/step20/#start",
    
    "relUrl": "/optimist/guided_tour/step20/#start"
  },"194": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "First Steps",
    "content": "To begin with, split the template into two parts. It is best practice to break larger setups up into multiple files. First, start with a simple template which contains only the network and the port. heat_template_version: 2014-10-16 description: A simple template which deploys 3 VMs resources: ExampleNet: type: OS::Neutron::Net properties: name: ExampleNet ExampleSubnet: type: OS::Neutron::Subnet properties: name: ExampleSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: ExampleNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . This is the basic structure for your stack, the file can be saved under the name groups.yaml. Create a new template exampleserver.yaml and define the VM here. Make sure that name and network_id are not defined. Use a valid value to fill image:. You can use the image name or ID. Exact image names / IDs can be obtained by running openstack image list. ß . heat_template_version: 2014-10-16 description: a single server description parameters: network_id: type: string server_name: type: string resources: VM: type: OS::Nova::Server properties: user_data_format: RAW image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small name: { get_param: server_name } networks: - port: { get_resource: ExamplePort } ExamplePort: type: OS::Neutron::Port properties: network: { get_param: network_id } . You can now modify your groups.yaml and add a resource group where you add the VMs with the required arguments. After the file has been updated, it can be saved as exampleserver.yaml . The next step is to integrate the second template created as a resource group. The number of instances, the names, etc. can also be specified here: . heat_template_version: 2014-10-16 description: A simple template which deploys 3 VMs resources:   ExampleVM: type: OS::Heat::ResourceGroup depends_on: ExampleSubnet properties: count: 3 resource_def: type: exampleserver.yaml properties: network_id: { get_resource: ExampleNet} server_name: ExampleVM_%index% ExampleNet: type: OS::Neutron::Net properties: name: ExampleNet ExampleSubnet: type: OS::Neutron::Subnet properties: name: ExampleSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: {get_resource: ExampleNet} ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - {start: 10.0.0.10, end: 10.0.0.250} . Now that you have supplied all data, you can create your stack: . openstack stack create -t groups.yaml &lt;Name of the stack&gt; . ",
    "url": "/optimist/guided_tour/step20/#first-steps",
    
    "relUrl": "/optimist/guided_tour/step20/#first-steps"
  },"195": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "Conclusion",
    "content": "Congratulations, you went from creating a single VM with the web interface to creating full stacks with the OpenStack client. Several instances can now be rolled out at the same time using a template, a good starting point for OpenStack administration. ",
    "url": "/optimist/guided_tour/step20/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step20/#conclusion"
  },"196": {
    "doc": "20: Build multiple VMs with HEAT",
    "title": "20: Build multiple VMs with HEAT",
    "content": " ",
    "url": "/optimist/guided_tour/step20/",
    
    "relUrl": "/optimist/guided_tour/step20/"
  },"197": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "Step 21: Start a VM with an SSD volume",
    "content": " ",
    "url": "/optimist/guided_tour/step21/#step-21-start-a-vm-with-an-ssd-volume",
    
    "relUrl": "/optimist/guided_tour/step21/#step-21-start-a-vm-with-an-ssd-volume"
  },"198": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "Start",
    "content": "Previously, you created a VM from scratch and also learned some HEAT basics. In this step you will boot a VM from an SSD volume. There are different ways to do this. In this step, we will first outline how to do so using Horizon (Dashboard) and secondly outline a different method which modifies the HEAT-Template from Step 18: Your VM will be reachable via IPv4. ",
    "url": "/optimist/guided_tour/step21/#start",
    
    "relUrl": "/optimist/guided_tour/step21/#start"
  },"199": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "The Horizon (Dashboard) way",
    "content": "To get started, you need to log in to Horizon, as described in “Step 1: The Horizon (Dashboard)” Step 1: The Horizon (Dashboard). Next, create a new volume with Project → Volumes → Volumes and a click on + CREATE VOLUME. You need to fill in some information in the new overlay; a description for all required fields are below. After filling out the form, click on CREATE VOLUME . | Volume Name: Defines the name of the volume. In our example it will be prefilled with Ubuntu 16.04 Xenial Xerus - Latest. | Description: If required, you can add a short description. In our example, it is empty. | Volume Source: You can choose between Image and No source, empty image. Please use Image. | Use image as a source: You can choose any image, here we used Ubuntu 16.04 Xenial Xerus - Latest (276.2 MB). | Type: Three options are available high-iops, low-iops or default. To use the SSD storage, you need to choose high-iops. | Size: You can define the size of the volume, we chose 20 GiB. | Availability Zone: Again there are three options available Any Availability Zone, es1, or ix1. Use ix1. | . After volume creation, it should look like this: . There are two options to start a VM. The first option is to click on the down arrow symbol next to Edit Volume (as pictured above) and click on Launch as Instance. There will be a new overlay where you can choose the name (Instance Name) and the availability zone. Please use the same availability zone as previously used for the image (ix1). Switch to Source and choose Volume for Select Boot Source and click on the up-arrow next to the available volume. Switch to Flavor and choose one of the available flavors by clicking on the up-arrow. Next you need to choose a network in Networks. Use one of your networks and click on the up-arrow next to it. Now all required settings are in place and the VM can be started with Launch Instance. If required you can add your own Security Groups and/or Key Pairs. ",
    "url": "/optimist/guided_tour/step21/#the-horizon-dashboard-way",
    
    "relUrl": "/optimist/guided_tour/step21/#the-horizon-dashboard-way"
  },"200": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "The HEAT way",
    "content": "This step uses the HEAT Template from Step 18. It will start a VM by default, so the next step is to adapt it, to allow it to create and boot from an SSD volume. First, add a new parameter “availability_zone”: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 . Now add boot_ssd at the end of our template: . boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . You have added a new parameter and will use this in your new volume. To start the VM from the volume, you need to edit Instanz in your template. You can delete or comment out image, because it is already associated with your volume. Now you can add availability_zone, name, networks, and block_device_mapping: . Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] . The template is finished and should look like this: . heat_template_version: 2014-10-16 parameters: key_name: type: string public_network_id: type: string default: provider availability_zone: type: string default: ix1 resources: Instanz: type: OS::Nova::Server properties: name: SSD-Test availability_zone: { get_param: availability_zone } key_name: { get_param: key_name } #image: Ubuntu 16.04 Xenial Xerus - Latest flavor: m1.small networks: - port: { get_resource: Port } block_device_mapping: [ { device_name: \"vda\", volume_id: { get_resource: boot_ssd }, delete_on_termination: \"true\" } ] Netzwerk: type: OS::Neutron::Net properties: name: BeispielNetzwerk Port: type: OS::Neutron::Port properties: network: { get_resource: Netzwerk } Router: type: OS::Neutron::Router properties: external_gateway_info: { \"network\": { get_param: public_network_id } name: BeispielRouter Subnet: type: OS::Neutron::Subnet properties: name: BeispielSubnet dns_nameservers: - 8.8.8.8 - 8.8.4.4 network: { get_resource: Netzwerk } ip_version: 4 cidr: 10.0.0.0/24 allocation_pools: - { start: 10.0.0.10, end: 10.0.0.250 } Router_Subnet_Bridge: type: OS::Neutron::RouterInterface depends_on: Subnet properties: router: { get_resource: Router } subnet: { get_resource: Subnet } Floating_IP: type: OS::Neutron::FloatingIP properties: floating_network: { get_param: public_network_id } port_id: { get_resource: Port } Sec_SSH: type: OS::Neutron:SecurityGroup properties: description: Diese Security Group erlaubt den eingehenden SSH-Traffic über Port22 und ICMP name: Ermöglicht SSH (Port22) und ICMP rules: - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, port_range_min: 22, port_range_max: 22, protocol: tcp } - { direction: ingress, remote_ip_prefix: 0.0.0.0/0, protocol: icmp } boot_ssd: type: OS::Cinder::Volume properties: name: boot_ssd size: 20 availability_zone: { get_param: availability_zone } volume_type: high-iops image: \"Ubuntu 16.04 Xenial Xerus - Latest\" . ",
    "url": "/optimist/guided_tour/step21/#the-heat-way",
    
    "relUrl": "/optimist/guided_tour/step21/#the-heat-way"
  },"201": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "Conclusion",
    "content": "You have learned to start an instance from a volume and how to use SSD storage. Additionally, you have refreshed your heat knowledge and included a volume. ",
    "url": "/optimist/guided_tour/step21/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step21/#conclusion"
  },"202": {
    "doc": "21: Start a VM with an SSD volume",
    "title": "21: Start a VM with an SSD volume",
    "content": " ",
    "url": "/optimist/guided_tour/step21/",
    
    "relUrl": "/optimist/guided_tour/step21/"
  },"203": {
    "doc": "22: Create a DNS record in Designate",
    "title": "Step 22: Create a DNS record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/#step-22-create-a-dns-record-in-designate",
    
    "relUrl": "/optimist/guided_tour/step22/#step-22-create-a-dns-record-in-designate"
  },"204": {
    "doc": "22: Create a DNS record in Designate",
    "title": "Start",
    "content": "The Openstack Optimist platform includes a technology called DNS-as-a-Service (DNSaaS), also known as Designate. DNSaaS includes a REST API for domain and records management, is multi-tenant and integrates the OpenStack Identity Service (Keystone) for authentication. In this step, we will create a fictitious zone (domain) with MX and A records and store the appropriate IP/CNAME. The first step is to revisit the access data in “Step 4: Our way to the console” and ensure that the python-designateclient is installed (pip install python-openstackclient python-designateclient) The next step is to serve the Openstack client and create a zone for our project. $ openstack zone create --email webmaster@foobar.cloud foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | CREATE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | PENDING | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | None | version | 1 | +----------------+--------------------------------------+ . Note the trailing “.” is required for the zone/domain to be created. The result so far: . $ openstack zone list +--------------------------------------+-----------------------+---------+------------+--------+--------+ | id | name | type | serial | status | action | +--------------------------------------+-----------------------+---------+------------+--------+--------+ | 036ae6e6-6318-47e1-920f-be518d845fb5 | foobar.cloud. | PRIMARY | 1534315524 | ACTIVE | NONE | +--------------------------------------+-----------------------+---------+------------+--------+--------+ $ openstack zone show foobar.cloud. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | NONE | attributes | | created_at | 2018-08-15T06:45:24.000000 | description | None | email | webmaster@foobar.cloud | id | 036ae6e6-6318-47e1-920f-be518d845fb5 | masters | | name | foobar.cloud. | pool_id | bb031d0d-b8ca-455a-8963-50ec70fe57cf | project_id | 2b62bc8ff48445f394d0318dbd058967 | serial | 1534315524 | status | ACTIVE | transferred_at | None | ttl | 3600 | type | PRIMARY | updated_at | 2018-08-15T06:45:30.000000 | version | 2 | +----------------+--------------------------------------+ . The domain “foobar.cloud” is now registered and ready to use (status: ACTIVE) for our project. In the next step, we want to create MX records (records for mail servers in this zone) for this domain. But first let’s see the content (recordsets) that already exist in your new zone. $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 | ACTIVE | NONE | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+--------+--------+ . Here you see an “empty shell” of a domain with automatically generated NS and SOA records which are ready to be queried. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud SOA dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534315524 3507 600 86400 3600 . Creating an MX record: You can now add, modify, or delete records within this zone (openstack recordset –help). For MX records we also set up the typical mail server priorities (10,20), where the lower value is always selected first and the second entry serves as a “backup”. $ openstack recordset create --record '10 mx1.foobar.cloud.' --record '20 mx2.foobar.cloud.' --type MX foobar.cloud. foobar.cloud. +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:15:32.000000 | description | None | id | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | name | foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 10 mx1.foobar.cloud. | | 20 mx2.foobar.cloud. | status | PENDING | ttl | None | type | MX | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ $ openstack recordset list foobar.cloud. +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | id | name | type | records | status | action | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534317332 3507 600 86400 3600 | PENDING | UPDATE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | PENDING | CREATE | | | 10 mx1.foobar.cloud. | | +--------------------------------------+---------------+------+--------------------------------------------------------------------------------+---------+--------+ . $ openstack recordset create --type A --record 1.2.3.4 foobar.cloud. www +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:28:15.000000 | description | None | id | d932688f-21d5-44b1-aa27-030c342788e7 | name | www.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 1.2.3.4 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Result: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318095 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . Once the recordsets are active you can use the designated DNS servers: . | dns1.ddns.innovo.cloud | dns2.ddns.innovo.cloud | . Query for these records: . $ dig +short @dns1.ddns.innovo.cloud foobar.cloud NS dns1.ddns.innovo.cloud. dns2.ddns.innovo.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud MX 10 mx1.foobar.cloud. 20 mx2.foobar.cloud. $ dig +short @dns1.ddns.innovo.cloud foobar.cloud www.foobar.cloud 1.2.3.4 . ATTENTION! At this time, this domain (foobar.cloud) is not yet resolvable worldwide. For this construct to be used worldwide, each domain managed by Designate must have delegation to the name servers dns1.ddns.innovo.cloud and dns2.ddns.innovo.cloud established by the respective registrar. Details about our authoritative DNS servers: . | dns1.ddns.innovo.cloud: ‘185.116.244.45’ / ‘2a00:c320:0:1::d’ | dns2.ddns.innovo.cloud: ‘185.116.244.46’ / ‘2a00:c320:0:1::e’ | . In order to complete the mail records, it is still advisable to store corresponding A-records for the mail servers . $ openstack recordset create --type A --record 2.3.4.5 foobar.cloud. mx1 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:31.000000 | description | None | id | 630d5103-7c02-4a58-83a5-97f802cf141c | name | mx1.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 2.3.4.5 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ and $ openstack recordset create --type A --record 3.4.5.6 foobar.cloud. mx2 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | action | CREATE | created_at | 2018-08-15T07:42:56.000000 | description | None | id | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | name | mx2.foobar.cloud. | project_id | 2b62bc8ff48445f394d0318dbd058967 | records | 3.4.5.6 | status | PENDING | ttl | None | type | A | updated_at | None | version | 1 | zone_id | 036ae6e6-6318-47e1-920f-be518d845fb5 | zone_name | foobar.cloud. | +-------------+--------------------------------------+ . Result after a few seconds: . $ openstack recordset list foobar.cloud. +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ | 4b666317-6170-4d71-8962-94f1cbe94dda | foobar.cloud. | NS | dns1.ddns.innovo.cloud. | ACTIVE | NONE | | | dns2.ddns.innovo.cloud. | | 7915c9c1-de30-4144-b82b-36295687b0fe | foobar.cloud. | SOA | dns2.ddns.innovo.cloud. webmaster.foobar.cloud. 1534318976 3507 600 86400 3600 | ACTIVE | NONE | 4197e380-dc61-4e1d-bf92-0dcf5344eafa | foobar.cloud. | MX | 20 mx2.foobar.cloud. | ACTIVE | NONE | | | 10 mx1.foobar.cloud. | | d932688f-21d5-44b1-aa27-030c342788e7 | www.foobar.cloud. | A | 1.2.3.4 | ACTIVE | NONE | 630d5103-7c02-4a58-83a5-97f802cf141c | mx1.foobar.cloud. | A | 2.3.4.5 | ACTIVE | NONE | 2b47dbe4-70b9-4edb-ac2f-25cb8398bacb | mx2.foobar.cloud. | A | 3.4.5.6 | ACTIVE | NONE | +--------------------------------------+-------------------+------+--------------------------------------------------------------------------------+--------+--------+ . ",
    "url": "/optimist/guided_tour/step22/#start",
    
    "relUrl": "/optimist/guided_tour/step22/#start"
  },"205": {
    "doc": "22: Create a DNS record in Designate",
    "title": "Conclusion",
    "content": "In this step, you have learned how to create a zone, configure a record set, and query it. ",
    "url": "/optimist/guided_tour/step22/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step22/#conclusion"
  },"206": {
    "doc": "22: Create a DNS record in Designate",
    "title": "22: Create a DNS record in Designate",
    "content": " ",
    "url": "/optimist/guided_tour/step22/",
    
    "relUrl": "/optimist/guided_tour/step22/"
  },"207": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Step 23: Object Storage (S3 compatible)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/#step-23-object-storage-s3-compatible",
    
    "relUrl": "/optimist/guided_tour/step23/#step-23-object-storage-s3-compatible"
  },"208": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Start",
    "content": "In the previous steps, you learned about various building blocks in OpenStack. Now we will take a look at the Object Storage, which offers some interesting ways to save data. ",
    "url": "/optimist/guided_tour/step23/#start",
    
    "relUrl": "/optimist/guided_tour/step23/#start"
  },"209": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Credentials",
    "content": "The first step is to obtain login data (ec2 credentials) in order to access Object Storage. Therefore, you need the OpenStackClient (as mentioned in Step 4: Our way to the console”), to create the credentials with the OpenStack API. To create the credentials, run the following command: . openstack ec2 credentials create . The output should look like this: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | &lt;your access_key&gt; | links | {u'self': u'https://identity.optimist.gec.io/v3/users/ | | user-id/credentials/OS-EC2/access_key'} | project_id | &lt;your project_id&gt; | secret | &lt;your secret_key&gt; | trust_id | None | user_id | &lt;your user_id&gt; | +------------+-----------------------------------------------------------------+ . Once the credentials have been created, you need some tools to interact with the ObjectStorage. ",
    "url": "/optimist/guided_tour/step23/#credentials",
    
    "relUrl": "/optimist/guided_tour/step23/#credentials"
  },"210": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "How to get access to the ObjectStorage (S3 compatible)",
    "content": "There are several tools available which allow us to interact with Object Storage, however we recommend using s3cmd as it is straightforward to use and handle. You have already installed “pip” as package-manager (in Step 4) you can also use it to install s3cmd: . pip install s3cmd . Since S3cmd is now installed, the previously created credentials must be entered in a file called .s3cfg in order to begin using it. The file should be located in the user’s home directory, for example, /home/username/ . The following process can now be used to create the .s3cfg file: . touch .s3cfg . You can open .s3cfg with your preferred text editor (for example, vi, vim, nano) and enter your credentials as follows: . access_key = &lt;your access_key&gt; check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = &lt;your secret_key&gt; use_https = True . ",
    "url": "/optimist/guided_tour/step23/#how-to-get-access-to-the-objectstorage-s3-compatible",
    
    "relUrl": "/optimist/guided_tour/step23/#how-to-get-access-to-the-objectstorage-s3-compatible"
  },"211": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "The bucket",
    "content": "After you have access to ObjectStorage (S3 compatible), you can start working with it. If required, you can see all s3cmd commands with: . s3cmd --help . You can now create a bucket. In the broadest sense, buckets are similar to folders, which are required for a structure. A file can only be saved in a bucket. It is important that the name is unique (for all customers). If there is already a bucket available with the name test, you cannot create another one with the name test. We recommend using a UUID and then resolving it in the corresponding application. You can also differentiate between public and private buckets. By default, all buckets are private, and only the creator of the bucket can access them. If needed, you can change it, for example with the Access Control List (ACL). IMPORTANT: If you set a bucket to public, all files in it are reachable. Information about files in this bucket that are set to private can also be retrieved. We recommend only setting specific files to public. Now that we know the key details, it is time to create a bucket with a UUID: . $ s3cmd mb s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 Bucket 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/' created . ",
    "url": "/optimist/guided_tour/step23/#the-bucket",
    
    "relUrl": "/optimist/guided_tour/step23/#the-bucket"
  },"212": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Upload a file",
    "content": "After the bucket has been created, let’s upload a file with the command s3cmd put file_name s3://bucket_name. The outcome should be similar to the below: . $ s3cmd put test.yaml s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189 upload: 'test.yaml' -&gt; 's3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml' [1 of 1] 4218 of 4218 100% in 0s 4.61 kB/s done . ",
    "url": "/optimist/guided_tour/step23/#upload-a-file",
    
    "relUrl": "/optimist/guided_tour/step23/#upload-a-file"
  },"213": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Get access to the files",
    "content": "The general URL for accessing files in Optimist is https://s3.es1.fra.optimist.gec.io/bucket_name/file_name. To get access to your example file, you need to change the settings from private to public. To do this, use the Access Control List (ACL): . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-public s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Public [1 of 1] . Now you can access the file with the following link: https://s3.es1.fra.optimist.gec.io/e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml . To set the file to private once again, use this command: . $ s3cmd setacl s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml --acl-private s3://e4d05df3-aa8e-4a37-b1b5-2745d189b189/test.yaml: ACL set to Private [1 of 1] . ",
    "url": "/optimist/guided_tour/step23/#get-access-to-the-files",
    
    "relUrl": "/optimist/guided_tour/step23/#get-access-to-the-files"
  },"214": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "Conclusion",
    "content": "You have taken your first steps with S3 compatible storage. ",
    "url": "/optimist/guided_tour/step23/#conclusion",
    
    "relUrl": "/optimist/guided_tour/step23/#conclusion"
  },"215": {
    "doc": "23: Object Storage (S3 compatible)",
    "title": "23: Object Storage (S3 compatible)",
    "content": " ",
    "url": "/optimist/guided_tour/step23/",
    
    "relUrl": "/optimist/guided_tour/step23/"
  },"216": {
    "doc": "Networking",
    "title": "Networking",
    "content": " ",
    "url": "/optimist/networking/",
    
    "relUrl": "/optimist/networking/"
  },"217": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Port Forwarding on Floating IPs",
    "content": "Floating IP port forwarding allows users to forward traffic from a TCP/UDP/other protocol port of a floating IP to a TCP/UDP/other protocol port associated to one of the fixed IPs of a Neutron port. ",
    "url": "/optimist/networking/port_forwarding/",
    
    "relUrl": "/optimist/networking/port_forwarding/"
  },"218": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Create a port forwarding rule on a floating IP",
    "content": "In order to apply port forwarding on a floating IP, the following information is required: . | The internal IP address to be used | The UUID of the port to be associated with the floating IP | The port number of the network port’s fixed IPv4 address | The external port number of the port forwarding’s floating IP address | The specific protocol to be used in the port forwarding (in this example TCP) | The floating IP for the port forwarding rule to be applied on. | . The example below demonstrates creation of port forwarding on a floating IP, using the required options: . $ openstack floating ip port forwarding create \\ --internal-ip-address 10.0.0.14 \\ --port 12c29300-0f8a-4c54-a9dc-bee4c12c6ad2 \\ --internal-protocol-port 80 \\ --external-protocol-port 8080 \\ --protocol tcp 185.116.244.141 . ",
    "url": "/optimist/networking/port_forwarding/#create-a-port-forwarding-rule-on-a-floating-ip",
    
    "relUrl": "/optimist/networking/port_forwarding/#create-a-port-forwarding-rule-on-a-floating-ip"
  },"219": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "List port forwarding settings applied to floating IPs",
    "content": "Within a project, a list of port forwarding rules applied to specific floating IPs can be obtained with the following command. $ openstack floating ip port forwarding list 185.116.244.141 . The command above can be further refined using --sort-column --port, --external-protcol-port and --protocol flags before the floating IP. ",
    "url": "/optimist/networking/port_forwarding/#list-port-forwarding-settings-applied-to-floating-ips",
    
    "relUrl": "/optimist/networking/port_forwarding/#list-port-forwarding-settings-applied-to-floating-ips"
  },"220": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Display details of a specific port forwarding rule",
    "content": "To display the specific details of a Port Forwarding rule for a Floating IP, the following command can be used: . $ openstack floating ip port forwarding show &lt;floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#display-details-of-a-specific-port-forwarding-rule",
    
    "relUrl": "/optimist/networking/port_forwarding/#display-details-of-a-specific-port-forwarding-rule"
  },"221": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Modifying Floating IP Port Forwarding Properties",
    "content": "If a port forwarding configuration on a floating IP has already been created using $ openstack floating ip port forwarding create, further changes can be made to the existing configuration using $ openstack floating ip port forwarding set .... The following aspects of the port forwarding can be modified: . | --port: The UUID of the network port | --internal-ip-address: The fixed internal address associated with the floating IP port forwarding rule. | --internal-protocol-port: The TCP/UDP/etc. port number of the network port fixed IPv4 address associated with the floating IP port forwarding rule | --external-protocol-port: The TCP/UDP/etc. port number of the port forwarding rule’s floating IP address | --protocol: The IP protocol used in the floating IP port forwarding rule (TCP/UDP/other) | --description: Text describing/contextualizing the use of the port forwarding configuration | . The configuration of any of the above options can be modified with a variation of the following command: . $ openstack floating ip port forwarding set \\ --port &lt;port&gt; \\ --internal-ip-address &lt;internal-ip-address&gt; \\ --internal-protocol-port &lt;port-number&gt; \\ --external-protocol-port &lt;port-number&gt; \\ --protocol &lt;protocol&gt; \\ --description &lt;description&gt;] \\ &lt;floating-ip&gt; &lt;port-forwarding-id&gt;` . ",
    "url": "/optimist/networking/port_forwarding/#modifying-floating-ip-port-forwarding-properties",
    
    "relUrl": "/optimist/networking/port_forwarding/#modifying-floating-ip-port-forwarding-properties"
  },"222": {
    "doc": "Port Forwarding on Floating IPs",
    "title": "Delete port forwarding from a floating IP",
    "content": "To remove a port forwarding rule from a floating IP, we need the following information: . | The floating IP from which the port forwarding rule is to be removed from. | The port forwarding ID (This ID is applied upon creation and can be obtained using the $ openstack floating ip port forwarding list ... command) | . The following command removes the port forwarding rule from a floating ip: . $ openstack floating ip port forwarding delete &lt;floating-ip&gt; &lt;port-forwarding-id&gt; . ",
    "url": "/optimist/networking/port_forwarding/#delete-port-forwarding-from-a-floating-ip",
    
    "relUrl": "/optimist/networking/port_forwarding/#delete-port-forwarding-from-a-floating-ip"
  },"223": {
    "doc": "Shared Networks",
    "title": "Shared Networks",
    "content": " ",
    "url": "/optimist/networking/shared_networks/",
    
    "relUrl": "/optimist/networking/shared_networks/"
  },"224": {
    "doc": "Shared Networks",
    "title": "Motivation",
    "content": "The question often arises of whether it is possible to share a network between two OpenStack projects. In this document, we will explain what is needed and how this can be implemented. ",
    "url": "/optimist/networking/shared_networks/#motivation",
    
    "relUrl": "/optimist/networking/shared_networks/#motivation"
  },"225": {
    "doc": "Shared Networks",
    "title": "Share Network",
    "content": "If access to both projects is available: . In order to share the network, we needto use the OpenStackClient, the Project ID into which the network is to be shared, as well as the Network ID of the network to be shared. The Project ID can be found in the output under “id” if we use the following command: . openstack project show &lt;Name of Project&gt; -f value -c id . Next, we need the Network ID of the network to be shared. We can find this in the output under “id” if the following command is used: . openstack network show &lt;Name of Network&gt; -f value -c id . With the obtained IDs, the network can now be shared into the corresponding project. To do this, we use Role-Based Access Control (RBAC): . openstack network rbac create --type network --action access_as_shared --target-project &lt;ID of Project&gt; &lt;ID of Network to share&gt; . If access to both projects is not available: . In this case, the network can only be shared by support after the approval of the other project owner. To share a network with a project, please send us an e-mail to support@gec.io with the following information: . | Name and ID of the network to be shared | Name and ID of the project in which the network should be visible | . ",
    "url": "/optimist/networking/shared_networks/#share-network",
    
    "relUrl": "/optimist/networking/shared_networks/#share-network"
  },"226": {
    "doc": "Shared Networks",
    "title": "Important information about shared networks",
    "content": "When accessing a shared network, there are limitations that must be considered. One limitation is that no remote security groups can be used. Additionally, there is no insight into ports and IP addresses from the other project. Therefore, one can also specify any specific IP addresses for new ports in a subnet (in the shared network), as it would be possible to find IPs that are already in use. In order to make use of the shared network, there is the option to create a new port. This then receives a random IP address to use, for example, to add a router through this port. This is not possible in the Horizon dashboard; we need to use the OpenStackClient. Please ensure that no spaces or special characters are used in names, as using these can lead to problems. First, we create the port and specify the shared network there: . openstack port create --network &lt;ID of shared Networks&gt; &lt;Name of Ports&gt; . Now, for example, a router can be created and then mapped to the newly created port: . ##Creation of the router $ openstack router create &lt;Name of Router&gt; ##Assigning a port to the router $ openstack router add port &lt;Name of Router&gt; &lt;Name of Port&gt; . ",
    "url": "/optimist/networking/shared_networks/#important-information-about-shared-networks",
    
    "relUrl": "/optimist/networking/shared_networks/#important-information-about-shared-networks"
  },"227": {
    "doc": "Shared Networks",
    "title": "Network Topology Project 1",
    "content": ". The network “shared” is shared from project 1 to project 2. The service “Example” is available in this network and runs on an instance there. ",
    "url": "/optimist/networking/shared_networks/#network-topology-project-1",
    
    "relUrl": "/optimist/networking/shared_networks/#network-topology-project-1"
  },"228": {
    "doc": "Shared Networks",
    "title": "Network Topology Project 2",
    "content": ". The network “shared” is also visible in project 2 and was attached to the router “router2”. In addition, we have the network “network” from which the services in the network “shared” should be accessed. You have to make sure that the corresponding route is set in the subnet “host route” configuration of the “shared” network in project 1 in order to enable the correct return transport of the packets. In our example the following route is required: 10.0.1.0/24,10.0.0.1 . ",
    "url": "/optimist/networking/shared_networks/#network-topology-project-2",
    
    "relUrl": "/optimist/networking/shared_networks/#network-topology-project-2"
  },"229": {
    "doc": "Octavia Loadbalancers",
    "title": "The Octavia Loadbalancer",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/#the-octavia-loadbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#the-octavia-loadbalancer"
  },"230": {
    "doc": "Octavia Loadbalancers",
    "title": "Preface",
    "content": "Octavia is a highly-available and scalable open source load balancing solution designed to work with OpenStack. Octavia handles load balancing services by managing and configuring a fleet of virtual machines – also known as amphorae – in its project. These amphorae run a HAproxy. ",
    "url": "/optimist/networking/octavia_loadbalancer/#preface",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#preface"
  },"231": {
    "doc": "Octavia Loadbalancers",
    "title": "First Steps",
    "content": "To use Octavia, the client first needs to be installed on your system. Instructions for the installation can be found in Step 04 of our guide. ",
    "url": "/optimist/networking/octavia_loadbalancer/#first-steps",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#first-steps"
  },"232": {
    "doc": "Octavia Loadbalancers",
    "title": "Creating an Octavia-Ladbalancer",
    "content": "In our example we use the example subnet we’ve already created in Step 10. $ openstack loadbalancer create --name Beispiel-LB --vip-subnet-id 32259126-dd37-44d5-922c-99d68ee870cd +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | flavor_id | | id | e94827f0-f94d-40c7-a7fd-b91bf2676177 | listeners | | name | Beispiel-LB | operating_status | OFFLINE | pools | | project_id | b15cde70d85749689e08106f973bb002 | provider | amphora | provisioning_status | PENDING_CREATE | updated_at | None | vip_address | 10.0.0.10 | vip_network_id | f2a8f00e-204b-4c37-9d19-1d5c8e4efbf6 | vip_port_id | 37fc5b34-ee07-49c8-b054-a8d591a9679f | vip_qos_policy_id | None | vip_subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | +---------------------+--------------------------------------+ . Now Octavia spawns amphorae instances in the background. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | PENDING_CREATE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . Once the provisioning_status is ACTIVE, the process has completed successfully and the Octavia load balancer can be further configured. $ openstack loadbalancer list +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | id | name | project_id | vip_address | provisioning_status | provider | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ | e94827f0-f94d-40c7-a7fd-b91bf2676177 | Beispiel-LB | b15cde70d85749689e08106f973bb002 | 10.0.0.10 | ACTIVE | amphora | +--------------------------------------+-------------+----------------------------------+--------------+---------------------+----------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#creating-an-octavia-ladbalancer",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#creating-an-octavia-ladbalancer"
  },"233": {
    "doc": "Octavia Loadbalancers",
    "title": "Create LB listener",
    "content": "In our example, we want to create a listener for HTTP on port 80. A listener here - as in other LB solutions - refers to the port of the front end. $ openstack loadbalancer listener create --name Beispiel-listener --protocol HTTP --protocol-port 80 Beispiel-LB +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2019-05-01T09:00:00 | default_pool_id | None | default_tls_container_ref | None | description | | id | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | insert_headers | None | l7policies | | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | name | Beispiel-listener | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | protocol_port | 80 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | | client_authentication | | client_crl_container_ref | +-----------------------------+--------------------------------------+ . Once the admin_state_up is true the loadbalancer has been successfully created. The Octavia loadbalancer can be further configured at this point. $ openstack loadbalancer listener list +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | id | default_pool_id | name | project_id | protocol | protocol_port | admin_state_up | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | None | Beispiel-listener | b15cde70d85749689e08106f973bb002 | HTTP | 80 | True | +--------------------------------------+-----------------+-------------------+----------------------------------+----------+---------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-lb-listener",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-lb-listener"
  },"234": {
    "doc": "Octavia Loadbalancers",
    "title": "Create LB pool",
    "content": "The LB-pool here refers to a collection of all objects (listeners, members, etc.) - comparable to a pool of public IP addresses from which one can be assigned. A pool for our example is created as follows: . $ openstack loadbalancer pool create --name Beispiel-pool --lb-algorithm ROUND_ROBIN --listener Beispiel-listener --protocol HTTP +----------------------+--------------------------------------+ | Field | Value | +----------------------+--------------------------------------+ | admin_state_up | True | created_at | 2019-05-01T09:00:00 | description | | healthmonitor_id | | id | 4053e88e-c2b5-47c6-987e-4387d837c88d | lb_algorithm | ROUND_ROBIN | listeners | 0a3312d1-8cf7-41a8-8d24-181246468cd7 | loadbalancers | e94827f0-f94d-40c7-a7fd-b91bf2676177 | members | | name | Beispiel-pool | operating_status | OFFLINE | project_id | b15cde70d85749689e08106f973bb002 | protocol | HTTP | provisioning_status | PENDING_CREATE | session_persistence | None | updated_at | None | tls_container_ref | | ca_tls_container_ref | | crl_container_ref | | tls_enabled | +----------------------+--------------------------------------+ . It should be noted that with openstack loadbalancer pool create --help all possible settings can be displayed. The most common settings and their choices: . --protocol: {TCP,HTTP,HTTPS,TERMINATED_HTTPS,PROXY,UDP} --lb-algorithm {SOURCE_IP,ROUND_ROBIN,LEAST_CONNECTIONS} . The pool has been successfully created when the provisioning_status has reached the status ACTIVE. $ openstack loadbalancer pool list +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | id | name | project_id | provisioning_status | protocol | lb_algorithm | admin_state_up | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ | 4053e88e-c2b5-47c6-987e-4387d837c88d | Beispiel-pool | b15cde70d85749689e08106f973bb002 | ACTIVE | HTTP | ROUND_ROBIN | True | +--------------------------------------+---------------+----------------------------------+---------------------+----------+--------------+----------------+ . ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-lb-pool",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-lb-pool"
  },"235": {
    "doc": "Octavia Loadbalancers",
    "title": "Create the LB member",
    "content": "For our loadbalancer to know which backends it is allowed to forward to, we still need a member, which we define as follows: . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.11 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.11 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . and . $ openstack loadbalancer member create --subnet-id 32259126-dd37-44d5-922c-99d68ee870cd --address 10.0.0.12 --protocol-port 80 Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | address | 10.0.0.12 | admin_state_up | True | created_at | 2019-05-01T09:00:00 | id | 2add1e17-73a6-4002-82af-538a3374e5dc | name | | operating_status | NO_MONITOR | project_id | b15cde70d85749689e08106f973bb002 | protocol_port | 80 | provisioning_status | PENDING_CREATE | subnet_id | 32259126-dd37-44d5-922c-99d68ee870cd | updated_at | None | weight | 1 | monitor_port | None | monitor_address | None | backup | False | +---------------------+--------------------------------------+ . It should be noted here that the two IP’s from 10.0.0. * already exist, listening on port 80 (web server), and deliver a simple website with the information about their service name. Assuming these web servers in the following example are Ubuntu/Debian and you have root permissions, you could quickly create a simple web page with: . root@BeispielInstanz1:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver1\" &gt; /var/www/html/index.html . root@BeispielInstanz2:~# apt-get update; apt-get -y install apache2; echo \"you hit: you hit: webserver2\" &gt; /var/www/html/index.html . We can check the status of created members as follows: . $ openstack loadbalancer member list Beispiel-pool +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | id | name | project_id | provisioning_status | address | protocol_port | operating_status | weight | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ | 703e27e0-e7fe-474b-b32d-68f9a8aeef07 | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.11 | 80 | NO_MONITOR | 1 | 2add1e17-73a6-4002-82af-538a3374e5dc | b15cde70d85749689e08106f973bb002 | ACTIVE | 10.0.0.12 | 80 | NO_MONITOR | 1 | +--------------------------------------+------+----------------------------------+---------------------+--------------+---------------+------------------+--------+ . Now the “internal” construct of the loadbalancer is configured. We now have: . | 2 members who provide the actual service via port 80 and between which the loadbalancing takes place, | a pool for this member, | a listener which listens on port TCP/80 and makes a ROUND_ROBIN to the two endpoints and | a loadbalancer, which we used to combine all components. | . The operating_status NO_MONITOR is corrected under healthmonitor. ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-the-lb-member",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-the-lb-member"
  },"236": {
    "doc": "Octavia Loadbalancers",
    "title": "Create and configure the floating IP",
    "content": "In order to be able to use the loadbalancer outside of our example network, we must reserve a floating IP and then link it to the vip_port_id of the example LB. Using the following command we can create a Floating IP from the provider network: . $ openstack floating ip create provider +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2019-05-01T09:00:00Z | description | | dns_domain | None | dns_name | None | fixed_ip_address | None | floating_ip_address | 185.116.247.133 | floating_network_id | 54258498-a513-47da-9369-1a644e4be692 | id | 46c0e8cf-783d-44a0-8256-79f8ae0be7fe | location | Munch({'project': Munch({'domain_id': 'default', 'id': u'b15cde70d85749689e08106f973bb002', 'name': 'beispiel-tenant', 'domain_name': None}), 'cloud': '', 'region_name': 'fra', 'zone': None}) | name | 185.116.247.133 | port_details | None | port_id | None | project_id | b15cde70d85749689e08106f973bb002 | qos_policy_id | None | revision_number | 0 | router_id | None | status | DOWN | subnet_id | None | tags | [] | updated_at | 2019-05-01T09:00:00Z | +---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ . In the next step we need the vip_port_id of the loadbalancer. Accomplished with the following command: . $ openstack loadbalancer show Beispiel-LB -f value -c vip_port_id 37fc5b34-ee07-49c8-b054-a8d591a9679f . With the following command we can now assign the public IP address to the loadbalancer. The LB (and thus also the endpoints behind it) can now be reached from the Internet. openstack floating ip set --port 37fc5b34-ee07-49c8-b054-a8d591a9679f 185.116.247.133 . We are now ready to test our loadbalancer deployment. With the following command we query our loadbalancer via port TCP/80 and then get a corresponding response from the single member: . $ for ((i=1;i&lt;=10;i++)); do curl http://185.116.247.133; sleep 1; done you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 you hit: webserver2 you hit: webserver1 ... (usw.) . ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-and-configure-the-floating-ip",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-and-configure-the-floating-ip"
  },"237": {
    "doc": "Octavia Loadbalancers",
    "title": "Create a healthmonitor",
    "content": "With the following command we create a monitor that, in the event of a failure of one of the backends, can remove a faulty backend from the load distribution, thus allowing the website or application to continue to be delivered cleanly. $ openstack loadbalancer healthmonitor create --delay 5 --max-retries 2 --timeout 10 --type HTTP --name Beispielmonitor --url-path / Beispiel-pool +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | PENDING_CREATE | updated_at | None | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | OFFLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . In this example, the monitor removes the failing backend from the pool if the integrity check (–type HTTP, –url-path / ) fails every two five-second intervals(–delay 5, –max-retries 2, –timeout 10). Once the server is restored and responds to TCP/80 again, it will be added back to the pool. A manual failover can be enforced if status code of the web server is not equal to “200” or in the event that there is no response from the web server at all. $ openstack loadbalancer healthmonitor show Beispielmonitor +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | project_id | b15cde70d85749689e08106f973bb002 | name | Beispielmonitor | admin_state_up | True | pools | 4053e88e-c2b5-47c6-987e-4387d837c88d | created_at | 2019-05-01T09:00:00 | provisioning_status | ACTIVE | updated_at | 2019-05-01T09:00:00 | delay | 5 | expected_codes | 200 | max_retries | 2 | http_method | GET | timeout | 10 | max_retries_down | 3 | url_path | / | type | HTTP | id | 368f9baa-708c-4e7f-ace1-02de15598c5d | operating_status | ONLINE | http_version | | domain_name | +---------------------+--------------------------------------+ . From our deployment example, the result should look something like this: . you hit: webserver1 Mi 22 Mai 2019 17:09:39 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:40 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:41 CEST you hit: webserver2 Mi 22 Mai 2019 17:09:42 CEST you hit: webserver1 Mi 22 Mai 2019 17:09:43 CEST - up to now both webservers were online, but now the webserver2 went offline. you hit: webserver1 Mi 22 Mai 2019 17:09:44 CEST - still expected hit by ROUND_ROBIN you hit: webserver1 Mi 22 Mai 2019 17:09:50 CEST - first retry to webserver2 fails you hit: webserver1 Mi 22 Mai 2019 17:09:56 CEST - second retry to webserver2 fails you hit: webserver1 Mi 22 Mai 2019 17:10:01 CEST - the backend (webserver2) was taken out of the pool. you hit: webserver1 Mi 22 Mai 2019 17:10:02 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:03 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:04 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:05 CEST you hit: webserver1 Mi 22 Mai 2019 17:10:06 CEST . ",
    "url": "/optimist/networking/octavia_loadbalancer/#create-a-healthmonitor",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#create-a-healthmonitor"
  },"238": {
    "doc": "Octavia Loadbalancers",
    "title": "Monitoring with Prometheus",
    "content": "The Octavia amphora driver provides a Prometheus endpoint. This allows you to collect metrics from Octavia load balancers. To add a Prometheus endpoint on an existing Octavia load balancer, create a listener with the protocol PROMETHEUS. This will enable the endpoint as /metrics on the listener. The listener supports all the features of an Octavia load balancer, such as allowed_cidrs, but it does not support attaching pools or L7 policies. All metrics are identified by the Octavia object ID (UUID) of the resources. Note: Currently, UDP and SCTP metrics are not reported via Prometheus endpoints when using the amphora provider. For example, to create a Prometheus endpoint on port 8088 for load balancer lb1, run the following command: . $ openstack loadbalancer listener create --name stats-listener --protocol PROMETHEUS --protocol-port 8088 lb1 +-----------------------------+--------------------------------------+ | Field | Value | +-----------------------------+--------------------------------------+ | admin_state_up | True | connection_limit | -1 | created_at | 2021-10-03T01:44:25 | default_pool_id | None | default_tls_container_ref | None | description | | id | fb57d764-470a-4b6b-8820-627452f55b96 | insert_headers | None | l7policies | | loadbalancers | b081ed89-f6f8-48cb-a498-5e12705e2cf9 | name | stats-listener | operating_status | OFFLINE | project_id | 4c1caeee063747f8878f007d1a323b2f | protocol | PROMETHEUS | protocol_port | 8088 | provisioning_status | PENDING_CREATE | sni_container_refs | [] | timeout_client_data | 50000 | timeout_member_connect | 5000 | timeout_member_data | 50000 | timeout_tcp_inspect | 0 | updated_at | None | client_ca_tls_container_ref | None | client_authentication | NONE | client_crl_container_ref | None | allowed_cidrs | None | tls_ciphers | None | tls_versions | None | alpn_protocols | None | tags | +-----------------------------+--------------------------------------+ . Once the PROMETHEUS listener is ACTIVE, you can configure Prometheus to collect metrics from the load balancer by updating the prometheus.yml file. [scrape_configs] - job_name: 'Octavia LB1' static_configs: - targets: ['192.0.2.10:8088'] . ",
    "url": "/optimist/networking/octavia_loadbalancer/#monitoring-with-prometheus",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#monitoring-with-prometheus"
  },"239": {
    "doc": "Octavia Loadbalancers",
    "title": "Known Issues",
    "content": "If you get the following error when assigning the public IP address to the loadbalancer: . ResourceNotFound: 404: Client Error for url: https://network.fra.optimist.gec.io/v2.0/floatingips/46c0e8cf-783d-44a0-8256-79f8ae0be7fe, External network 54258498-a513-47da-9369-1a644e4be692 is not reachable from subnet 32259126-dd37-44d5-922c-99d68ee870cd. Therefore, cannot associate Port 37fc5b34-ee07-49c8-b054-a8d591a9679f with a Floating IP. Then a connection between its example network (router) and the provider network is missing Step 10 . The default connect settings of the haproxy processes within an amphora is 50 seconds. If a connection lasts longer than 50 seconds, you must configure these values on the listener. An example of a connect with timeout: . $ time kubectl -n kube-system exec -ti machine-controller-5f649c5ff4-pksps /bin/sh ~ $ 50.69 real 0.08 user 0.05 sys . An example extending the timeout to 4h: . openstack loadbalancer listener set --timeout_client_data 14400000 &lt;Listener ID&gt; openstack loadbalancer listener set --timeout_member_data 14400000 &lt;Listener ID&gt; . If Octavia tries to start a LB in a network with port_security_enabled = False, the LB will end up in an ERROR state. ",
    "url": "/optimist/networking/octavia_loadbalancer/#known-issues",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#known-issues"
  },"240": {
    "doc": "Octavia Loadbalancers",
    "title": "Conclusion",
    "content": "It always makes sense to establish a monitor for your pool. ",
    "url": "/optimist/networking/octavia_loadbalancer/#conclusion",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/#conclusion"
  },"241": {
    "doc": "Octavia Loadbalancers",
    "title": "Octavia Loadbalancers",
    "content": " ",
    "url": "/optimist/networking/octavia_loadbalancer/",
    
    "relUrl": "/optimist/networking/octavia_loadbalancer/"
  },"242": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service (VPNaaS)",
    "content": "OpenStack supports on demand Site-to-Site VPNs as a service. This allows the user to connect two private networks to each other. To achieve this, OpenStack will configure a fully functional IPsec VPN within a project, without the need for additional networking VMs. ",
    "url": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas",
    
    "relUrl": "/optimist/networking/vpnaas/#vpn-as-a-service-vpnaas"
  },"243": {
    "doc": "VPN as a Service",
    "title": "Setting up a Site-to-Site IPSec VPN",
    "content": "Create left and right networks and subnets . Before we can create a VPN, we need two separate networks to connect to each other. For this guide, we will create these networks in two different OpenStack projects which will be referred to as “left” and “right”. The following steps have to be repeated for both networks (“left” and “right”), assuming you want to connect two different OpenStack clusters. For the sake of simplicity, this guide will only demonstrate how to create the left network. For OpenStack, the steps to create the right network are exactly the same with exception to naming and subnet prefix In this example, we will be using the subnet prefix 2001:db8:1:33bc::/64 for the left network and 2001:db8:1:33bd::/64 for the right. If you already have two networks you would like to connect via Site-to-Site VPN, you can skip to creating IKE and IPSec policies. Using Horizon (GUI) . | Create the left network with a new subnet. | . Within your project, navigate to Network → Networks and click Create Network. Give your new network a name, select Enable Admin State to enable the network and Create Subnet to create the network and subnet all in one step. Click Next. Assign a name to your new network subnet, select Enter Network Address manually and enter your desired subnet within Network Address, if you would like to use your own subnet. To use a subnet from a predefined pool instead, select Allocate Network Address from a pool and pick a pool. Click Next. For documentation purposes, we will be using our own previously mentioned prefixes. Select Enable DHCP and IPv6 Address Configuration Mode “DHCPV6 STATEFUL”. Allocation pools will be generated automatically. Click Create. Using the CLI . | Create the left network using the openstack network create command. | . $ openstack network create vpnaas-left-network +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:45:42Z | description | | dns_domain | | id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | ipv4_address_scope | None | ipv6_address_scope | None | is_default | False | is_vlan_transparent | None | mtu | 1500 | name | vpnaas-left-network | port_security_enabled | True | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | provider:network_type | None | provider:physical_network | None | provider:segmentation_id | None | qos_policy_id | None | revision_number | 1 | router:external | Internal | segments | None | shared | False | status | ACTIVE | subnets | | tags | | updated_at | 2022-09-12T12:45:42Z | +---------------------------+--------------------------------------+ . | Create a new subnet and allocate it to the newly created network, using the openstack subnet create command. | . $ openstack subnet create \\ vpnaas-left-network-subnet \\ --subnet-range 2001:db8:1:33bc::/64 --ip-version 6 \\ --network vpnaas-left-network +----------------------+--------------------------------------------------------+ | Field | Value | +----------------------+--------------------------------------------------------+ | allocation_pools | 2001:db8:1:33bc::1-2001:db8:1:33bc:ffff:ffff:ffff:ffff | cidr | 2001:db8:1:33bc::/64 | created_at | 2022-09-12T12:47:51Z | description | | dns_nameservers | | dns_publish_fixed_ip | None | enable_dhcp | True | gateway_ip | 2001:db8:1:33bc:: | host_routes | | id | e217a377-48c7-4c18-93b5-cfd805bde40a | ip_version | 6 | ipv6_address_mode | None | ipv6_ra_mode | None | name | vpnaas-left-network-subnet | network_id | ff7c61f1-4dcb-49bf-be9f-efdcaa1e0aaa | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 0 | segment_id | None | service_types | | subnetpool_id | None | tags | | updated_at | 2022-09-12T12:47:51Z | +----------------------+--------------------------------------------------------+ . Create left and right routers . Using Horizon (GUI) . | Create a router with the provider network as an external gateway. | . Within your project, navigate to Network → Routers and click Create Router. Give your new router a name, select Enable Admin State to enable the router and “PROVIDER” as “External Network”. Click Create Router. Using the CLI . | Create a router using the openstack router create command. | . $ openstack router create vpnaas-left-router +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | UP | availability_zone_hints | | availability_zones | | created_at | 2022-09-12T12:48:15Z | description | | enable_ndp_proxy | None | external_gateway_info | null | flavor_id | None | id | 052e968a-a63b-4824-b904-eb70c42c53e5 | name | vpnaas-left-router | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | revision_number | 2 | routes | | status | ACTIVE | tags | | tenant_id | 281fa14f782e4d4cbfd4e34a121c2680 | updated_at | 2022-09-12T12:48:15Z | +-------------------------+--------------------------------------+ . | Use the openstack router set command to set the provider network as an external gateway for the router. | . $ openstack router set vpnaas-left-router --external-gateway provider . Attach the subnet to the router . Using Horizon (GUI) . Within your project, navigate to Network → Routers and select the previously created router. Select Interfaces and click Add Interface. Select your subnet and click Submit. Using the CLI . Use the openstack router add subnet command to add the subnet to the router. $ openstack router add subnet vpnaas-left-router vpnaas-left-network-subnet . Create IKE and IPSec policies on both sides . The IKE and IPSec policies need to be configured identically on both sides. For the purpose of this guide, we will be using the following parameters. | Parameter | IKE Policy | IPSec Policy | . | Authorization algorithm | SHA256 | SHA256 | . | Encryption algorithm | AES-256 | AES-256 | . | Encapsulation mode | N/A | TUNNEL | . | IKE Version | V2 | N/A | . | Perfect Forward Secrecy | GROUP14 | GROUP14 | . | Transform Protocol | N/A | ESP | . Using Horizon (GUI) . | Create the IKE policy | . Within your project, navigate to Network → VPN, select IKE Policies and click Add IKE Policy. Give your IKE policy a name, and fill in the IKE policy parameters. Click Add. | Create the IPSec policy. | . Still within Network → VPN, select IPSec Policies and click Add IPsec Policy. Give your IPSec policy a name, and fill in the IPSec policy parameters. Click Add. Using the CLI . | Create the IKE policy using the openstack vpn ike policy create command. | . $ openstack vpn ike policy create \\ vpnaas-left-ike-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --ike-version v2 \\ --pfs group14 +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encryption Algorithm | aes-256 | ID | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IKE Version | v2 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ike-policy | Perfect Forward Secrecy (PFS) | group14 | Phase1 Negotiation Mode | main | Project | 281fa14f782e4d4cbfd4e34a121c2680 | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . | Create the IPSec policy using the openstack vpn ipsec policy create command. | . $ openstack vpn ipsec policy create \\ vpnaas-left-ipsec-policy \\ --auth-algorithm sha256 \\ --encryption-algorithm aes-256 \\ --pfs group14 \\ --transform-protocol esp +-------------------------------+--------------------------------------+ | Field | Value | +-------------------------------+--------------------------------------+ | Authentication Algorithm | sha256 | Description | | Encapsulation Mode | tunnel | Encryption Algorithm | aes-256 | ID | 553a600e-f39d-47a0-9550-97f2b4033685 | Lifetime | {'units': 'seconds', 'value': 3600} | Name | vpnaas-left-ipsec-policy | Perfect Forward Secrecy (PFS) | group14 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Transform Protocol | esp | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------------------------+--------------------------------------+ . Create the VPN service on both sides . Using Horizon (GUI) . Within your project, navigate to Network → VPN, select VPN Services and click Add VPN Service. Give your VPN service a name, select your router and Enable Admin State. A subnet is not needed as we will be using endpoint groups. Click Add. Using the CLI . Use the openstack vpn service create command to create the VPN service. $ openstack vpn service create vpnaas-left-vpn --router vpnaas-left-router +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | Description | | Flavor | None | ID | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | Name | vpnaas-left-vpn | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Router | 052e968a-a63b-4824-b904-eb70c42c53e5 | State | True | Status | PENDING_CREATE | Subnet | None | external_v4_ip | 185.116.244.85 | external_v6_ip | 2a00:c320:1003::23a | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +----------------+--------------------------------------+ . Create the endpoint groups . When using multiple subnets, make sure your VPN endpoint supports routing multiple subnets through the same connection. While OpenStack does, for implementations that do not support this, multiple endpoint groups need to be created, one for each subnet. Using Horizon (GUI) . | Create the local endpoint group for the left side. | . Within your project, navigate to Network → VPN, select Endpoint Groups and click Add Endpoint Group. Give your endpoint group a name, select the Type “Subnet” and select your subnet under Local System Subnets. Click Add. | Create the peer endpoint group for the left side. | . Still within Network → VPN, Endpoint Groups, click Add Endpoint Group again. Give your endpoint group a name, select the Type “CIDR” and enter the subnet of the right side. Click Add. Using the CLI . | Use the openstack vpn endpoint group create command to create the local endpoint group for the left side. | . $ openstack vpn endpoint group create \\ vpnaas-left-local \\ --type subnet \\ --value vpnaas-left-network-subnet +-------------+------------------------------------------+ | Field | Value | +-------------+------------------------------------------+ | Description | | Endpoints | ['e217a377-48c7-4c18-93b5-cfd805bde40a'] | ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Name | vpnaas-left-local | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | subnet | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+------------------------------------------+ . | Use the openstack vpn endpoint group create command again to create the peer endpoint group for the left side. | . $ openstack vpn endpoint group create \\ vpnaas-left-remote \\ --type cidr \\ --value 2001:db8:1:33bd::/64 +-------------+--------------------------------------+ | Field | Value | +-------------+--------------------------------------+ | Description | | Endpoints | ['2001:db8:1:33bd::/64'] | ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Name | vpnaas-left-remote | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Type | cidr | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +-------------+--------------------------------------+ . Create the site connections . Just like with the endpoint groups, if your VPN endpoint does not support routing multiple subnets through the same connection, you need to create multiple site connections, one for each subnet/endpoint group. Using Horizon (GUI) . Within your project, navigate to Network → VPN, select IPSec Site Connections and click Add IPSec Site Connection. Give your connection a name, select the previously created VPN service, local endpoint group, IKE and IPSec policy, Pre-Shared Key, Peer IP and router identity. For the purposes of this guide, we will assume that 2001:db8::4:703 is the IP address of the right router. Using the CLI . Use the openstack vpn ipsec site connection create command to create the VPN service. $ openstack vpn ipsec site connection create \\ vpnaas-left-connection \\ --vpnservice vpnaas-left-vpn \\ --ikepolicy vpnaas-left-ike-policy \\ --ipsecpolicy vpnaas-left-ipsec-policy \\ --local-endpoint-group vpnaas-left-local \\ --peer-address 2001:db8::4:703 \\ --peer-id 2001:db8::4:703 \\ --peer-endpoint-group vpnaas-left-remote \\ --psk 1gHAsAeR8lFEDDu7 +--------------------------+----------------------------------------------------+ | Field | Value | +--------------------------+----------------------------------------------------+ | Authentication Algorithm | psk | Description | | ID | d81dbe28-ccda-4ee3-ba96-145fadc74e0f | IKE Policy | 561387b8-b5c1-415e-abc9-79ba93dd48ff | IPSec Policy | 553a600e-f39d-47a0-9550-97f2b4033685 | Initiator | bi-directional | Local Endpoint Group ID | 949ccc53-5dc6-457d-95bf-278fdf9a3e5d | Local ID | | MTU | 1500 | Name | vpnaas-left-connection | Peer Address | 2001:db8::4:703 | Peer CIDRs | | Peer Endpoint Group ID | 9146346d-1306-4b03-a3ce-04ee51832ed8 | Peer ID | 2001:db8::4:703 | Pre-shared Key | 1gHAsAeR8lFEDDu7 | Project | 281fa14f782e4d4cbfd4e34a121c2680 | Route Mode | static | State | True | Status | PENDING_CREATE | VPN Service | cc258fd7-0e87-4058-ad7d-355f32c1ab5e | dpd | {'action': 'hold', 'interval': 30, 'timeout': 120} | project_id | 281fa14f782e4d4cbfd4e34a121c2680 | +--------------------------+----------------------------------------------------+ . ",
    "url": "/optimist/networking/vpnaas/#setting-up-a-site-to-site-ipsec-vpn",
    
    "relUrl": "/optimist/networking/vpnaas/#setting-up-a-site-to-site-ipsec-vpn"
  },"244": {
    "doc": "VPN as a Service",
    "title": "VPN as a Service",
    "content": " ",
    "url": "/optimist/networking/vpnaas/",
    
    "relUrl": "/optimist/networking/vpnaas/"
  },"245": {
    "doc": "Storage",
    "title": "Storage",
    "content": " ",
    "url": "/optimist/storage/",
    
    "relUrl": "/optimist/storage/"
  },"246": {
    "doc": "S3 Compatible Object Storage",
    "title": "S3 Compatible Object Storage Introduction",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/#s3-compatible-object-storage-introduction",
    
    "relUrl": "/optimist/storage/s3_documentation/#s3-compatible-object-storage-introduction"
  },"247": {
    "doc": "S3 Compatible Object Storage",
    "title": "What exactly is Object Storage?",
    "content": "Object storage is an alternative to classic block storage. The individual data is not assigned to individual blocks on a device, but is stored as binary objects within a storage cluster. The necessary metadata is stored in a separate database. The storage cluster consists of several individual servers (nodes), which in turn have several storage devices installed. With the storage devices, we have a mix of classic HDDs, SSDs, and modern NVMe solutions. The CRUSH algorithm, which is implemented on the server-side of the Object Storage, decides which device an object ultimately lands on. ",
    "url": "/optimist/storage/s3_documentation/#what-exactly-is-object-storage",
    
    "relUrl": "/optimist/storage/s3_documentation/#what-exactly-is-object-storage"
  },"248": {
    "doc": "S3 Compatible Object Storage",
    "title": "How can I access it?",
    "content": "Access to this type of storage is accomplished exclusively via HTTPS. For this, we provide a highly available endpoint, where the individual operations can be executed. We support two independent protocols: . | S3 | Swift | . S3 is a protocol created by Amazon in order to work with this type of data. Swift is the protocol provided by the OpenStack service of the same name. Regardless of which protocol you use, you always have access to all your data. You can, therefore, use both protocols in combination. There are tools for all common platforms to work with the data in the object storage: . | Windows: s3cmd, Cyberduck | MacOS: s3cmd, Cyberduck | Linux: s3cmd | . In addition, there are integrations in all popular programming languages. ",
    "url": "/optimist/storage/s3_documentation/#how-can-i-access-it",
    
    "relUrl": "/optimist/storage/s3_documentation/#how-can-i-access-it"
  },"249": {
    "doc": "S3 Compatible Object Storage",
    "title": "How secure is my data?",
    "content": "The Object Storage is based on our Openstack Cloud Platform with the distributed Ceph Storage Cluster. The objects are distributed and replicated on the server-side across several storage devices. Ceph ensures the replication and integrity of the data sets. If a server or hard disk fails, the affected data records are replicated to available servers and the desired replication level is automatically restored. In addition, the data is mirrored to another data centre on another dedicated storage cluster and can be used from there in the event of a disaster. ",
    "url": "/optimist/storage/s3_documentation/#how-secure-is-my-data",
    
    "relUrl": "/optimist/storage/s3_documentation/#how-secure-is-my-data"
  },"250": {
    "doc": "S3 Compatible Object Storage",
    "title": "Advantages at a glance",
    "content": ". | Deployment via API: The HTTPS interface is compatible with both the Amazon S3 API and the OpenStack Swift API. | Supports all common operating systems and programming languages. | Full scalability - Storage can be used dynamically. | Maximum reliability thanks to integrated replication and mirroring via 2 independent data centres. | Access is possible from almost any internet-enabled device - thus a good alternative to NFS and Co. | PAYG billing according to used monthly average. | Transparent billing and therefore good predictability - no extra traffic costs or costs for access to the data. | Ability to define s3 lifecycle policies to manage the objects inside the buckets. | . ",
    "url": "/optimist/storage/s3_documentation/#advantages-at-a-glance",
    
    "relUrl": "/optimist/storage/s3_documentation/#advantages-at-a-glance"
  },"251": {
    "doc": "S3 Compatible Object Storage",
    "title": "S3 Compatible Object Storage",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/",
    
    "relUrl": "/optimist/storage/s3_documentation/"
  },"252": {
    "doc": "Create and Use S3 Credentials",
    "title": "Create and Use S3 Credentials",
    "content": "Contents: . | Create S3 credentials | Entering User Data in the Configuration File . | S3cmd | S3Browser | Cyberduck | Boto3 | . | Show s3 credentials | Delete s3 credentials | . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/"
  },"253": {
    "doc": "Create and Use S3 Credentials",
    "title": "Create S3 credentials",
    "content": "In order to access Object Storage, we first need login data (credentials). To generate this data via the OpenStackAPI, we need to use the OpenStack Client and execute the following command there: . $ openstack ec2 credentials create . If the data has been created correctly, the output will be similar to the below: . $ openstack ec2 credentials create +------------+-----------------------------------------------------------------+ | Field | Value | +------------+-----------------------------------------------------------------+ | access | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | links | {u'self': u'https://identity.optimist.gec.io/v3/users/bbb | | bbbbbbbbbbbbbbbbbbbbbbbbbbbbb/credentials/OS- | | EC2/aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'} | project_id | cccccccccccccccccccccccccccccccc | secret | dddddddddddddddddddddddddddddddd | trust_id | None | user_id | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | +------------+-----------------------------------------------------------------+ . Once the credentials are available, we need a way to access the S3 compatible ObjectStorage. For this, there are different options, in this documentation we present 3 possibilities: S3cmd for Linux/Mac, S3Browser for Windows, Cyberduck and Boto3. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#create-s3-credentials",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#create-s3-credentials"
  },"254": {
    "doc": "Create and Use S3 Credentials",
    "title": "Entering User Data in the Configuration File",
    "content": "S3cmd . To install s3cmd, we need a package manager such as “pip”. The installation and usage is explained in Step 4: “Our way to the console” of our Guided Tour. Once pip is installed, the command for the installation of S3cmd is then: . $ pip install s3cmd . After the successful installation of S3cmd, the previously created credentials must be entered into S3cmd configuration file. The file responsible for this is “.s3cfg”, which is located in the home directory by default. If this file does not yet exist, it must first be created. We then enter the following data in the .s3cfg and save it: . access_key = aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa check_ssl_certificate = True check_ssl_hostname = True host_base = s3.es1.fra.optimist.gec.io host_bucket = s3.es1.fra.optimist.gec.io secret_key = dddddddddddddddddddddddddddddddd use_https = True . S3Browser . The S3Browser can be downloaded here and easily installed afterwards. After this has been successfully installed, we then need to enter all necessary credentials. To do this, we open the S3Browser and the following window opens automatically the first time we run the application: . Here we enter the following values and click on “Add new account”. * Account Name: Freely selectable name for the account. * Account Type: S3 Compatible Storage * REST Endpoint: s3.es1.fra.optimist.gec.io * Signature Version: Signature V2 * Access Key ID: The corresponding Access Key (in our example: ddddddddddddddddddddddddddddddddddddd) * Secret Access Key: The corresponding secret (in our example: ddddddddddddddddddddddddddddddddddddd) . Cyberduck . To use Cyberduck it is first necessary to download the application here. After installing and running the program for the first time, click on “New connection”. (1) A new window opens in which “Amazon S3” is selected in the dropdown menu (2). The following data is then required: . | Server(3): s3.es1.fra.optimist.gec.io | Access Key ID(4): The corresponding Access Key (in our example: ddddddddddddddddddddddddddddddddddddd) | Secret Access Key(5): The corresponding Secret (In the example: dddddddddddddddddddddddddddddddddd) | . Finally, to establish a connection, click on “Connect”. Boto3 . To install boto3, we need a package manager such as “pip”. The installation and usage of pip is explained in Step 4: “Our way to the console” of our Guided Tour. Once pip is installed, the command for the installation of Boto3 is then: . $ pip install boto3 . After the successful installation of Boto3 it is now usable; it is important that Boto3 creates a script which is executed at the end. Therefore, the configuration section which is shown below, is always part of subsequent scripts used later. For this we create a Python file such as “Example.py” and add the following content: . | endpoint_url: s3.es1.fra.optimist.gec.io | aws_access_key_id: The corresponding Access Key (in our example: dddddddddddddddddddddddddddddddddddd) | aws_secret_access_key: The corresponding Secret (In the example: dddddddddddddddddddddddddddddddddd) | . #!/usr/bin/env/python import boto3 from botocore.client import Config s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='dddddddddddddddddddddddddddddddd', ) . This serves as a starting point and is referenced and used in the following scripts. ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#entering-user-data-in-the-configuration-file",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#entering-user-data-in-the-configuration-file"
  },"255": {
    "doc": "Create and Use S3 Credentials",
    "title": "Show s3 credentials",
    "content": "In order to show existing Object Storage ec2-credentials we need to use the OpenStack Client and execute the following command there: . $ openstack ec2 credentials list . the output will be similar to the below: . $ openstack ec2 credentials list +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | Access | Secret | Project ID | User ID | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ | aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx | 12341234123412341234123412341234 | 32132132132132132132132132132132 | bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb | yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy | 56756756756756756756756756756756 | 65465465465465465465465465465465 | cccccccccccccccccccccccccccccccc | zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz | 89089089089089089089089089089089 | 09809809809809809809809809809809 | +----------------------------------+----------------------------------+----------------------------------+----------------------------------+ . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#show-s3-credentials",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#show-s3-credentials"
  },"256": {
    "doc": "Create and Use S3 Credentials",
    "title": "Delete s3 credentials",
    "content": "In order to delete Object Storage ec2-credentials we need to use the OpenStack Client and execute the following command there: . $ openstack ec2 credentials delete &lt;access-key&gt; . ",
    "url": "/optimist/storage/s3_documentation/createanduses3credentials/#delete-s3-credentials",
    
    "relUrl": "/optimist/storage/s3_documentation/createanduses3credentials/#delete-s3-credentials"
  },"257": {
    "doc": "Create and Delete a Bucket",
    "title": "Create and Delete a Bucket",
    "content": "Contents: . | S3cmd | S3Browser | Cyberduck | Boto3 | . To upload your data (documents, photos, videos, etc.) it is necessary to create a bucket, which is similar to a folder. First create an S3 bucket and then you can upload as many objects as required into the bucket. Due to the way our object storage works, it is necessary to use a globally unique name for your bucket. If a bucket with the selected name already exists, the name cannot be used until the existing bucket has been deleted. If the desired name is already in use by another customer, you must choose another name. It is advisable to use names of the format “content-description.bucket.my-domain.tld” or similar. ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/"
  },"258": {
    "doc": "Create and Delete a Bucket",
    "title": "S3cmd",
    "content": "Create a bucket . To create a bucket, use the following command: . s3cmd mb s3://NameOfTheBucket . The output in the command line will look similar to this: . $ s3cmd mb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' created . Delete a bucket . To delete a bucket, use the following command: . s3cmd rb s3://NameOfTheBucket . The output in the command line will look similar to this: . $ s3cmd rb s3://iNNOVO-Test Bucket 's3://iNNOVO-Test/' removed . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3cmd"
  },"259": {
    "doc": "Create and Delete a Bucket",
    "title": "S3Browser",
    "content": "Create a bucket . After opening S3Browser, we click on “New bucket”(1) in the upper left corner, in the newly opened window, we assign the name of the bucket via “Bucket name”(2) and then click on “Create new bucket”(3). Delete a bucket . First select the bucket you want to delete(1) and then click on “Delete bucket”(2) in the upper left corner. In the window that opens, confirm that you want to delete the file by checking the checkbox(1) and then click on “Delete Bucket”(2). ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#s3browser"
  },"260": {
    "doc": "Create and Delete a Bucket",
    "title": "Cyberduck",
    "content": "Create a bucket . After opening Cyberduck, we click on “Action”(1) and on “New folder”(2) in the middle of the top. A new window opens, here we can define the name(1) and confirm this with “Create”(2): . Delete a bucket . To delete a bucket, select it with a left mouse click. The bucket is then deleted via “Action”(1) and “Delete”(2). Confirm the action by clicking on “Delete”(1) once again. ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#cyberduck"
  },"261": {
    "doc": "Create and Delete a Bucket",
    "title": "Boto3",
    "content": "In Boto3 we first need the S3 identifier so that a script can be used. For details see: Create and use S3 credentials #Boto3 . Create a bucket . To create a bucket, we first need a client for it and we will then create the bucket afterwards. One option looks like this: . ## Create the S3 client s3 = boto3.client('s3') ## Create a bucket s3.create_bucket(Bucket='iNNOVO-Test') . A complete script for boto 3 including authentication may look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Create the S3 client s3 = boto3.client('s3') ## Create a bucket s3.create_bucket(Bucket='iNNOVO-Test') . Delete a bucket . As before, we first need a client to delete the bucket. One option looks like this: . ## Create the S3 client s3 = boto3.client('s3') ## Delete a bucket s3.delete_bucket(Bucket='iNNOVO-Test') . A complete script for boto 3 including authentication may look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Create the S3 client s3 = boto3.client('s3') ## Delete a bucket s3.delete_bucket(Bucket='iNNOVO-Test') . ",
    "url": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/createanddeletebucket/#boto3"
  },"262": {
    "doc": "Upload and delete an object",
    "title": "Upload and delete an object",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/"
  },"263": {
    "doc": "Upload and delete an object",
    "title": "Contents:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . To upload your data (documents, photos, videos, etc.) it is first necessary to create a bucket. A file can only be saved in a bucket. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#contents",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#contents"
  },"264": {
    "doc": "Upload and delete an object",
    "title": "S3cmd",
    "content": "Upload an object . To upload a file, use the following command: . s3cmd put NameOfTheFile s3://NameOfTheBucket/NameOfTheFile . The output in the command will be similar to this: . $ s3cmd put innovo.txt s3://innovo-test/innovo.txt upload: 'innovo.txt' -&gt; 's3://innovo-test/innovo.txt' [1 of 1] 95 of 95 100% in 0s 176.63 B/s done . Delete an object . To delete a file, use the following command: . s3cmd del s3://NameOfTheBucket/NameOfTheFile . The output in the command will be similar to this: . $ s3cmd del s3://innovo-test/innovo.txt delete: 's3://innovo-test/innovo.txt' . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3cmd"
  },"265": {
    "doc": "Upload and delete an object",
    "title": "S3Browser",
    "content": "Upload an object . After opening S3Browser, we click on the desired “Bucket”(1), then select “Upload”(2) and finally “Upload file(s)”(3) . Here we select the file(1) and click on Open(2). Delete an object . To delete a file, select it with a left mouse click(1). Then click on “Delete”(2). Finally, confirm the action with “Yes”. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#s3browser"
  },"266": {
    "doc": "Upload and delete an object",
    "title": "Cyberduck",
    "content": "Upload an object . After opening Cyberduck, click on the Bucket(1), then click on Action(2) and then on Upload(3). Here we choose our file and click on Upload. Delete an object . To delete a file, select it with a left mouse click(1). It is then deleted via “Action”(2) and “Delete”(3). This action is then confirmed by clicking on “Delete” again. ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#cyberduck"
  },"267": {
    "doc": "Upload and delete an object",
    "title": "Boto3",
    "content": "At boto3 we first need the S3 identifier so that a script can be used. For details: Create and use S3 credentials #Boto3. Upload an object . To upload a file, we have to use a client and specify the bucket which the file should be uploaded to. One option could look like this: . ## Create the S3 client s3 = boto3.client('s3') ## Upload an object s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . A complete script for boto 3 including authentication may be similar to this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Create the S3 client s3 = boto3.client('s3') ## Upload an object s3.upload_file(Bucket='iNNOVO-Test', Key='innovo.txt') . Delete an object . As well as being used to upload a file, the client is also required to delete the file. For this, we specify the bucket in which the file is stored, in addition to the file itself. One option could look like this: . ## Create the S3 client s3 = boto3.client('s3') ## Delete an object s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . A complete script for boto 3 including authentication may look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Create the S3 client s3 = boto3.client('s3') ## Delete an object s3.delete_object(Bucket='iNNOVO-Test', Key='innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/uploadanddeleteobject/#boto3"
  },"268": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "Enable and Disable Versioning, and Delete a versioned object",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/versioning/",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/"
  },"269": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "Contents:",
    "content": ". | S3cmd | S3Browser | Cyberduck | Boto3 | . Versioning makes it possible to store multiple versions of an object in a bucket. For example, files named innovo.txt (version 1) and innovo.txt (version 2) can be stored in a single bucket. Versioning can protect you from the consequences of accidental overwrites or deletion. ",
    "url": "/optimist/storage/s3_documentation/versioning/#contents",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#contents"
  },"270": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "S3cmd",
    "content": "With S3cmd it is not possible to enable versioning or to delete versioned files. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3cmd",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3cmd"
  },"271": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "S3Browser",
    "content": "Enable versioning . To enable versioning, select a Bucket(1). Right click on the bucket and then click on “Edit Versioning Settings”(2). In the newly opened window, click the checkbox for “Enable versioning for bucket”(1) and confirm this with “OK”(2). Disable versioning . To disable versioning, select a Bucket(1). Right-click on the bucket and select “Edit Versioning Settings”(2). In the newly opened window, remove the checkbox at “Enable versioning for bucket”(1) and confirm this with “OK”(2). Delete a versioned object . This is not possible in the free version of S3Browser. ",
    "url": "/optimist/storage/s3_documentation/versioning/#s3browser",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#s3browser"
  },"272": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "Cyberduck",
    "content": "To see the different versions of a file, hidden files must be displayed. This option can be found at View(1) → Show hidden files(2) . Enable versioning . After opening Cyberduck, we select the file on which we wish to activate versioning(1) for. Then click Action(2) and Info(3). Then the following window opens, here we check the box “Bucket Versioning”(1): . Disable versioning . To disable versioning, we select the file(1) again, go to Action(2) and Info(3). In the window that opens, the check mark for “Bucket Versioning” should be removed. Delete a versioned object . Simply select the file to be deleted(1) and click Action(2) → Delete(3) to remove it. ",
    "url": "/optimist/storage/s3_documentation/versioning/#cyberduck",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#cyberduck"
  },"273": {
    "doc": "Enable and Disable Versioning, and Delete a versioned object",
    "title": "Boto3",
    "content": "In Boto3 we first need S3 credentials so that a script can be used. For details see: Create and use S3 credentials #Boto3. Enable versioning . To enable the versioning, we will enter the bucket first and then activate the versioning. One option looks like this: . ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Activate versioning bucket.configure_versioning(True) . A complete script for boto 3, including authentication, could look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Activate versioning bucket.configure_versioning(True) . Disable versioning . As with the activation of versioning, the bucket is needed to deactivate versioning. One option looks like this: . ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Deactivate versioning bucket.configure_versioning(False) . A complete script for boto 3 including authentication could look like this: . #!/usr/bin/env/python ## Define that boto3 should be used import boto3 from botocore.client import Config ## Authentication s3 = boto3.resource('s3', endpoint_url='https://s3.es1.fra.optimist.gec.io', aws_access_key_id='aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', aws_secret_access_key='bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb', ) ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Deactivate versioning bucket.configure_versioning(False) . Delete a versioned object . To delete a versioned object completely, the following command is helpful: . ## Specifies the bucket in which versioning is to be activated. bucket = s3.Bucket('iNNOVO-Test') ## Delete versioned object bucket.object_versions.all().delete('innovo.txt') . ",
    "url": "/optimist/storage/s3_documentation/versioning/#boto3",
    
    "relUrl": "/optimist/storage/s3_documentation/versioning/#boto3"
  },"274": {
    "doc": "S3 Security",
    "title": "S3 Security",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/security/",
    
    "relUrl": "/optimist/storage/s3_documentation/security/"
  },"275": {
    "doc": "S3 Security",
    "title": "Introduction",
    "content": "This page gives an overview of the following topics relating to S3 buckets / Swift: . | Container Access Control Lists (ACLs) | Bucket Policies | . Operations on container ACLs must be performed at the OpenStack level using Swift commands, while Bucket Policies must be set on each bucket within a project using the s3cmd command line. In this document we will outline some examples of each type of operation. ",
    "url": "/optimist/storage/s3_documentation/security/#introduction",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#introduction"
  },"276": {
    "doc": "S3 Security",
    "title": "Container Access Control Lists (ACLs)",
    "content": "By default, only project owners have permission to create, read and modify containers and objects. However, an owner can grant access to other users on by using an Access Control List (ACL). The ACL can be set on each container, and is applicable only to that container and the objects within that container. Some of the main elements which can be used to set an ACL on a container are listed below: . | Element | Description | . | .r:*. | Any user has access to objects. No token is required in the request. | . | .r:&lt;referrer&gt; | The referrer is granted access to objects. The referrer is identified by the Referer request header in the request. No token is required. | . | .r:-&lt;referrer&gt; | This syntax (with “-” prepended to the referrer) is supported. However, it does not deny access if another element (e.g., .r:*) grants access. | . | .rlistings | Any user can perform HEAD or GET operations on the container if the user also has read access on objects (e.g., also has .r:* or .r:&lt;referrer&gt;. No token is required. | . As an example, we will set the policy .r:*. on a container called &lt;example-container&gt;. This policy will allow any external user access to the objects within the container. swift post example-container --read-acl \".r:*\" . Conversely, we can also allow users to list but not access the list of objects within a container by setting the .rlistings policy on our example-container: . swift post example-container --read-acl \".rlistings\" . To remove any read policy and set the container to its default private state, the following command can be used: . swift post -r \"\" example-container . To check which ACL is set on a container, use the following command. swift stat example-container . This gives an overview of the stats for the container and displays the current ACL rule for a container. Prevent Listing on Containers when using the .r:*. policy: . In the current version of OpenStack, to prevent the contents from being listed while using the .r:*. policy on a container, we recommend creating an empty index.html object within the container. This will allow users to download objects without listing the contents of the buckets. This can be accomplished with the following steps: . Firstly, add the blank index.html file to our example-container: . swift post -m 'web-index: index.html’ example-container . Then create the index.html file as an object within the container: . touch index.html &amp;&amp; openstack object create example-container index.html . This will allow external users access to specific files without listing the contents of the container. ",
    "url": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#container-access-control-lists-acls"
  },"277": {
    "doc": "S3 Security",
    "title": "Bucket Policies",
    "content": "Bucket policies are used to control access to each bucket in a project. It is recommended to set a policy on all buckets upon creation. The first step is to create a policy as follows. The following template only requires that you change the bucket name for subsequent policies, The below example creates a policy for bucket example-bucket: . cat &gt; examplepolicy { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Sid\": \"AddPerm\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::example-bucket/*\" } ] } . Breakdown of each element within the above policy example: . | Version: Specifies the language syntax rules to be used to process the policy. It is recommended to always use: “2012-10-17” as this is the current version of the policy language. | Statement: The main element of the policy, the other elements are located within this statement. | SID: The Statement ID, this is an optional identifier which can be used to describe the policy statement. Recommended so that the purpose of each policy is clear. | Effect: Set either to “Allow” or “Deny” | Principal: Specifies the principal that is allowed or denied access to a resource. Here, the wildcard “*” is used to apply the rule to all. | Action: Describes the specific actions that will be allowed or denied. | . (For further information on the available policy options and how to tailor this to your specific needs, please see the official AWS documentation). Next, apply the newly created policy to bucket example-bucket: . s3cmd setpolicy examplepolicy s3://example-bucket . You will also be able to run the following command afterwards to see that the policy is in place: . s3cmd info s3://example-bucket . Once the policy is applied, you can set once again set Public Access: Disabled on the Dashboard. Once the above steps have been taken we will have the following results: . | The container will be private, and files will not be listed or displayed via XML. | The policy now allows access to specific files with a direct link. | . ",
    "url": "/optimist/storage/s3_documentation/security/#bucket-policies",
    
    "relUrl": "/optimist/storage/s3_documentation/security/#bucket-policies"
  },"278": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving Static Websites",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#swift---serving-static-websites"
  },"279": {
    "doc": "Swift - Serving a Static Website",
    "title": "Introduction",
    "content": "Using the Swift command line, it is possible to serve the data in containers as a static website. The following guide will outline the main steps to get started, as well as including an example of a website. ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#introduction",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#introduction"
  },"280": {
    "doc": "Swift - Serving a Static Website",
    "title": "First Steps",
    "content": "Create a container . We will first create a container named example-webpage which we will use as the basis of this guide: . swift post example-webpage . Make the container publically readable . Next, we must ensure that the container is publically readable. You can learn more about securing containers and setting bucket policies here: . swift post -r '.r:*' example-webpage . Set site index file . Set the index file. In this case, index.html will be the default file displayed when the site appears: . swift post -m 'web-index:index.html' example-webpage . Enable file listing . Optionally, we can also enable file listing. If you need to provide multiple downloads, enabling the directory listing makes sense: . swift post -m 'web-listings: true' example-webpage . Enable CSS for file listing . Enable a custom listings style sheet: . swift post -m 'web-listings-css:style.css' example-webpage . Set error pages . Finally, we should include a custom error page: . swift post -m 'web-error:404error.html' example-webpage . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#first-steps",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#first-steps"
  },"281": {
    "doc": "Swift - Serving a Static Website",
    "title": "Example Webpage",
    "content": "Let’s recap the steps we have taken so far to enable static webpages: . swift post example-webpage swift post -r '.r:*' example-webpage swift post -m 'web-index:index.html' example-webpage swift post -m 'web-listings: true' example-webpage swift post -m 'web-listings-css:style.css' example-webpage swift post -m 'web-error:404error.html' example-webpage . Once the steps above have been completed, we can now begin to customise our static webpage. The following demonstrates a quick setup using our container example-webpage . Customising index.html, page.html, and 404error.html pages . This will serve as the homepage, which will create a link to a secondary page. &lt;!-- index.html --&gt; &lt;html&gt; &lt;h1&gt; See the web page &lt;a href=\"mywebsite/page.html\"&gt;here&lt;/a&gt;. &lt;/h1&gt; &lt;/html&gt; . The next page (page.html) will display an image called sample.png: . &lt;!-- page.html --&gt; &lt;html&gt; &lt;img src=\"sample.png\"&gt; &lt;/html&gt; . We can also add custom error pages. Note that currently only 401 (Unauthorized) and 404 (Not Found) errors are supported. The following example demonstrates the creation of a 404 Error page: . &lt;!-- 404error.html --&gt; &lt;html&gt; &lt;h1&gt; 404 Not Found - We cannot find the page you are looking for! &lt;/h1&gt; &lt;/html&gt; . Upload the index.html and page.html files . Once the contents of the files have been added, upload the files with the following commands: . swift upload example-webpage index.html swift upload example-webpage mywebsite/page.html swift upload example-webpage mywebsite/sample.png swift upload example-webpage 404error.html . Viewing the website . Once all of the above steps have been completed, we can now view our newly created website. The link to the website can be found on the Optimist Dashboard &gt; Object Store &gt; Containers using the link shown. Clicking on the link displays our newly created website: . Click on “here” to navigate to the page where we uploaded our sample image: . In the event that we try to navigate to a page which does not exist, our custom 404 page will be displayed: . ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#example-webpage",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/#example-webpage"
  },"282": {
    "doc": "Swift - Serving a Static Website",
    "title": "Swift - Serving a Static Website",
    "content": " ",
    "url": "/optimist/storage/s3_documentation/swiftservestaticwebsite/",
    
    "relUrl": "/optimist/storage/s3_documentation/swiftservestaticwebsite/"
  },"283": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Problem statement",
    "content": "When performing file-level backups of a node into S3, the backup software needs write permissions for the S3 bucket. But if an attacker gains access to the machine, he can also destroy the backups in the bucket, since the S3 credentials are present on the compromised system. The solution can be as simple as limiting the level of access of the backup software to the bucket. Unfortunately, this isn’t trivial with S3. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#problem-statement",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#problem-statement"
  },"284": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Background",
    "content": "S3 access control lists (ACLs) enable you to manage access to buckets and objects, but they have limitations. They essentially differentiate READ and WRITE permissions: . | READ - Allows grantee to list the objects in the bucket | WRITE - Allows grantee to create, overwrite, and delete any object in the bucket | . The limitations of ACLs were addressed by the access policy permissions (ACP). We can attach a no-delete policy to the bucket, e.g. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"nodelete1\", \"Effect\": \"Deny\", \"Action\": [ \"s3:DeleteBucket\", \"s3:DeleteBucketPolicy\", \"s3:DeleteBucketWebsite\", \"s3:DeleteObject\", \"s3:DeleteObjectVersion\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] } ] } . Unfortunately, the S3 protocol itself wasn’t designed with the concept of WORM (write once read many) backups in mind. Access policy permissions do not differentiate between changing an existing object (which would effectively allow deleting it) and creating a new object. Attaching the above policy on a bucket does not prevent the objects in it from being overwritten. $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.46 KB/s done # policy allows write $ s3cmd rm s3://appendonly-bucket/testfile ERROR: S3 error: 403 (AccessDenied) # policy denies deletion $ $ s3cmd put testfile s3://appendonly-bucket/testfile upload: 'testfile' -&gt; 's3://appendonly-bucket/testfile' [1 of 1] 1054 of 1054 100% in 0s 5.50 KB/s done # :( . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#background",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#background"
  },"285": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Proposed solution",
    "content": "Since an attacker on a compromised system will always have access to the S3 credentials and every service running on the system - including restic itself, proxies, etc. - we need a second, locked-down VM which can restrict delete operations. Restic can be integrated perfectly with rclone, so we’ll use it in this example. Our environment . | appsrv: the system which has access to the files we would like to backup | rclonesrv: the system running the rclone proxy (and nothing else, to minimise the attack surface) | . Set up the rclone proxy . | Install rclone on the rclonesrv . sudo apt install rclone . | Create user for rclone . sudo useradd -m rcloneproxy sudo su - rcloneproxy . | Create rclone backend configuration . mkdir -p .config/rclone cat &lt;&lt; EOF &gt; .config/rclone/rclone.conf [s3-resticrepo] type = s3 provider = Other env_auth = false access_key_id = 111122223333444455556666 secret_access_key = aaaabbbbccccddddeeeeffffgggghhhh region = eu-central-1 endpoint = s3.es1.fra.optimist.gec.io acl = private bucket_acl = private upload_concurrency = 8 EOF . | Verify that the access to the repository is working with: . rclone lsd s3-resticrepo:databucket 0 2021-11-21 20:02:10 -1 data 0 2021-11-21 20:02:10 -1 index 0 2021-11-21 20:02:10 -1 keys 0 2021-11-21 20:02:10 -1 snapshots . | . Configure the appserver . | Generate an SSH-Keypair on appsrv with the user you’re performing the backup with: . ssh-keygen -o -a 256 -t ed25519 -C \"$(hostname)-$(date +'%d-%m-%Y')\" . | Set the environment variables for restic: . export RESTIC_PASSWORD=\"MyV3ryS3cUr3r3571cP4ssW0rd\" export RESTIC_REPOSITORY=rclone:s3-resticrepo:databucket . | . Restrict the SSH key to restic-only commands . The last step is to go back to the rclonesrv and edit the SSH authorized_keys file to restrict the newly generated SSH key to a single command. This way, an attacker is not able to use the ssh keypair to run arbitrary commands on the rclone proxy and compromise the backups. vi ~/.ssh/authorized_keys # add an entry with the restic user's public key generated in a previous step: command=\"rclone serve restic --stdio --append-only s3-resticrepo:databucket\" ssh-ed25519 AAAAC3fdsC1lZddsDNTE5ADsaDgfTwNtWmwiocdT9q4hxcss6tGDfgGTdiNN0z7zN appsrv-18-11-2021 . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#proposed-solution",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#proposed-solution"
  },"286": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Using restic with the rclone proxy",
    "content": "With the environment variables set, restic should work now from appsrv. Example backing up /srv/myapp: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" backup /srv/myapp . Listing snapshots: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" snapshots . Deleting snapshots: . restic -o rclone.program=\"ssh rcloneproxy@rclonesrv.mydomain.com\" forget 2738e969 repository b71c391e opened successfully, password is correct Remove(&lt;snapshot/2738e9693b&gt;) returned error, retrying after 446.577749ms: blob not removed, server response: 403 Forbidden (403) . Oh, right, that doesn’t work. That was our goal! . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#using-restic-with-the-rclone-proxy",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#using-restic-with-the-rclone-proxy"
  },"287": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Summary",
    "content": "This way, . | The rclone proxy doesn’t even run on the rclonesrv as a service. It will just be spawned on demand, for the duration of the restic operation. Communication happens over HTTP2 over stdin/stdout, in an encrypted SSH tunnel. | Since rclone is running with --append-only, it is not possible to delete (or overwrite) snapshots in the S3 bucket. | All data (except credentials) is encrypted/decrypted locally, then sent/received via rclonesrv to/from S3. | All the credentials are only stored on rclonesrv to communicate with S3. | . Since the command is hard-coded into the SSH configuration for the user’s SSH key, there is no way to use the the keys to get access to the rclone proxy. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#summary",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#summary"
  },"288": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Some more thoughts",
    "content": "The advantages of this construct are probably clear already by now. Furthermore, . | Managing snapshots (both manually and with a retention policy) is only possible on the rclone proxy. | A single rclone proxy VM (or even a docker container on an isolated VM) can serve multiple backup clients. | It is highly recommended to use one key for every server which backs up data. | If you’d like to use more than one repository out of a node, you’ll need new SSH keys for them. You can then specify which key to use with -i ~/.ssh/id_ed25519_another_repo in the rclone.program arguments just like you would with SSH. | . ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/#some-more-thoughts",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/#some-more-thoughts"
  },"289": {
    "doc": "Secure Backups with Restic and Rclone",
    "title": "Secure Backups with Restic and Rclone",
    "content": "Restic is a very simple and powerful file-level backup solution, which is rapidly gaining popularity. It can be used in combination with S3 which makes it a great tool to use with Optimist. ",
    "url": "/optimist/storage/s3_documentation/backupwithrestic/",
    
    "relUrl": "/optimist/storage/s3_documentation/backupwithrestic/"
  },"290": {
    "doc": "Localstorage",
    "title": "Compute localstorage for your instances",
    "content": "L1 / Localstorage Flavors can be requested from support@gec.io. ",
    "url": "/optimist/storage/localstorage/#compute-localstorage-for-your-instances",
    
    "relUrl": "/optimist/storage/localstorage/#compute-localstorage-for-your-instances"
  },"291": {
    "doc": "Localstorage",
    "title": "What exactly is Compute Localstorage?",
    "content": "With Localstorage, the storage of your instances is located directly on the hypervisor (server). The localstorage feature (available through our l1 flavors) is intended for use with applications that require low latency. ",
    "url": "/optimist/storage/localstorage/#what-exactly-is-compute-localstorage",
    
    "relUrl": "/optimist/storage/localstorage/#what-exactly-is-compute-localstorage"
  },"292": {
    "doc": "Localstorage",
    "title": "Data security and availability",
    "content": "Since your data is directly bound by your instance on our local hypervisor storage, we recommend that you distribute this data via a HA concept, over the provided Availability Zones. The storage backend of the local storage instances is protected against the failure of individual storage media in the array, however, the resulting redundancy compared to the Ceph-based instances only exists within the hypervisor node providing the instance. When replacing individual components following a hardware issue, there may be limited availability and performance for a short time until recovery. The hypervisors are subject to our defined patch cycle where we must boot through the hypervisors one by one. As the instances are using local storage, the maintenance work cannot be carried out uninterrupted, unlike flavors based on Ceph storage. Consequently, there is a regular maintenance window for l1 Flavors. Within each Availability Zone, one server after the other is updated and rebooted during the defined maintenance window. Within the maintenance window, running instances are shut down by our system and stopped after 10 minutes. ",
    "url": "/optimist/storage/localstorage/#data-security-and-availability",
    
    "relUrl": "/optimist/storage/localstorage/#data-security-and-availability"
  },"293": {
    "doc": "Localstorage",
    "title": "Standard Maintainance Windows",
    "content": "| Interval | day | time (in UTC) | . | weekly | Wednesday | 9:00 a.m. - 4:00 p.m. | . ",
    "url": "/optimist/storage/localstorage/#standard-maintainance-windows",
    
    "relUrl": "/optimist/storage/localstorage/#standard-maintainance-windows"
  },"294": {
    "doc": "Localstorage",
    "title": "OpenStack Features",
    "content": "OpenStack provides many ways to handle your instances, such as resizing, shelving, and snapshots. If you want to use l1 flavors for your instances please note the following: . Resize: The resize option will be displayed, but it is technically not possible to resize an instance based on an l1 flavor. However, you can address this by doing a cluster setup (application based) with l1 flavors, running larger l1 flavors in parallel and rolling your data from the old l1 to the new l1 flavors. Shelving/Snapshotting: Both features are possible, but due to the larger disk size within l1 flavors we do not recommend this, as the associated upload will take much longer. In this case we recommend using your external backup solution. ",
    "url": "/optimist/storage/localstorage/#openstack-features",
    
    "relUrl": "/optimist/storage/localstorage/#openstack-features"
  },"295": {
    "doc": "Localstorage",
    "title": "Localstorage",
    "content": " ",
    "url": "/optimist/storage/localstorage/",
    
    "relUrl": "/optimist/storage/localstorage/"
  },"296": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/optimist/faq/",
    
    "relUrl": "/optimist/faq/"
  },"297": {
    "doc": "FAQ",
    "title": "The command openstack --help shows the error “Could not load EntryPoint.parse”",
    "content": "In this case some components of the OpenstackClient are outdated. To get an overview of the components that need to be updated, use the following command: . openstack --debug --help . To update the components, use the command below. (Replace &lt;PROJECT&gt; with the correct project): . pip install python-&lt;PROJECT&gt;client -U . ",
    "url": "/optimist/faq/#the-command-openstack---help-shows-the-error-could-not-load-entrypointparse",
    
    "relUrl": "/optimist/faq/#the-command-openstack---help-shows-the-error-could-not-load-entrypointparse"
  },"298": {
    "doc": "FAQ",
    "title": "How can I use VRRP?",
    "content": "To use VRRP, it must first be enabled in a security group which is then assigned to an actual VM. You can only add this with the OpenStack client. For example: . openstack security group rule create --remote-ip 10.0.0.0/24 --protocol vrrp --ethertype IPv4 --ingress default . ",
    "url": "/optimist/faq/#how-can-i-use-vrrp",
    
    "relUrl": "/optimist/faq/#how-can-i-use-vrrp"
  },"299": {
    "doc": "FAQ",
    "title": "Why am I charged for Floating IPs I am not using?",
    "content": "We have to charge for reserved Floating IPs. In this case, there is a high probability that Floating IPs were created but not deleted correctly after use. To get an overview of your Floating IPs, you can use the Horizon Dashboard, where you can find the list Project → Network → Floating-IPs. You can accomplish the same using the OpenStack client: . $ openstack floating ip list +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | ID | Floating IP Address | Fixed IP Address | Port | Floating Network | Project | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ | 84eca713-9ac1-42c3-baf6-860ba920a23c | 185.116.245.222 | 192.0.2.7 | a3097883-21cc-49fa-a060-bccc1678ece7 | 54258498-a513-47da-9369-1a644e4be692 | b15cde70d85749689e6568f973bb002 | +--------------------------------------+---------------------+------------------+--------------------------------------+--------------------------------------+----------------------------------+ . ",
    "url": "/optimist/faq/#why-am-i-charged-for-floating-ips-i-am-not-using",
    
    "relUrl": "/optimist/faq/#why-am-i-charged-for-floating-ips-i-am-not-using"
  },"300": {
    "doc": "FAQ",
    "title": "How can I change the flavor of a virtual machine (instance resize)?",
    "content": "In the Optimist Stack you can change the flavor of an instance – the available RAM and vCPUs – with the following steps: . Resizing using the command line . With the name or UUID of the server you wish to resize, you can resize it with the openstack server resize command. Specify the desired new flavor and then the instance name or UUID: . openstack server resize --flavor FLAVOR SERVER . Resizing can take some time. During this time, the instance status is displayed as RESIZE. When the resize is complete, the instance status is displayed as VERIFY_RESIZE. To change the status to ACTIVE, confirm the resize: . openstack server resize --confirm SERVER . Resizes are confirmed automatically after 1 hour, if not confirmed or reverted manually. Resizing with the Optimist dashboard . On Optimist Dashboard → Instances navigate to the instance to resize, then choose: Actions → _Resize Flavor:. The current flavor is shown. To choose the new flavor, use the “Select a new flavor” dropdown list, and confirm with “Resize”. This does not apply to l1 (localstorage) flavors. For more information please see Storage → Localstorage. ",
    "url": "/optimist/faq/#how-can-i-change-the-flavor-of-a-virtual-machine-instance-resize",
    
    "relUrl": "/optimist/faq/#how-can-i-change-the-flavor-of-a-virtual-machine-instance-resize"
  },"301": {
    "doc": "FAQ",
    "title": "Why are the logs of the compute instance in the optimist dashboard empty?",
    "content": "Due to maintenance work or load redistribution in OpenStack, the instance may have been migrated. In this case, the log file will be recreated and new messages logged here. ",
    "url": "/optimist/faq/#why-are-the-logs-of-the-compute-instance-in-the-optimist-dashboard-empty",
    
    "relUrl": "/optimist/faq/#why-are-the-logs-of-the-compute-instance-in-the-optimist-dashboard-empty"
  },"302": {
    "doc": "FAQ",
    "title": "Why do I get the error “Conflict (HTTP 409)” when creating a swift container?",
    "content": "Swift uses unique names across the entire OpenStack environment. The error message states that the selected name is already in use. ",
    "url": "/optimist/faq/#why-do-i-get-the-error-conflict-http-409-when-creating-a-swift-container",
    
    "relUrl": "/optimist/faq/#why-do-i-get-the-error-conflict-http-409-when-creating-a-swift-container"
  },"303": {
    "doc": "FAQ",
    "title": "HowTo Mount Cinder Volumes to Instances by UUID",
    "content": "When attaching multiple Cinder Volumes to an instance, the mount points may be shuffled on every reboot. Mounting the volumes by UUID ensures that the correct volumes are reattached to the correct mount points in the event the instance requires a power cycle. Change the mountpoint in /etc/fstab to use the UUID after fetching the infos with blkid on e.g.: . # /boot was on /dev/sda2 during installation /dev/disk/by-uuid/f6a0d6f3-b66c-bbe3-47ba-d264464cb5a2 /boot ext4 defaults 0 2 . ",
    "url": "/optimist/faq/#howto-mount-cinder-volumes-to-instances-by-uuid",
    
    "relUrl": "/optimist/faq/#howto-mount-cinder-volumes-to-instances-by-uuid"
  },"304": {
    "doc": "FAQ",
    "title": "Is it possible to have multiattached volumes on Cinder?",
    "content": "We do not support multiattached volumes on our instances as cluster-capable file systems are required for multi attach volumes to handle concurrent file system access. Attempts to use multi-attached volumes without cluster-capable file systems carry a high risk of data corruption, therefore this feature is not enabled on the Optimist platform. ",
    "url": "/optimist/faq/#is-it-possible-to-have-multiattached-volumes-on-cinder",
    
    "relUrl": "/optimist/faq/#is-it-possible-to-have-multiattached-volumes-on-cinder"
  },"305": {
    "doc": "FAQ",
    "title": "Why am I unable to create a snapshot of a running instance?",
    "content": "In order to provide consistent snapshots, the Optimist platform utilises the property os_require_quiesce=yes. This property allows fsfreeze to suspend and resume access on running instances and ensures ensures that a consistent image is created from the disk. The Optimist Platform supports the following options to facilitate the creation of snapshots on running instances: . The first option is to take a snapshot of the running instance by installing and running the qemu-guest-agent. It can be installed and run as follows: . apt install qemu-guest-agent systemctl start qemu-guest-agent systemctl enable qemu-guest-agent . Once the qemu-guest-agent is running, the snapshot can be created. Additionally, when uploading your own images, we recommend that you include --property hw_qemu_guest_agent=True to install this upon creation of the new image. The second option is to stop the running instance, create the snapshot, then start the instance again. This can be done via the Horizon Dashboard or on the CLI as follows: . openstack server stop ExampleInstance openstack server image create --name ExampleInstanceSnapshot ExampleInstance openstack server start ExampleInstance . ",
    "url": "/optimist/faq/#why-am-i-unable-to-create-a-snapshot-of-a-running-instance",
    
    "relUrl": "/optimist/faq/#why-am-i-unable-to-create-a-snapshot-of-a-running-instance"
  },"306": {
    "doc": "Specifications",
    "title": "Specifications",
    "content": " ",
    "url": "/optimist/specs/",
    
    "relUrl": "/optimist/specs/"
  },"307": {
    "doc": "Flavor Specifications",
    "title": "Flavor Specifications",
    "content": "In the OpenStack context the term “flavor” refers to a hardware profile that can be used for a virtual machine. In Optimist we have set up various standard hardware profiles (flavors). These have different limits, which are listed below for all available flavors. L1 / Localstorage Flavors can be requested from support@gec.io. ",
    "url": "/optimist/specs/flavor_specification/",
    
    "relUrl": "/optimist/specs/flavor_specification/"
  },"308": {
    "doc": "Flavor Specifications",
    "title": "Migrating between Flavor Types",
    "content": "To change the flavors of existing instances, the OpenStack “Resize Instance” Option can be used either via the Dashboard or the CLI. This will result in a reboot of the Instance but the content of the instance will be preserved. Please note that changing Flavors from Large Root Disk Types to a Flavor with a smaller Root Disk is not possible. This does not apply to l1 (localstorage) flavors For more information please see Storage → Localstorage . ",
    "url": "/optimist/specs/flavor_specification/#migrating-between-flavor-types",
    
    "relUrl": "/optimist/specs/flavor_specification/#migrating-between-flavor-types"
  },"309": {
    "doc": "Flavor Specifications",
    "title": "Flavor Types",
    "content": "Standard Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | s1.micro | 1 | 2 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | s1.small | 2 | 4 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | s1.medium | 4 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | s1.large | 8 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.xlarge | 16 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | s1.2xlarge | 30 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Standard CPU Large Disk Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | s1.micro.d | 1 | 2 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | s1.small.d | 2 | 4 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | s1.medium.d | 4 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | s1.large.d | 8 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.xlarge.d | 16 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | s1.2xlarge.d | 30 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Dedicated CPU Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | d1.micro | 1 | 8 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 1 Gbit/s | . | d1.small | 2 | 16 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 2 Gbit/s | . | d1.medium | 4 | 32 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 3 Gbit/s | . | d1.large | 8 | 64 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.xlarge | 16 | 128 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . | d1.2xlarge | 30 | 256 GB | 20 GB | 1000 / 1000 | 200 MB/s / 200 MB/s | 4 Gbit/s | . Dedicated CPU Large Disk Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | d1.micro.d | 1 | 8 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 1 Gbit/s | . | d1.small.d | 2 | 16 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 2 Gbit/s | . | d1.medium.d | 4 | 32 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 3 Gbit/s | . | d1.large.d | 8 | 64 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.xlarge.d | 16 | 128 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . | d1.2xlarge.d | 30 | 256 GB | 100 GB | 2500 / 2500 | 250 MB/s / 250 MB/s | 4 Gbit/s | . Localstorage Flavors . | Name | Cores | RAM | Disk | IOPS Limits (read/write) | IO throughput (read/write) | Network Bandwidth | . | l1.micro | 1 | 8 GB | 300 GB | 25000 / 10000 | 125 MB/s / 60 MB/s | 1 Gbit/s | . | l1.small | 2 | 16 GB | 600 GB | 50000 / 25000 | 250 MB/s / 125 MB/s | 2 Gbit/s | . | l1.medium | 4 | 32 GB | 1200 GB | 100000 / 50000 | 500 MB/s / 250 MB/s | 3 Gbit/s | . | l1.large | 8 | 64 GB | 2500 GB | 100000 / 100000 | 1000 MB/s / 500 MB/s | 4 Gbit/s | . | l1.xlarge | 16 | 128 GB | 5000 GB | 100000 / 100000 | 2000 MB/s / 1125 MB/s | 4 Gbit/s | . | l1.2xlarge | 30 | 256 GB | 10000 GB | 100000 / 100000 | 2000 MB/s / 2000 MB/s | 4 Gbit/s | . ",
    "url": "/optimist/specs/flavor_specification/#flavor-types",
    
    "relUrl": "/optimist/specs/flavor_specification/#flavor-types"
  },"310": {
    "doc": "Default Quotas",
    "title": "OpenStack Default Quotas",
    "content": "In Optimist we have defined default quotas for the OpenStack Compute service, the OpenStack Block Storage service, and the OpenStack Networking service. We also have separate quotas for the Octavia Loadbalancer service and its associated components. These default values are listed below. ",
    "url": "/optimist/specs/default_quota/#openstack-default-quotas",
    
    "relUrl": "/optimist/specs/default_quota/#openstack-default-quotas"
  },"311": {
    "doc": "Default Quotas",
    "title": "Compute Settings",
    "content": "| Field | Value | . | Cores | 256 | . | Fixed IPs | Unlimited | . | Floating IPs | 15 | . | Injected File Size | 10240 | . | Injected Files | 100 | . | Instances | 100 | . | Key Pairs | 100 | . | Properties | 128 | . | Ram | 524288 | . | Server Groups | 10 | . | Server Group Members | 10 | . ",
    "url": "/optimist/specs/default_quota/#compute-settings",
    
    "relUrl": "/optimist/specs/default_quota/#compute-settings"
  },"312": {
    "doc": "Default Quotas",
    "title": "Block Storage settings",
    "content": "| Field | Value | . | Backups | 100 | . | Backup Gigabytes | 10000 | . | Gigabytes | 10000 | . | Per-volume-gigabytes | Unlimited | . | Snapshots | 100 | . | Volumes | 100 | . ",
    "url": "/optimist/specs/default_quota/#block-storage-settings",
    
    "relUrl": "/optimist/specs/default_quota/#block-storage-settings"
  },"313": {
    "doc": "Default Quotas",
    "title": "Network settings",
    "content": "| Field | Value | . | Floating IPs | 15 | . | Secgroup Rules | 1000 | . | Secgroups | 100 | . | Networks | 100 | . | Subnets | 200 | . | Ports | 500 | . | Routers | 50 | . | RBAC Policies | 100 | . | Subnetpools | Unlimited | . ",
    "url": "/optimist/specs/default_quota/#network-settings",
    
    "relUrl": "/optimist/specs/default_quota/#network-settings"
  },"314": {
    "doc": "Default Quotas",
    "title": "Octavia Loadbalancers",
    "content": "| Field | Value | . | Load Balancers | 100 | . | Listeners | 100 | . | Pools | 100 | . | Health Monitors | 100 | . | Members | 100 | . ",
    "url": "/optimist/specs/default_quota/#octavia-loadbalancers",
    
    "relUrl": "/optimist/specs/default_quota/#octavia-loadbalancers"
  },"315": {
    "doc": "Default Quotas",
    "title": "Default Quotas",
    "content": " ",
    "url": "/optimist/specs/default_quota/",
    
    "relUrl": "/optimist/specs/default_quota/"
  },"316": {
    "doc": "Application Credentials",
    "title": "Introduction",
    "content": "Users can create Application Credentials to allow their applications to authenticate to the OpenStack Authentication component (Keystone) without needing to use the user’s personal credentials. With application credentials, applications can authenticate with the application credential ID and a secret string which is not the user’s password. This way, the user’s password is not embedded in the application’s configuration. Users can delegate a subset of their role assignments on a project to application credentials, granting the application the same or restricted permissions within a project. ",
    "url": "/optimist/specs/application_credentials/#introduction",
    
    "relUrl": "/optimist/specs/application_credentials/#introduction"
  },"317": {
    "doc": "Application Credentials",
    "title": "Requirements for Application Credentials",
    "content": "Name / Secrets . Application credentials can be generated for your project via the command-line or via the dashboard. These will be associated with the project in which they are created. The only required parameter to create the credentials is a name, however a specific secret can be set by using the —-secret parameter. If the secret parameter is left blank, a secret will instead be auto-generated in the output. It is important to make note of the secret in either case as the secret is hashed before it is stored and will not be retrievable after it has been set. If the secret is lost, a new application credential should be created. Roles . We also recommend setting the roles that the Application Credentials should have in the project, since by default, a newly created set of credentials will inherit all available roles. Below are the available roles which can be assigned to a set of application credentials. When applying these roles to a set of credentials using the --role parameter, please be aware that all role names are case-sensitive: . | Member: The “member” role only has administrative access to the assigned project. | heat_stack_owner: As “heat_stack_owner” you are able to use and execute existing HEAT templates. | load-balancer_member: As a “load-balancer_member” you can use the Octavia LoadBalancer resources. | . Expiration . By default, created Application Credentials will not expire, however, fixed expiration dates/times can be set for credentials upon creation, using the --expires parameter in the command (for example: --expires '2021-07-15T21:00:00'). ",
    "url": "/optimist/specs/application_credentials/#requirements-for-application-credentials",
    
    "relUrl": "/optimist/specs/application_credentials/#requirements-for-application-credentials"
  },"318": {
    "doc": "Application Credentials",
    "title": "Creating Application Credentials via the CLI",
    "content": "A set of Application Credentials can be created in the desired project via the CLI, the example below demonstrates how to create a set of credentials with the following parameters: . | Name: test-credentials | Secret: ZYQZm2k6pk | Roles: Member, heat_stack_owner, load-balancer_member | Expiration Date/Time: 2021-07-12 at 21:00:00 | . The new credentials should appear as follows: . $ openstack application credential create test-credentials --secret ZYQZm2k6pk --role Member --role heat_stack_owner --role load-balancer_member --expires '2021-07-15T21:00:00' +--------------+----------------------------------------------+ | Field | Value | +--------------+----------------------------------------------+ | description | None | expires_at | 2021-07-15T21:00:00.000000 | id | 707d14e835124b4f957938bb5a57d1be | name | test-credentials | project_id | c704ac5a32b84b54a0407d28ad448399 | roles | Member heat_stack_owner load-balancer_member | secret | ZYQZm2k6pk | system | None | unrestricted | False | user_id | 1d9f1ecb5de3607e8982695f72036fa5 | +--------------+----------------------------------------------+ . Note: The secret (whether set by the user or auto-generated) will be displayed upon creation of the credentials. Please take note of the secret at this time. ",
    "url": "/optimist/specs/application_credentials/#creating-application-credentials-via-the-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#creating-application-credentials-via-the-cli"
  },"319": {
    "doc": "Application Credentials",
    "title": "Viewing Application Credentials via the CLI",
    "content": "The list of application credentials belonging to a project can be listed with the following command. $ openstack application credential list +----------------------------------+-------------------+----------------------------------+-------------+------------+ | ID | Name | Project ID | Description | Expires At | +----------------------------------+-------------------+----------------------------------+-------------+------------+ | 707d14e835124b4f957938bb5a57d1be | test-credentials | c704ac5a32b84b54a0407d28ad448399 | None | None | +----------------------------------+-------------------+----------------------------------+-------------+------------+ . Individual credentials can be viewed using the $ openstack application credential show &lt;name&gt; command. ",
    "url": "/optimist/specs/application_credentials/#viewing-application-credentials-via-the-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#viewing-application-credentials-via-the-cli"
  },"320": {
    "doc": "Application Credentials",
    "title": "Deleting Application Credentials via the CLI",
    "content": "Application credentials can be deleted via the CLI with the following command with the name or ID of the specific set of credentials: . openstack application credential delete test-credentials . ",
    "url": "/optimist/specs/application_credentials/#deleting-application-credentials-via-the-cli",
    
    "relUrl": "/optimist/specs/application_credentials/#deleting-application-credentials-via-the-cli"
  },"321": {
    "doc": "Application Credentials",
    "title": "Creating and Deleting Application Credentials via the Optimist Dashboard",
    "content": "Alternatively, Application credentials can also be generated via the Optimist Dashboard under Identity &gt; Application credentials: . Note: Multiple roles can be selected here by holding shift and navigating through the options. Once created, a dialog box will appear to instruct you to capture the ID and secret. Once done, click “Close”. The credentials here can also deleted at any point by using the checkbox to highlight the set of credentials to be deleted and then clicking “DELETE APPLICATION CREDENTIAL” . ",
    "url": "/optimist/specs/application_credentials/#creating-and-deleting-application-credentials-via-the-optimist-dashboard",
    
    "relUrl": "/optimist/specs/application_credentials/#creating-and-deleting-application-credentials-via-the-optimist-dashboard"
  },"322": {
    "doc": "Application Credentials",
    "title": "Testing Application credentials",
    "content": "Once we have created a set of Application Credentials either via the CLI or dashboard, we can test them by using the following curl command to verify that they are working. We need to use our &lt;name&gt; and &lt;secret&gt; in the curl command: . curl -i -H \"Content-Type: application/json\" -d ' { \"auth\": { \"identity\": { \"methods\": [\"application_credential\"], \"application_credential\": { \"id\": “&lt;id&gt;\", \"secret\": “&lt;secret&gt;\"}}}}' https://identity.optimist.gec.io/v3/auth/tokens . A successful curl attempt will output an x-subject-token, unsuccessful attempts where the credentials are incorrect will result in a 401 error. ",
    "url": "/optimist/specs/application_credentials/#testing-application-credentials",
    
    "relUrl": "/optimist/specs/application_credentials/#testing-application-credentials"
  },"323": {
    "doc": "Application Credentials",
    "title": "Application Credentials",
    "content": " ",
    "url": "/optimist/specs/application_credentials/",
    
    "relUrl": "/optimist/specs/application_credentials/"
  },"324": {
    "doc": "Volume Specifications",
    "title": "Volume Specifications",
    "content": "In OpenStack, “volumes” are persistent storage that you can attach to your running OpenStack Compute instances to. In Optimist we have set up three classes of service for volumes. These have different limits, which are detailed below. ",
    "url": "/optimist/specs/volume_specification/",
    
    "relUrl": "/optimist/specs/volume_specification/"
  },"325": {
    "doc": "Volume Specifications",
    "title": "Volume Types",
    "content": "We have three main volume types: . | high-iops | default | low-iops | . ",
    "url": "/optimist/specs/volume_specification/#volume-types",
    
    "relUrl": "/optimist/specs/volume_specification/#volume-types"
  },"326": {
    "doc": "Volume Specifications",
    "title": "Volume Type List",
    "content": "An overview of the three volume types below: . | Name | Read Bytes Sec | Read IOPS Sec | Write Bytes Sec | Write IOPS Sec | . | high-iops | 524288000 | 10000 | 524288000 | 10000 | . | default | 209715200 | 2500 | 209715200 | 2500 | . | low-iops | 52428800 | 300 | 52428800 | 300 | . ",
    "url": "/optimist/specs/volume_specification/#volume-type-list",
    
    "relUrl": "/optimist/specs/volume_specification/#volume-type-list"
  },"327": {
    "doc": "Volume Specifications",
    "title": "Choosing a Volume Type",
    "content": "You can select one of the three volume types upon creation of a volume with the following command (Unless otherwise specified, the type “default” is always used): $ openstack volume create &lt;volume-name&gt; --size 10 --type high-iops . ",
    "url": "/optimist/specs/volume_specification/#choosing-a-volume-type",
    
    "relUrl": "/optimist/specs/volume_specification/#choosing-a-volume-type"
  },"328": {
    "doc": "Images",
    "title": "Images",
    "content": "There are 4 types of images in OpenStack: . | Public Images: These images are maintained by us, available to all users, regularly updated and recommended for use. | Community Images: Previously public images, which have been superseded by newer versions. We’re keeping these images until they’re no longer in use, so as not to compromise your deployments. | Private Images: Images uploaded by you that are only available to your project. | Shared Images: Private images, which are either shared by you, or with you, across multiple different projects. | . Only the first two types are maintained by us. ",
    "url": "/optimist/specs/images/",
    
    "relUrl": "/optimist/specs/images/"
  },"329": {
    "doc": "Images",
    "title": "Public and community images",
    "content": "For your convenience, we’re providing you with a number of selected images. The current list of images is as follows: . | Ubuntu 24.04 LTS (Noble Numbat) | Ubuntu Minimal 24.04 LTS (Noble Numbat) | Ubuntu 22.04 LTS (Jammy Jellyfish) | Ubuntu Minimal 22.04 LTS (Jammy Jellyfish) | Ubuntu 20.04 LTS (Focal Fossa) | Debian 12 (Bookworm) | Debian 11 (Bullseye) | Debian 10 (Buster) | CentOS 8 | CentOS 7 | CoreOS (stable) | Flatcar Linux | Windows Server 2019 (GUI/Core) | . These images are checked for new releases daily. The latest available version is always a public image, and contains the Latest-suffix. All previous versions of an imags are automatically converted to “community images”, renamed (Latest is replaced by the date of the first upload), and eventially deleted if they are no longer in use at all. OpenStack and many deployment tools support using these images either by name or by their UUID. By using a name, for example Ubuntu 22.04 Jammy Jellyfish - Latest, you can easily stay up to date by redeploying or rebuilding your instances, even if we replace the image in the interim. You can avoid this behaviour by using the UUID instead. This may be useful for cluster deployments, where you want to ensure that all nodes are running the same version of the image. ",
    "url": "/optimist/specs/images/#public-and-community-images",
    
    "relUrl": "/optimist/specs/images/#public-and-community-images"
  },"330": {
    "doc": "Images",
    "title": "Linux Images",
    "content": "All of our provided linux images are unmodified and come directly from their official maintainers. We test them during the upload process to ensure they are deployable. ",
    "url": "/optimist/specs/images/#linux-images",
    
    "relUrl": "/optimist/specs/images/#linux-images"
  },"331": {
    "doc": "Images",
    "title": "Windows Images",
    "content": "What’s inside? . Sadly, there are no prebuilt images for windows deployments, so we built our own. Our changes are minimal, just enough to allow easy use within our instances. Our images are based on a regular installation of Windows Server 2019 standard edition, version 1809 (LTSC). We have added the latest drivers for our virtualization infrastructure, for the network card and storage. Next, we installed the most recent OpenSSH build for windows, and the most recent version of PowerShell. Both are required for the following provisioning steps, and to allow you to initially connect to your instance. We also enabled the RDP service, which is required for remote desktop connections. Don’t forget to add the required security groups for this, and be sure to limit access as much as possible. We have also disabled AutoLogon for security reasons. Our images also come with Spectre and Meltdown mitigations enabled. Additionally we had to disable the random MAC address generator, since our virtual networks enforce fixed MAC addresses. For your convenience and security, we provide these windows images with the latest cumulative updates for Windows and the .NET framework. After booting up an instance, you’ll most likely only have to update the Windows Defender definitions. Finally, we optimized the available DotNetAssemblies, added firewall rules to allow ICMP echo replies and installed cloud-init. The latter is responsible for adding your ssh keys to the new instances. How to use them? . Almost as easy as deploying a linux instance. Add your ssh key to OpenStack (CLI or Dashboard) and deploy the instance. You can then connect to the instance with the following command: . ssh -i ~/.ssh/id_rsa $instanceIP -l Administrator . From here, you can set a password for your Administrator account for use with Remote Desktop: . net user Administrator $password . We strongly discourage using the previous method, adding admin_pass to the instances metadata. This is not encrypted or protected in any way, and is not guaranteed to work due to password security requirements. Be aware: Our Images come without product keys or licenses. You will have to provide your own. ",
    "url": "/optimist/specs/images/#windows-images",
    
    "relUrl": "/optimist/specs/images/#windows-images"
  },"332": {
    "doc": "Images",
    "title": "Uploading your own images",
    "content": "Instead of using the images we provide, you can upload your own images. Easiest way of doing so is to use the OpenStack CLI. openstack image create \\ --property hw_disk_bus=scsi \\ --property hw_qemu_guest_agent=True \\ --property hw_scsi_model=virtio-scsi \\ --property os_require_quiesce=True \\ --private \\ --disk-format qcow2 \\ --container-format bare \\ --file ~/my-image.qcow2 \\ my-image . The command to upload images requires these fields at a minimum: . | --disk-format: qcow2, in this case. This depends on the image format. | --file: The source file on your machine | Name of the Image: my-image for example. | . Additionally, to enable the creation of Snapshots on running Instances, we recommend that you set --property hw_qemu_guest_agent=True on the images you create, and to install the qemu-guest-agent upon creation of the new image. See our FAQ for more details. You can also use the dashboard to upload images. Make sure to use the same properties there. ",
    "url": "/optimist/specs/images/#uploading-your-own-images",
    
    "relUrl": "/optimist/specs/images/#uploading-your-own-images"
  },"333": {
    "doc": "Shelving Instances",
    "title": "Shelving Instances",
    "content": " ",
    "url": "/optimist/specs/shelving_instances/",
    
    "relUrl": "/optimist/specs/shelving_instances/"
  },"334": {
    "doc": "Shelving Instances",
    "title": "Introduction",
    "content": "On the OpenStack platform, you have the ability to shelve an instance. Shelving instances allows you to stop an instance without having it consume resources. A shelved instance, as well as its assigned resources (such as IP address etc), will be retained as a bootable instance. This feature may be used as part of an instance life cycle process or to conserve resources. This does not apply to l1 (localstorage) flavors. For more information please see Storage → Localstorage. ",
    "url": "/optimist/specs/shelving_instances/#introduction",
    
    "relUrl": "/optimist/specs/shelving_instances/#introduction"
  },"335": {
    "doc": "Shelving Instances",
    "title": "Shelve an Instance",
    "content": "Instances can be shelved as follows: $ openstack server shelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#shelve-an-instance",
    
    "relUrl": "/optimist/specs/shelving_instances/#shelve-an-instance"
  },"336": {
    "doc": "Shelving Instances",
    "title": "Unshelve an Instance",
    "content": "Instances can be unshelved with the following command: $ openstack server unshelve &lt;server-id&gt; . ",
    "url": "/optimist/specs/shelving_instances/#unshelve-an-instance",
    
    "relUrl": "/optimist/specs/shelving_instances/#unshelve-an-instance"
  },"337": {
    "doc": "Shelving Instances",
    "title": "View Event List for Instances",
    "content": "You can view the shelving / unshelving history of any server by viewing the event list: . $ openstack server event list &lt;server-id&gt; +------------------------------------------+--------------------------------------+--------+----------------------------+ | Request ID | Server ID | Action | Start Time | +------------------------------------------+--------------------------------------+--------+----------------------------+ | req-8d593999-a09b-41a7-8916-1d7c28cd4dc0 | 846112be-d107-4c75-db75-a32eb47a78c5 | shelve | 2022-07-17T15:28:08.000000 | req-076969ee-15a4-470e-8913-051c6f9d4bd3 | 846112be-d107-4c75-db75-a32eb47a78c5 | create | 2022-07-19T16:15:22.000000 | +------------------------------------------+--------------------------------------+--------+----------------------------+ . ",
    "url": "/optimist/specs/shelving_instances/#view-event-list-for-instances",
    
    "relUrl": "/optimist/specs/shelving_instances/#view-event-list-for-instances"
  },"338": {
    "doc": "Shelving Instances",
    "title": "Why Use Shelving?",
    "content": "This feature is useful for archiving instances you are not currently using but do not want to delete. Shelving an instance allows you to you retain the instance data and resource allocations, but frees up the instance memory. When you shelve an instance, the Compute service generates a snapshot image that captures the state of the instance, and uploads it to the Glance image library. If the instance is unshelved, it will be rebuilt using the snapshot. The snapshot image will be deleted if the instance is later unshelved or deleted. ",
    "url": "/optimist/specs/shelving_instances/#why-use-shelving",
    
    "relUrl": "/optimist/specs/shelving_instances/#why-use-shelving"
  },"339": {
    "doc": "Shelving Instances",
    "title": "Billing for Shelved Instances",
    "content": "From a billing perspective, only the root disk of the shelved instance continues to be billed. Once the instance is shelved, CPU and memory resources from the flavor of the instance cease to be billed, however, billing will automatically resume again after unshelving. Shelving has no effect on the utilization of quotas in the project. Shelved resources do not release their quota to ensure sufficient resources for unshelving the instance in the project at all times. ",
    "url": "/optimist/specs/shelving_instances/#billing-for-shelved-instances",
    
    "relUrl": "/optimist/specs/shelving_instances/#billing-for-shelved-instances"
  },"340": {
    "doc": "Changelog",
    "title": "Changelog Optimist",
    "content": "All notable changes to the Optimist Platform are documented on this page. ",
    "url": "/optimist/changelog/#changelog-optimist",
    
    "relUrl": "/optimist/changelog/#changelog-optimist"
  },"341": {
    "doc": "Changelog",
    "title": "Upcoming",
    "content": "Upcoming changes to the Optimist platform are listed here . ",
    "url": "/optimist/changelog/#upcoming",
    
    "relUrl": "/optimist/changelog/#upcoming"
  },"342": {
    "doc": "Changelog",
    "title": "Completed",
    "content": "2022-04-28 . | Optimist Horizon Upgrade (Train) | . 2022-04-27 . | Optimist Heat Upgrade (Train) | . 2022-04-21 . | Optimist Neutron Upgrade (Train) | . 2022-04-05 . | Optimist Nova Upgrade (Train) | . 2022-03-01 . | Optimist Cinder Upgrade (Train) | . 2022-02-23 . | Optimist Designate Upgrade (Train) | . 2022-02-22 . | Optimist Glance Upgrade (Train) | . 2022-02-10 . | Neutron LBaaS removed from Optimist | . 2022-01-25 . | Optimist Keystone Upgrade (Train) | . 2021-08-24 . | Optimist Cinder Upgrade (Stein) | . 2021-08-18 . | Optimist Neutron Feature: . We activated the internal DNS feature. This allows customers to assign dns names to neutron ports. Nova will automatically add the instance name as dns name to the neutron port. | . 2021-07-20 . | Optimist Neutron Upgrade (Stein) | . 2021-06-23 . | Optimist Nova Upgrade (Stein) | . 2021-06-02 . | Optimist Glance upgrade (Stein) | . 2021-06-01 . | Optimist Keystone upgrade (Stein) | . ",
    "url": "/optimist/changelog/#completed",
    
    "relUrl": "/optimist/changelog/#completed"
  },"343": {
    "doc": "Changelog",
    "title": "Changelog",
    "content": " ",
    "url": "/optimist/changelog/",
    
    "relUrl": "/optimist/changelog/"
  },"344": {
    "doc": "About Managed Kubernetes",
    "title": "About Our Managed Kubernetes",
    "content": "We provide a Hosted CNCF Certified Kubernetes Cluster tailored to your needs. Unlike self-service Kubernetes offerings, we take full responsibility for creating, updating, and managing your Kubernetes clusters. Once the cluster is set up, we provide you with a kubeconfig file that grants you secure access to the cluster. ",
    "url": "/managedk8s/about/#about-our-managed-kubernetes",
    
    "relUrl": "/managedk8s/about/#about-our-managed-kubernetes"
  },"345": {
    "doc": "About Managed Kubernetes",
    "title": "What We Manage for You",
    "content": "As part of our managed Kubernetes service, we handle the following tasks: . | Cluster Creation: We set up a Kubernetes cluster that meets CNCF certification standards, ensuring compatibility with the latest Kubernetes versions. | Cluster Updates: We regularly update the cluster to the latest stable version, ensuring security and performance enhancements. | Cluster Management: We monitor the health and performance of the cluster control plane. | . By managing these core elements, we enable you to focus on your applications without worrying about the operational complexities of Kubernetes. In case anything goes wrong, you can reach us here. ",
    "url": "/managedk8s/about/#what-we-manage-for-you",
    
    "relUrl": "/managedk8s/about/#what-we-manage-for-you"
  },"346": {
    "doc": "About Managed Kubernetes",
    "title": "Getting Started",
    "content": " ",
    "url": "/managedk8s/about/#getting-started",
    
    "relUrl": "/managedk8s/about/#getting-started"
  },"347": {
    "doc": "About Managed Kubernetes",
    "title": "Step 1: Receive Your kubeconfig File",
    "content": "Once your Kubernetes cluster is ready, we will provide you with a kubeconfig file. This file contains the necessary credentials and connection details to access the cluster. ",
    "url": "/managedk8s/about/#step-1-receive-your-kubeconfig-file",
    
    "relUrl": "/managedk8s/about/#step-1-receive-your-kubeconfig-file"
  },"348": {
    "doc": "About Managed Kubernetes",
    "title": "Step 2: Install the kubectl Command-Line Tool",
    "content": "To interact with your managed Kubernetes cluster, you’ll need to install kubectl, the Kubernetes command-line tool. Installation Instructions: . | On macOS: brew install kubectl | On Windows: Download the executable from Kubernetes Releases | On Linux: Use the package manager for your distribution or download from Kubernetes Releases | . ",
    "url": "/managedk8s/about/#step-2-install-the-kubectl-command-line-tool",
    
    "relUrl": "/managedk8s/about/#step-2-install-the-kubectl-command-line-tool"
  },"349": {
    "doc": "About Managed Kubernetes",
    "title": "Step 3: Configure kubectl to Use the Provided kubeconfig",
    "content": "Once kubectl is installed, configure it to use the provided kubeconfig file: . Save the kubeconfig file in a secure location on your local machine. Set the KUBECONFIG environment variable to point to the location of your kubeconfig file: . export KUBECONFIG=/path/to/your/kubeconfig # Verify the configuration by running: # This command will display the details of your current Kubernetes configuration. kubectl config view . ",
    "url": "/managedk8s/about/#step-3-configure-kubectl-to-use-the-provided-kubeconfig",
    
    "relUrl": "/managedk8s/about/#step-3-configure-kubectl-to-use-the-provided-kubeconfig"
  },"350": {
    "doc": "About Managed Kubernetes",
    "title": "Step 4: Access Your Cluster",
    "content": "You can now use kubectl commands to interact with your managed Kubernetes cluster. Here are some basic commands to get you started: . Check Cluster Nodes: . kubectl get nodes . List Namespaces: . kubectl get namespaces . Deploy Applications: . To deploy applications, create Kubernetes manifests (YAML files) defining your application configurations (Deployments, Services, etc.) and apply them to the cluster: . kubectl apply -f your-deployment-file.yaml . ",
    "url": "/managedk8s/about/#step-4-access-your-cluster",
    
    "relUrl": "/managedk8s/about/#step-4-access-your-cluster"
  },"351": {
    "doc": "About Managed Kubernetes",
    "title": "Certifications",
    "content": ". Our managed kubernetes service is a product to efficiently set up and operate Kubernetes clusters on Wiit Cloud. Customers can use a managed kubernetes cluster that pass the conformance checks and requirements by the Cloud Native Computing Foundation. Conformance results of our platform can be validated and checked at CNCF Kubernetes Conformance . ",
    "url": "/managedk8s/about/#certifications",
    
    "relUrl": "/managedk8s/about/#certifications"
  },"352": {
    "doc": "About Managed Kubernetes",
    "title": "About Managed Kubernetes",
    "content": " ",
    "url": "/managedk8s/about/",
    
    "relUrl": "/managedk8s/about/"
  },"353": {
    "doc": "Support",
    "title": "Support Channels",
    "content": "During the onboarding process, we will choose the suitable communication channel for your organization. The following support channels are available for you to choose from: . Microsoft Teams . We can create a dedicated support channel within Microsoft Teams for your organization. This channel enables real-time communication with our support engineers, allowing for quick response. It’s ideally for organizations that use Microsoft Teams as their primary communication tool. Availability: During standard business hours. Slack . If your organization prefers Slack, we can set up a private Slack channel for direct communication with our support team. This channel provides a space for interactive support, allowing for faster response times and more collaborative problem-solving. It’s suited for teams already using Slack for internal communication. Availability: During standard business hours. Email . For formal communication, you can reach our support team via email. Response Time: Typically, you will receive a response during the working hours. The response time may vary depending on the nature and severity of the issue. Phone Support . For critical issues that require immediate attention, we provide a oncall support. This is ideal for high-priority incidents that may impact your production environment during the non working hours. Availability: 24/7 for critical issues, depending on your SLA. During the onboarding process, we will discuss and agree upon the preferred support channels for your organization. We will also define the service level agreement (SLA) that aligns with your business needs. ",
    "url": "/managedk8s/about/support/#support-channels",
    
    "relUrl": "/managedk8s/about/support/#support-channels"
  },"354": {
    "doc": "Support",
    "title": "Support",
    "content": " ",
    "url": "/managedk8s/about/support/",
    
    "relUrl": "/managedk8s/about/support/"
  },"355": {
    "doc": "Supported Kubernetes Versions",
    "title": "Supported Kubernetes Versions",
    "content": "As part of our Managed Kubernetes service, we ensure that your clusters are always running on supported and stable Kubernetes versions. To provide clarity on our version support policy, here is an overview of our supported Kubernetes versions, deprecation, and end-of-life (EOL). | Version | Deprecation | End-of-Life | . | v1.31 |   |   | . | v1.30 |   |   | . | v1.29 |   |   | . ",
    "url": "/managedk8s/about/kubernetesversions/",
    
    "relUrl": "/managedk8s/about/kubernetesversions/"
  },"356": {
    "doc": "Supported Kubernetes Versions",
    "title": "Force Upgrade Policy",
    "content": "If a customer cluster is not updated by its owner until the announced End-of-Life date, it will be automatically upgraded to the next supported version. You can read more about the deprecation and force upgrade policy here. ",
    "url": "/managedk8s/about/kubernetesversions/#force-upgrade-policy",
    
    "relUrl": "/managedk8s/about/kubernetesversions/#force-upgrade-policy"
  },"357": {
    "doc": "Cluster Lifecycle",
    "title": "Cluster Lifecycle",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/",
    
    "relUrl": "/managedk8s/clusterlifecycle/"
  },"358": {
    "doc": "Cluster Creation",
    "title": "Cluster creation",
    "content": "To successfully create a cluster, please provide the required information outlined below. Ensure all details are filled correctly before proceeding with the cluster creation. Copy the block below and complete it with the necessary information for your new cluster: . # required infromation cluster_name: customer_id: # other k8s version (by default latest k8s version provided by us) kubernetes_version: # controlplane flavor # by default s1.medium flavor: ## Machine deployments one block per MD (default: 1 md, 3 replicas, s1.large, random AZ) # machine deployment name # by default clusterName-az-md md_name: # replicas by default 3 replicas: # by default s1.large flavor: # by default random-az availability_zone: ## add the autoscaler (default: disabled) min_size: max_size: . Detailed information about options can be found below. What You Will Receive . After the cluster is created, you will receive an admin kubeconfig through a secure method, along with a unique Cluster ID. For further communication we need the cluster id and cluster name to identify your cluster. ",
    "url": "/managedk8s/clusterlifecycle/clustercreation/#cluster-creation",
    
    "relUrl": "/managedk8s/clusterlifecycle/clustercreation/#cluster-creation"
  },"359": {
    "doc": "Cluster Creation",
    "title": "Detailed Information",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/clustercreation/#detailed-information",
    
    "relUrl": "/managedk8s/clusterlifecycle/clustercreation/#detailed-information"
  },"360": {
    "doc": "Cluster Creation",
    "title": "Prerequisites",
    "content": "OpenStack Credentials . You must have an existing or newly created OpenStack tenant. In this tenant, create and provide the following: . | Application credentialslinked to the project where you want the new cluster to be created. | . Required Information for the New Cluster . | Cluster Name: A maximum of 22 characters. | Customer ID: Defaults to the company ID number if not specified. | Application Credentials: As mentioned in the prerequisites. | . Optional Requirements and Configurable Features (with Default Values) . Kubernetes Version . If not specified, the cluster will be deployed with the latest supported Kubernetes minor version. For more details on supported versions, deprecations, EOL, or other version concerns, click here . OpenStack Network . Provide an existing network ID, or we will create a new network for you. Machine Deployments, Worker Nodes and Autoscaling . For Machine Deployment look into the more detailed docs . As ther are a some more options. Default will get you a single Machine Deployment with 3 Nodes on 1 AZ . Cluster autoscaler (the same for each machineDeployment if more than one) . We support the Cluster Autoscalar which we can activate seperatly for each Machine Deploment. Details can be found here . ",
    "url": "/managedk8s/clusterlifecycle/clustercreation/#prerequisites",
    
    "relUrl": "/managedk8s/clusterlifecycle/clustercreation/#prerequisites"
  },"361": {
    "doc": "Cluster Creation",
    "title": "Cluster Creation",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/clustercreation/",
    
    "relUrl": "/managedk8s/clusterlifecycle/clustercreation/"
  },"362": {
    "doc": "Openstack Application Credentials",
    "title": "Openstack Application Credentials",
    "content": "To set up a managed Kubernetes cluster on OpenStack, we require a pair of OpenStack application credentials to ensure proper access within the OpenStack project. Follow the guidelines Openstack Documentation to create and manage these credentials securely. Please be aware : . | Do not send the credentials through unsecured channels, use only the tools and methods agreed upon during onboarding. | Application credentials should be associated with a dedicated service account, not a personalized user account. This is to avoid issues related to credentials being revoked when a user is offboarded. | If needed, we can rotate the application credentials within the cluster. | . ",
    "url": "/managedk8s/clusterlifecycle/appcredentials/",
    
    "relUrl": "/managedk8s/clusterlifecycle/appcredentials/"
  },"363": {
    "doc": "OIDC",
    "title": "OIDC",
    "content": "You can add a custom oidc configuration for the cluster. This can only be done in the cluster creation process. Available variables that can be passed to the kubernetes API: . oidc_ca_file: \"path_to_file\" oidc_client_id: \"12345\" oidc_groups_claim: \"email\" oidc_groups_prefix: \"oidc:\" oidc_issuer_url: \"https://...\" oidc_required_claims: - 'key=value' oidc_signing_algs: \"RS256\" oidc_username_claim: \"sub\" oidc_username_prefix: \"...\" . You don’t need all variables. This is highly dependent on your oidc Provider. Please check your provider Documentation for details. Example for gitlab as oidc provider: . oidc_client_id: &lt;asdasdasdasdasdasdasdasdasd&gt; oidc_groups_claim: groups oidc_groups_prefix: 'oidc:' oidc_issuer_url: https://gitlab.address.example oidc_signing_algs: RS256 oidc_username_claim: sub oidc_username_prefix: https://gitlab.address.example# . ",
    "url": "/managedk8s/clusterlifecycle/oidc/",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/"
  },"364": {
    "doc": "OIDC",
    "title": "Requirements",
    "content": "To use the oidc login for kubernetes you need a kubectl plugin, a valid / prepared kubeconfig and RBAC permissions. ",
    "url": "/managedk8s/clusterlifecycle/oidc/#requirements",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/#requirements"
  },"365": {
    "doc": "OIDC",
    "title": "kubectl plugin",
    "content": "To handle the auth part automaticly you need an plugin for kubectl. this can be found here: int128/kubelogin . Hint: There is an kubectl plugin manager: krew this could be usefull if you handle more than one plugin. ",
    "url": "/managedk8s/clusterlifecycle/oidc/#kubectl-plugin",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/#kubectl-plugin"
  },"366": {
    "doc": "OIDC",
    "title": "kubeconf",
    "content": "The kubeconf must reflect the oidc . apiVersion: v1 kind: Config preferences: {} clusters: - cluster: certificate-authority-data: ABC= server: https://api.cluster.example:6443 name: example-cluster-0 contexts: - context: cluster: example-cluster-0 namespace: mi5 user: kubernetes-admin name: example-cluster-0 users: - name: oidc user: exec: apiVersion: client.authentication.k8s.io/v1beta1 args: - oidc-login - get-token - --oidc-issuer-url=https://gitlab.address.example - --oidc-client-id=asdasdasdasd - --oidc-client-secret=qweqweqweqweqweqweqwe command: kubectl env: null interactiveMode: IfAvailable provideClusterInfo: false . ",
    "url": "/managedk8s/clusterlifecycle/oidc/#kubeconf",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/#kubeconf"
  },"367": {
    "doc": "OIDC",
    "title": "RBAC",
    "content": "You need the proper RBAC configuration / permissions. Check out the offial documentation for this topic. Roles / ClusterRoles . You need a Role / ClusterRole to define the access of the oidc users. As an example: . apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: example-reader rules: - apiGroups: - '*' resources: - '*' verbs: # we don't want to delete with normal roles - get - list - watch - nonResourceURLs: - '*' verbs: # we don't want to delete with normal roles - get . Rolebinding / ClusterRoleBinding . You need a RoleBinding / ClusterRoleBinding to bind the role to an oidc user. for example: . kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-oidc-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: example-reader subjects: - kind: User # James Bond name: https://gitlab.address.example#007 . ",
    "url": "/managedk8s/clusterlifecycle/oidc/#rbac",
    
    "relUrl": "/managedk8s/clusterlifecycle/oidc/#rbac"
  },"368": {
    "doc": "Root Disks",
    "title": "Root Disks",
    "content": "You can define disk size and disk type for the controle plane and the worker groups. Please provide this information in the following format: . # Controle plane disk_size_cp: &lt;INT&gt; volume_type_cp: &lt;vType&gt; # Worker # default disk_size: &lt;INT&gt; volume_type: &lt;vType&gt; . &lt;INT&gt; replace this with in positive integer. This is the size of the root disk in Gigabyte. &lt;vType&gt; is dependent on the used plattform. Please consult the plattform documentation for this information. | optimist | . The default value schould be default and only be changed if you know that you have different requirements. ",
    "url": "/managedk8s/clusterlifecycle/rootdisk/",
    
    "relUrl": "/managedk8s/clusterlifecycle/rootdisk/"
  },"369": {
    "doc": "Root Disks",
    "title": "Considerations",
    "content": ". | While choosing a disksize make sure you take in to account that the pulled docker images are living on this disks. So if you have a lot of big docekr images adjust your disk size accordingly. | If you use a lot of ephemeral-volumes or store / process large amounts of data adjust your disk size accordingly. | if you know you have a lot of k8s API changes consider a higher iops controle plane root disk. | If you specify a root disk it will create the corresponding disks in your tenant. | . Hint: The default volume is if nothing is specified type: default and size: 20GB . ",
    "url": "/managedk8s/clusterlifecycle/rootdisk/#considerations",
    
    "relUrl": "/managedk8s/clusterlifecycle/rootdisk/#considerations"
  },"370": {
    "doc": "Machine Deployments",
    "title": "Machine Deployments",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/"
  },"371": {
    "doc": "Machine Deployments",
    "title": "Machine Deployment Creation",
    "content": ". | We are responsible for creating machine deployments based on the your requirements. | Each machine deployment will be configured to meet specific needs such as compute capacity, storage, and availibily zone. | Please provide the name of the machineDeployment(s). If the machine deployment name is not provided, the name will default to the format clusterName-az-md (e.g., cluster-test-ix2-md). | We can enable autoscaler for you, please see (here) [/managedk8s/clusterlifecycle/autoscaling/] | We can enable pre cordoned (clusterwide) so all nodes get cordoned before rotating. Detail see Node Rotation Section. | The default machine deployment configuration is the following: ```yaml workers: | name: clusterName-az-md replicas: 3 autoscaler: min: 0 max: 0 availability_zones: . | roles: | “worker” restrictions: [] machine_type: s1.large use_custom_disk: false ``` If you need any other changes in the configuration, please mention them when requesting a new machine deployment. However, first, please review the following configuration details and limitations: | . | Flavor/Machine Type - the default type is s1.large (8 cores, 16GB RAM and 20GB disk size), you can select each flavor that you can see in your OpenStack project, but make sure the flavor: . | has at least 2 cores and 2 GB RAM | is not a windows image | . | Number of replicas the default is 3 nodes. | Availability zone (AZ) - if preferred, specify the AZ from ix1, ix2 or es1 available zones. The default value will be a random AZ. | If there are multiple AZs configured, there will be a machineDeployment created for each AZ with the specified replica count. For example if you want to use 3 replicas with AZs ix1 and ix2, there will be 2 machineDeployments each with 3 replicas. | . | Custom root disk size and/or volume type - if not enabled foe more information please see here[/managedk8s/clusterlifecycle/rootdisks] | roles are set as labels on the nodes. where the labels have the following format: node-role.kubernetes.io/&lt;ROLENAME&gt;: \"\" | restrictions are set as labels on the nodes. where the labels have the following format: node-role.kubernetes.io/&lt;RESTRICTIONNAME&gt;: \"\" | . Please be aware that we only accept this format for role and restriction: node-role.kubernetes.io/NAME: ““ . ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#machine-deployment-creation",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#machine-deployment-creation"
  },"372": {
    "doc": "Machine Deployments",
    "title": "Machine Deployment Updates",
    "content": ". | We can update the machine deployment for you, you can: . | update the flavor | update replicas | update the application credentials | enable/disable the autoscaler | enable/disable oidc | . | We can create a new machine deployment(s) for you. | Please be aware that: . | We can not update the availibity zone , as the volumes will be stack in the old availibility zone. | We can not create a machine deployment in different availibility zone, if you need multiple availibility zone you need to create multiple machinedeployment in each availibility zone. | We will regularly update the os to ensure they are running the latest software versions, patches, and security updates. | . | . ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#machine-deployment-updates",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#machine-deployment-updates"
  },"373": {
    "doc": "Machine Deployments",
    "title": "Node Rotation",
    "content": "We are rotating the nodes in your Kubernetes cluster to ensure optimal performance, security, and reliability. We are rotating the nodes of all the machinedeployments on the same time. The process is the following: . | if configured all nodes in the machinedeployment will be cordoned (only triggered by an image update) | Creating a new one(s): A new node(s) is created to replace the drained one | Draining the nodes(s): We start by safely draining each node(s) to gracefully evict all running pods. | Deleting the old node(s): Once the new node is fully operational and all workloads are running smoothly, the old node is deleted. | . This process is applied to all nodes in the cluster’s machine deployments, and it’s performed periodically or whenever needed to maintain the desired state of the cluster. Node rotation is necessary for several reasons, including: . Security Updates: Applying critical security patches and updates to the operating system. Performance Improvements: Upgrading to newer versions of the Kubernetes. Resource Optimization: Adjusting node configurations. ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#node-rotation",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#node-rotation"
  },"374": {
    "doc": "Machine Deployments",
    "title": "What You Need to Take Care Of:",
    "content": "Workload Stability: Ensure that your applications are resilient to potential disruptions. Kubernetes will attempt to reschedule your workloads automatically, but some applications may experience brief downtime or require manual intervention. Pod Disruption Budgets: Review and, if necessary, update your Pod Disruption Budgets (PDBs) to control the number of pods that can be safely evicted during the rotation without impacting application availability. By taking these steps, you can help ensure a smooth node rotation with minimal impact on your services. If you have any questions or need further assistance, please let us know. ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#what-you-need-to-take-care-of",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#what-you-need-to-take-care-of"
  },"375": {
    "doc": "Machine Deployments",
    "title": "pre cordoned",
    "content": "This optinal feature was implemented to reduce the pod restarts in an node rotation triggered by an image update. New nodes didn’t get cordoned. That results in the following behavior: Drained pods are only sheduled on new nodes. So there should be only one restart per pod per node rotation (if the pods are stable and there is no other reason a pod restart is triggered). ",
    "url": "/managedk8s/clusterlifecycle/machinedeployments/#pre-cordoned",
    
    "relUrl": "/managedk8s/clusterlifecycle/machinedeployments/#pre-cordoned"
  },"376": {
    "doc": "Cluster Autoscaler",
    "title": "Cluster Node Autoscaler",
    "content": "The Cluster Node Autoscaler automatically adjusts the number of nodes in a Kubernetes cluster to match the current workload. It ensures efficient resource utilization by scaling the cluster up when there is insufficient capacity to run all scheduled pods and scaling it down when there are unused nodes. ",
    "url": "/managedk8s/clusterlifecycle/autoscaling/#cluster-node-autoscaler",
    
    "relUrl": "/managedk8s/clusterlifecycle/autoscaling/#cluster-node-autoscaler"
  },"377": {
    "doc": "Cluster Autoscaler",
    "title": "How the Cluster Autoscaler Works:",
    "content": "Scaling Up: When the autoscaler detects that some pods cannot be scheduled due to insufficient resources (CPU, memory), it will automatically add new nodes to the cluster to provide the necessary capacity. Scaling Down: If the autoscaler identifies nodes that are underutilized or completely not used for a configurable period, it will remove those nodes to optimize resource usage and reduce costs. Before scaling down, it ensures that there are no critical pods running on those nodes and that workloads can be safely moved to other nodes. The Cluster Nodes Autoscaler is not enabled by default. To enable it, please contact us using the instructions provided here. When reaching out, include the following information: . | The cluster ID and Name | The Specific Machine Deployment(s) Name | Minimum Number of Worker Nodes in the specific Machine Deployment: Define the minimum number of nodes that should always be maintained in the specific Machine Deployment. | Maximum Number of Worker Nodes: Define the maximum number of nodes in the specific Machine Deployment the cluster can scale up to. | . It is possible to enable or disable the cluster-autoscaler feature anytime in one or multiple machineDeployments. Note: Ensure pods have appropriate resource requests and limits to make the autoscaling effective. ",
    "url": "/managedk8s/clusterlifecycle/autoscaling/#how-the-cluster-autoscaler-works",
    
    "relUrl": "/managedk8s/clusterlifecycle/autoscaling/#how-the-cluster-autoscaler-works"
  },"378": {
    "doc": "Cluster Autoscaler",
    "title": "Cluster Autoscaler",
    "content": " ",
    "url": "/managedk8s/clusterlifecycle/autoscaling/",
    
    "relUrl": "/managedk8s/clusterlifecycle/autoscaling/"
  },"379": {
    "doc": "Deprecation Policy",
    "title": "Deprecation Policy",
    "content": "The upstream Kubernetes project releases approximately three Kubernetes versions a year and deprecates the same number of old versions. Kubernetes follows an N-2 support policy (meaning that the 3 most recent minor versions receive security and bug fixes). A good visualization of the support period for each release is available below: . Managed Kubernetes aligns loosely to this lifecycle by continuously introducing new versions and deprecating older ones. After a given Kubernetes version has reached End-of-Life, it will not get any bugfixes or security updates. Hence, we cannot support it anymore either and have to deprecate it. ",
    "url": "/managedk8s/clusterlifecycle/deprecationpolicy/",
    
    "relUrl": "/managedk8s/clusterlifecycle/deprecationpolicy/"
  },"380": {
    "doc": "Deprecation Policy",
    "title": "Deprecation Process",
    "content": "If we decide to deprecate a specific Kubernetes version, we will notify you well in advance. The notification will include an End-of-Life announcement outlining the version’s deprecation timeline. During that time you should plan and prepare for the upgrade of your cluster to a supported Kubernetes version before the deprecated version is removed. You can find the list of supported Kubernetes versions and their planned End-of-Life dates here. What Does an End-of-Life Announcement Mean for Me? . If an End-of-Life announcement has been made for a specific Kubernetes version, we suggest customers to contact us for upgrading their clusters to a newer version, preferably the latest one. What Happens If I Do Not Update Before the EOL Date? . After the EOL period has ended: . | The EOL Kubernetes version will no longer be available in the managed service. | Any clusters still running the EOL version will be automatically upgraded to a supported version. | New clusters cannot be created with the EOL version. | . Can I Stay on an EOL Version Forever? . No, as this would possibly mean serious security issues in the future. ",
    "url": "/managedk8s/clusterlifecycle/deprecationpolicy/#deprecation-process",
    
    "relUrl": "/managedk8s/clusterlifecycle/deprecationpolicy/#deprecation-process"
  },"381": {
    "doc": "Deprecation Policy",
    "title": "Force Upgrade Policy",
    "content": "If a Kubernetes version reaches End-of-Life, we have to remove its support from Managed Kubernetes since it will not receive any bugfixes or security updates anymore. It is important to emphasize the following technical limitations in Kubernetes: . | A (control plane of a) Kubernetes cluster can be upgraded by one version at a time, e.g. from v1.30 -&gt; v1.31. | It is not possible to upgrade more than one versions at once. | It is not possible to downgrade a cluster. | . This means that if customers do not update their clusters before the removal of the next EOL version, they risk not being able to upgrade after the removal of the next deprecated version. This would imply a serious problem as their only alternative would be to create a new cluster and migrate the workload from the old one as upgrading would not be possible (since it would require upgrading two versions at once). To overcome this issue, we need to actively force customers to upgrade clusters running on an EOL Kubernetes version, before we remove the next deprecated version. What Happens to My Clusters During Force Upgrade? . We will initiate an automated Kubernetes upgrade for the control plane and the Machine Deployment(s). While this should work, it cannot be guaranteed to work, given the diversity of applications and use cases. Breaking changes in the Kubernetes API can lead to broken/incompatible applications inside the Kubernetes cluster. We can not overtake responsibility for such problems. To ensure optimal performance and security, we strongly advise all customers to keep their Kubernetes clusters up to date. Please contact us for executing your cluster upgrades. ",
    "url": "/managedk8s/clusterlifecycle/deprecationpolicy/#force-upgrade-policy",
    
    "relUrl": "/managedk8s/clusterlifecycle/deprecationpolicy/#force-upgrade-policy"
  },"382": {
    "doc": "Cluster Changes",
    "title": "How to request support for your GKSv3 cluster",
    "content": "You can find the list of the Support here . In case of requesting changes in the machineDeployment, please specify the machineDeployment where the changes should be applied. A format like md-name-az-md or cluster-name-az-mdis required so that we refer to the correct machineDeployment. We will handle the requested changes or reach out for more details if needed before applying them to your cluster. ",
    "url": "/managedk8s/clusterlifecycle/clusterchanges/#how-to-request-support-for-your-gksv3-cluster",
    
    "relUrl": "/managedk8s/clusterlifecycle/clusterchanges/#how-to-request-support-for-your-gksv3-cluster"
  },"383": {
    "doc": "Cluster Changes",
    "title": "Cluster Changes",
    "content": "We can support the following updates in a running GKSv3 cluster: . Kubernetes Version Upgrade . | Upgrade your cluster to a newer Kubernetes version. | . Kubeconfig Rotation . | Rotate the kubeconfig for security or operational needs. | . Control Plane Changes . | Update the flavor type of the control plane. | . Machine Deployment Changes . | Add new machine deployments. | Delete existing machine deployments. | Update existing machine deployments, including: . | Name | Replicas | Flavor type | . | . Please Note: We cannot migrate a volume from one Availability Zone (AZ) to another. If you plan to update the machine deployment’s AZ, be aware that this may cause issues with the volumes associated with that machine deployment. Custom Root Volume Modifications . | Add or remove custom root volumes for the control plane or worker nodes, including: . | Disk size | Volume type | . | . Cluster Autoscaler Feature . | Enable or disable the cluster-autoscaler for a specific machine deployment. | Adjust the minimum and maximum size of worker nodes to enable autoscaling. | . ",
    "url": "/managedk8s/clusterlifecycle/clusterchanges/",
    
    "relUrl": "/managedk8s/clusterlifecycle/clusterchanges/"
  },"384": {
    "doc": "Cluster Deletion",
    "title": "Cluster Deletion",
    "content": "As a customer, you are responsible for cleaning up all applications and resources, and stopping any automation running inside the cluster before requesting its deletion. Once all applications, resources, and automation processes have been deleted or stopped, you can request cluster deletion via your preferred support channel (Microsoft Teams, Slack, Email) with providing the cluster ID and name. Then, we will proceed with deleting the cluster. If any applications or resources are still running inside the cluster, we will perform a forced deletion. You will then be responsible for manually cleaning up any leftover resources in Openstack. ",
    "url": "/managedk8s/clusterlifecycle/clusterdeletion/",
    
    "relUrl": "/managedk8s/clusterlifecycle/clusterdeletion/"
  },"385": {
    "doc": "FAQ",
    "title": "Frequently Asked Questions",
    "content": " ",
    "url": "/managedk8s/faq/#frequently-asked-questions",
    
    "relUrl": "/managedk8s/faq/#frequently-asked-questions"
  },"386": {
    "doc": "FAQ",
    "title": "What’s the recommanded cluster size?",
    "content": "For high availability and fault tolerance, a common recommendation is to have: . | 3 control plane Nodes. | 3 Worker Nodes: Having at least two worker nodes per machine deployment helps distribute the workload and provides redundancy. If one worker node fails, the other can continue to run the applications, minimizing downtime. | . The recommended approach for machine deployment is to create separate machine deployments in each Availability Zones (AZs). This helps ensure that your application remains available even if one or more AZs experience failures. For example, create three machine deployments: . | Machine Deployment in ix1 | Machine Deployment in ix2 | Machine Deployment in es1 | . For each machine deployment, configure at least 2 replicas. This ensures that there is redundancy within each AZ. ",
    "url": "/managedk8s/faq/#whats-the-recommanded-cluster-size",
    
    "relUrl": "/managedk8s/faq/#whats-the-recommanded-cluster-size"
  },"387": {
    "doc": "FAQ",
    "title": "What is Cluster Autoscaler?",
    "content": "Cluster Autoscaler is a standalone program that adjusts the size of a Kubernetes cluster to meet the current needs. ",
    "url": "/managedk8s/faq/#what-is-cluster-autoscaler",
    
    "relUrl": "/managedk8s/faq/#what-is-cluster-autoscaler"
  },"388": {
    "doc": "FAQ",
    "title": "When does Cluster Autoscaler change the size of a cluster?",
    "content": "Cluster Autoscaler increases the size of the cluster when: . | there are pods that failed to schedule on any of the current nodes due to insufficient resources. | adding a node similar to the nodes currently present in the cluster would help. | . Cluster Autoscaler decreases the size of the cluster when some nodes are consistently unneeded for a significant amount of time. A node is unneeded when it has low utilization and all of its important pods can be moved elsewhere. ",
    "url": "/managedk8s/faq/#when-does-cluster-autoscaler-change-the-size-of-a-cluster",
    
    "relUrl": "/managedk8s/faq/#when-does-cluster-autoscaler-change-the-size-of-a-cluster"
  },"389": {
    "doc": "FAQ",
    "title": "Can We Reserve a Specific IP for a Kubernetes Service of Type LoadBalancer?",
    "content": "Yes, you can use the loadbalancer.openstack.org/keep-floatingip annotation to ensure that the floating IP remains associated with your project and is reused by the Kubernetes service. Here’s an example of how to configure a Kubernetes service with a reserved IP address using the annotation: . apiVersion: v1 kind: Service metadata: name: nginx-internet annotations: loadbalancer.openstack.org/keep-floatingip: \"true\" # Annotation to keep the floating IP in the project spec: type: LoadBalancer selector: app: nginx ports: - port: 80 targetPort: 80 loadBalancerIP: 45.94.08.9 # Specific floating IP to be reserved for the service . ",
    "url": "/managedk8s/faq/#can-we-reserve-a-specific-ip-for-a-kubernetes-service-of-type-loadbalancer",
    
    "relUrl": "/managedk8s/faq/#can-we-reserve-a-specific-ip-for-a-kubernetes-service-of-type-loadbalancer"
  },"390": {
    "doc": "FAQ",
    "title": "Can We Create a Kubernetes Service with a Specific Floating IP?",
    "content": "Yes, you can create a Kubernetes Service that uses a specific floating IP by setting the loadBalancerIP field in the service definition. The IP you specify as loadBalancerIP must already be allocated as a floating IP in OpenStack. Here’s an example of how you can specify a floating IP for a Kubernetes Service: . apiVersion: v1 kind: Service metadata: name: nginx-internet spec: type: LoadBalancer selector: app: nginx ports: - port: 80 targetPort: 80 loadBalancerIP: 45.94.08.9 # Specific floating IP to be reserved for the service . By using the loadBalancerIP field, you ensure that the service will use the specified floating IP when a load balancer is provisioned. ",
    "url": "/managedk8s/faq/#can-we-create-a-kubernetes-service-with-a-specific-floating-ip",
    
    "relUrl": "/managedk8s/faq/#can-we-create-a-kubernetes-service-with-a-specific-floating-ip"
  },"391": {
    "doc": "FAQ",
    "title": "Can I use external-dns and openstack designate for automatic dns?",
    "content": "Yes, a few installation and configuration steps are necessary for this which are explained here. ",
    "url": "/managedk8s/faq/#can-i-use-external-dns-and-openstack-designate-for-automatic-dns",
    
    "relUrl": "/managedk8s/faq/#can-i-use-external-dns-and-openstack-designate-for-automatic-dns"
  },"392": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": " ",
    "url": "/managedk8s/faq/",
    
    "relUrl": "/managedk8s/faq/"
  },"393": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Can I use external-dns and openstack designate for automatic dns?",
    "content": "Yes, to reduce manual effort and automate the configuration of DNS zones, you may want to use external-dns. In summary, external-dns allows you to control DNS records dynamically with Kubernetes resources in a DNS provider-agnostic way. external-dns is not a DNS server by itself, but merely configures other DNS providers (for example, OpenStack Designate, Amazon Route53, Google Cloud DNS, and so on.) . ",
    "url": "/managedk8s/faq/automatic_dns",
    
    "relUrl": "/managedk8s/faq/automatic_dns"
  },"394": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Prerequisites",
    "content": "To successfully complete the following steps, you need the following: . | kubectl latest version | A running Kubernetes cluster on our openstack | A valid kubeconfig for your cluster | Installed OpenStack CLI tools | OpenStack API access | A valid domain | . ",
    "url": "/managedk8s/faq/automatic_dns#prerequisites",
    
    "relUrl": "/managedk8s/faq/automatic_dns#prerequisites"
  },"395": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Configure Your domain to use designate",
    "content": "Delegate your domains from your DNS provider to the following DNS name servers so that Designate can control the DNS resources of your domain. dns1.ddns.innovo.cloud dns2.ddns.innovo.cloud . ",
    "url": "/managedk8s/faq/automatic_dns#configure-your-domain-to-use-designate",
    
    "relUrl": "/managedk8s/faq/automatic_dns#configure-your-domain-to-use-designate"
  },"396": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Create a new DNS Zone",
    "content": "Before you use ExternalDNS, you need to add your DNS zones to your DNS provider, in this case, Designate DNS. In our example we use the test domain name example.foo. It is important to create the zones before starting to control the DNS resources with Kubernetes. Note: You must include the final . at the end of the zone/domain to be created. $ openstack zone create --email webmaster@example.foo example.foo. Next, make sure that the zone was created successfully and the status is active. $ openstack zone list $ openstack zone show example.foo. ",
    "url": "/managedk8s/faq/automatic_dns#create-a-new-dns-zone",
    
    "relUrl": "/managedk8s/faq/automatic_dns#create-a-new-dns-zone"
  },"397": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Create application credentials for external-dns",
    "content": "Attention: It is imported to login with your service user into your openstack project, because Application Credentials are user specific. Visit the WebGui and go to Identity -&gt; Application Credentials, then click on the Button + CREATE APPLICATION CREDENTIAL or create credentials via cli: . $ openstack application credential create &lt;name_of_app_credentials&gt; . ",
    "url": "/managedk8s/faq/automatic_dns#create-application-credentials-for-external-dns",
    
    "relUrl": "/managedk8s/faq/automatic_dns#create-application-credentials-for-external-dns"
  },"398": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Install external-dns into your cluster",
    "content": "Note: Don’t forget to change the openstack application credentials and the command line arguments in the external-dns deployment --domain-filter=example.foo and the --txt-owner-id=&lt;owner-id&gt;. The domain-filter is the dns zone. With the txt-owner-id external-dns can identify the entries managed by itself. You should change the image to a new version if available (and shedule updates) if you want to use it in production. | Namespace: | . apiVersion: v1 kind: Namespace metadata: name: external-dns . | RBAC: | . --- apiVersion: v1 kind: ServiceAccount metadata: name: external-dns namespace: external-dns --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\"\"] resources: [\"services\", \"endpoints\", \"pods\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"extensions\", \"networking.k8s.io\"] resources: [\"ingresses\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"watch\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: external-dns . | Secret: | . apiVersion: v1 kind: Secret type: Opaque metadata: name: designate-openstack-credentials namespace: external-dns stringData: OS_AUTH_URL: &lt;auth-url&gt; OS_REGION_NAME: &lt;region&gt; OS_AUTH_TYPE: v3applicationcredential OS_APPLICATION_CREDENTIAL_ID: &lt;appcred_id&gt; OS_APPLICATION_CREDENTIAL_SECRET: &lt;appcred_secret&gt; . | Deployment: | . apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: external-dns spec: selector: matchLabels: app: external-dns strategy: type: Recreate template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns containers: - args: - --source=service - --source=ingress - --registry=txt - --provider=designate - --domain-filter=example.foo - --txt-owner-id=&lt;owner-id&gt; - --log-level=debug envFrom: - secretRef: name: designate-openstack-credentials image: registry.k8s.io/external-dns/external-dns:v0.14.2 imagePullPolicy: IfNotPresent name: external-dns . ",
    "url": "/managedk8s/faq/automatic_dns#install-external-dns-into-your-cluster",
    
    "relUrl": "/managedk8s/faq/automatic_dns#install-external-dns-into-your-cluster"
  },"399": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Annotate the service or ingress",
    "content": "To add the dns entry to designate the service or ingress needs to be annotated with external-dns.alpha.kubernetes.io/hostname: my-app.example.foo. For example: . apiVersion: v1 kind: Namespace metadata: name: my-app . apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: my-app spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ports: - containerPort: 80 . apiVersion: v1 kind: Service metadata: name: nginx namespace: my-app annotations: external-dns.alpha.kubernetes.io/hostname: my-app.example.foo spec: selector: app: nginx type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 80 . ",
    "url": "/managedk8s/faq/automatic_dns#annotate-the-service-or-ingress",
    
    "relUrl": "/managedk8s/faq/automatic_dns#annotate-the-service-or-ingress"
  },"400": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "DNS Lookup",
    "content": "Make the dns record will be looked up correct: . $ openstack recordset list example.foo. $ dig my-app.example.foo @dns1.ddns.innovo.cloud. $ dig my-app.example.foo @dns2.ddns.innovo.cloud. $ dig my-app.example.foo . ",
    "url": "/managedk8s/faq/automatic_dns#dns-lookup",
    
    "relUrl": "/managedk8s/faq/automatic_dns#dns-lookup"
  },"401": {
    "doc": "Can I use external-dns and openstack designate for automatic dns?",
    "title": "Further information",
    "content": "For further information, please have a look at: . | kubernetes-sigs.github.io/external-dns/latest/docs/tutorials/designate | github.com/kubernetes-sigs/external-dns | docs.openstack.org/python-designateclient | . ",
    "url": "/managedk8s/faq/automatic_dns#further-information",
    
    "relUrl": "/managedk8s/faq/automatic_dns#further-information"
  },"402": {
    "doc": "Logging and Metrics",
    "title": "User Guide for Logging and Metrics",
    "content": "This document provides an overview of logging and metrics capabilities in our Managed Kubernetes Service. Please note that we only monitor the control plane, we do not collect or store any user application data or user-generated logs from within customer-deployed workloads. Our focus is exclusively on metrics and logs related to the cluster control plane’s health, performance, and operational efficiency. Please note that you can access all logs/metrics endpoints even from the control plane. Customers are responsible for managing their application logs, pod metrics, and data generated within their workloads. This includes: . | Collecting Application Logs: Customer may need to configure their logging solutions (e.g., Fluentd, Filebeat) to gather logs from applications and store them in a preferred location. | Monitoring Application Metrics: For custom application metrics, customer can set up their own Prometheus or other monitoring solutions. | . We recommend the following practices for managing application logs and metrics independently: . | Deploy Logging Solutions: Configure logging agents (e.g., Fluentd, Logstash) on each node to collect logs from your applications and forward them to a centralized logging service of your choice (e.g., Elasticsearch, Loki), we recommand to ship the data to our central logging system Elastic . | Set Up Prometheus and Grafana: For a complete metrics solution, deploy Prometheus and Grafana to monitor and visualize resource usage, custom metrics, and application performance. | . ",
    "url": "/managedk8s/faq/logging_metrics/#user-guide-for-logging-and-metrics",
    
    "relUrl": "/managedk8s/faq/logging_metrics/#user-guide-for-logging-and-metrics"
  },"403": {
    "doc": "Logging and Metrics",
    "title": "Logging and Metrics",
    "content": " ",
    "url": "/managedk8s/faq/logging_metrics/",
    
    "relUrl": "/managedk8s/faq/logging_metrics/"
  },"404": {
    "doc": "StorageClass Setup",
    "title": "StorageClass Setup",
    "content": "We provide one default storage class per Cluster. Caution: This is managed by WIIT and can be overwritten at any time. Please create a separate storage class for your changes. kubectl get storageclasses.storage.k8s.io NAME PROVISIONER AGE cinder-csi (default) cinder.csi.openstack.org 6h45m . ",
    "url": "/managedk8s/storageclasses/",
    
    "relUrl": "/managedk8s/storageclasses/"
  },"405": {
    "doc": "StorageClass Setup",
    "title": "Openstack Volume Types",
    "content": "The Openstack volume types sorted by maximum possible IOPS: . | low-iops | default &lt;- used in the default class | high-iops | . ",
    "url": "/managedk8s/storageclasses/#openstack-volume-types",
    
    "relUrl": "/managedk8s/storageclasses/#openstack-volume-types"
  },"406": {
    "doc": "StorageClass Setup",
    "title": "Volume Features",
    "content": "We don’t provide Read-Write-Many Volumes. All Volumes are Read-Write-Once! . ",
    "url": "/managedk8s/storageclasses/#volume-features",
    
    "relUrl": "/managedk8s/storageclasses/#volume-features"
  },"407": {
    "doc": "StorageClass Setup",
    "title": "Adding Your Own Classes",
    "content": "If you need use one of the other types, you can add your own definitions. Example: . apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: my-high-iops-class provisioner: cinder.csi.openstack.org parameters: type: high-iops . Apply with kubectl apply -f storage-class.yaml. | name: Choose a unique one, as we don’t want to interfere with the default names. | provisioner: Use the one of your cluster. You can always have a look in the default class to verify the right provider. | type: Use one of the official provided types from the Optimist platform (at the time of writing low-iops and high-iops). | . To use the new storage class you need to change your volumes definitions and add the new StorageClass name. ",
    "url": "/managedk8s/storageclasses/#adding-your-own-classes",
    
    "relUrl": "/managedk8s/storageclasses/#adding-your-own-classes"
  }
}
